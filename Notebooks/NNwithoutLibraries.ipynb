{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"/home/rakesh47/NNfromScratch/PreparedData/X_train.csv\", header=None)\n",
    "y_train = pd.read_csv(\"/home/rakesh47/NNfromScratch/PreparedData/y_train.csv\", header=None)\n",
    "\n",
    "X_test = pd.read_csv(\"/home/rakesh47/NNfromScratch/PreparedData/X_test.csv\", header=None)\n",
    "y_test = pd.read_csv(\"/home/rakesh47/NNfromScratch/PreparedData/y_test.csv\", header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. User-defined hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((354, 13), (354, 1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D_in is input dimension; H is hidden dimension; D_out is output dimension.\n",
    "\n",
    "batch_size, D_in, H, D_out = 16, 13, 100, 1\n",
    "learning_rate = 1e-6\n",
    "n_epochs = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Helper class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomMatrix:\n",
    "    \"Generates a random 2D-matrix with given dimensions and gaussian distibution.\"\n",
    "    \n",
    "    def __init__(self, n_rows, n_cols, mu=0, sigma=1):\n",
    "        self.n_rows = n_rows\n",
    "        self.n_cols = n_cols\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.matrix = [[random.gauss(mu, sigma) for _ in range(n_cols)] for _ in range(n_rows)]\n",
    "        \n",
    "        \n",
    "    def size(self):\n",
    "        return (self.n_rows, self.n_cols)\n",
    "    \n",
    "    \n",
    "    def matmul(self, M):\n",
    "        if self.n_cols != M.n_rows:  raise Exception(\"matmul not possible because n_cols of first matrix = \" + str(self.n_cols) + \" , which is NOT EQUAL TO n_rows of second matrix = \" + str(M.n_cols))\n",
    "        \n",
    "        Result = RandomMatrix(self.n_rows, M.n_cols)\n",
    "        for i in range(self.n_rows):\n",
    "            for j in range(M.n_cols):\n",
    "                res_ij = 0\n",
    "                for k in range(self.n_cols):\n",
    "                    res_ij += self.matrix[i][k] * M.matrix[k][j]\n",
    "                Result.matrix[i][j] = res_ij\n",
    "        \n",
    "        return Result\n",
    "    \n",
    "    \n",
    "    def relu(self):\n",
    "        Result = RandomMatrix(self.n_rows, self.n_cols)\n",
    "        Result.matrix = [[self.matrix[i][j] if self.matrix[i][j]>0 else 0. for j in range(self.n_cols)] for i in range(self.n_rows)]\n",
    "        return Result\n",
    "    \n",
    "    \n",
    "    def __sub__(self, M):\n",
    "        if self.n_rows!=M.n_rows or self.n_cols!=M.n_cols:  raise Exception(\"__sub__ not possible because the matrics are not of same dimensions.\")\n",
    "        Result = RandomMatrix(self.n_rows, self.n_cols)\n",
    "        Result.matrix = [[self.matrix[i][j]-M.matrix[i][j] for j in range(self.n_cols)] for i in range(self.n_rows)]\n",
    "        return Result\n",
    "    \n",
    "    \n",
    "    def __mul__(self, scalar):\n",
    "        Result = RandomMatrix(self.n_rows, self.n_cols)\n",
    "        Result.matrix = [[self.matrix[i][j]*scalar for j in range(self.n_cols)] for i in range(self.n_rows)]\n",
    "        return Result\n",
    "    \n",
    "    \n",
    "    def pow(self, p):\n",
    "        Result = RandomMatrix(self.n_rows, self.n_cols)\n",
    "        Result.matrix = [[self.matrix[i][j]**p for j in range(self.n_cols)] for i in range(self.n_rows)]\n",
    "        return Result\n",
    "    \n",
    "    \n",
    "    def sum(self):\n",
    "        Result = 0.\n",
    "        for i in range(self.n_rows):\n",
    "            for j in range(self.n_cols):  Result += self.matrix[i][j]\n",
    "        return Result\n",
    "    \n",
    "    \n",
    "    def transpose(self):\n",
    "        Result = RandomMatrix(self.n_cols, self.n_rows)\n",
    "        Result.matrix = [[self.matrix[j][i] for j in range(self.n_rows)] for i in range(self.n_cols)]\n",
    "        return Result\n",
    "    \n",
    "    \n",
    "    def copy(self):\n",
    "        Result = RandomMatrix(self.n_rows, self.n_cols)\n",
    "        Result.matrix = [[self.matrix[i][j] for j in range(self.n_cols)] for i in range(self.n_rows)]\n",
    "        return Result\n",
    "    \n",
    "    \n",
    "    def arg_lt(self, val):\n",
    "        Result = []\n",
    "        for i in range(self.n_rows):\n",
    "            for j in range(self.n_cols):  \n",
    "                if self.matrix[i][j] < val:  Result.append((i,j))\n",
    "        return Result\n",
    "    \n",
    "    \n",
    "    def arg_set(self, idxs, val):\n",
    "        Result = self\n",
    "        for (i, j) in idxs:  Result.matrix[i][j] = val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. Random weight initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = RandomMatrix(D_in, H)\n",
    "w2 = RandomMatrix(H, D_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI. Perform training : CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, batch_size, model, n_epochs):\n",
    "    \n",
    "    X_train, y_train, X_test, y_test = data\n",
    "    w1, w2 = model\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for t in range(n_epochs):\n",
    "\n",
    "        epoch_start = time.time()\n",
    "        train_loss = 0.\n",
    "        val_loss = 0.    \n",
    "        for state in ['train', 'val']:        \n",
    "\n",
    "            batch_start_idx = 0\n",
    "            n_samples = len(X_train) if state=='train' else len(X_test)        \n",
    "            while batch_start_idx < n_samples:\n",
    "\n",
    "                # Get next batch of data \n",
    "                batch_end_idx = batch_start_idx + batch_size\n",
    "                if batch_end_idx > n_samples:  batch_end_idx = n_samples\n",
    "                x, y = RandomMatrix((batch_end_idx-batch_start_idx), X_train.shape[1]), RandomMatrix((batch_end_idx-batch_start_idx), y_train.shape[1])\n",
    "                if state == 'train':  x.matrix, y.matrix = X_train.iloc[batch_start_idx:batch_end_idx, :].values, y_train.iloc[batch_start_idx:batch_end_idx, :].values\n",
    "                else:  x.matrix, y.matrix = X_test.iloc[batch_start_idx:batch_end_idx, :].values, y_test.iloc[batch_start_idx:batch_end_idx, :].values\n",
    "\n",
    "                # Forward pass: Compute output and loss\n",
    "                h = x.matmul(w1)\n",
    "                h_relu = h.relu()\n",
    "                y_pred = h_relu.matmul(w2)\n",
    "                loss = (y_pred - y).pow(2).sum()\n",
    "                if state == 'train':  train_loss += loss\n",
    "                else:  val_loss += loss\n",
    "\n",
    "                # Backward pass: Compute gradients\n",
    "                if state == 'train':\n",
    "                    grad_y_pred = (y_pred - y) * 2.\n",
    "                    grad_w2 = h_relu.transpose().matmul(grad_y_pred)\n",
    "                    grad_h_relu = grad_y_pred.matmul(w2.transpose())\n",
    "                    grad_h = grad_h_relu.copy()\n",
    "                    lt_zero_idxs = h.arg_lt(0)\n",
    "                    grad_h.arg_set(lt_zero_idxs, 0)\n",
    "                    grad_w1 = x.transpose().matmul(grad_h)\n",
    "\n",
    "                ## Update weights using gradient descent\n",
    "                    w1 -= grad_w1 * learning_rate\n",
    "                    w2 -= grad_w2 * learning_rate\n",
    "\n",
    "                batch_start_idx = batch_end_idx\n",
    "\n",
    "        # Print statistics\n",
    "        train_loss /= len(X_train)\n",
    "        train_losses.append(train_loss)\n",
    "        val_loss /= len(X_test)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        epoch_end = time.time()\n",
    "        epoch_time = epoch_end - epoch_start\n",
    "        print(\"Epoch \", (t+1), \"/\", n_epochs, \" : Train-loss = \", train_loss, \", Val-loss = \", val_loss, \", Time for epoch = \", epoch_time, \"s\")\n",
    "\n",
    "    # Plot loss-curves\n",
    "    plt.figure()\n",
    "    plt.plot(range(2, n_epochs+1), train_losses[1:], label='Train-loss')\n",
    "    plt.plot(range(2, n_epochs+1), val_losses[1:], label='Val-loss')\n",
    "    plt.title('Loss curves')\n",
    "    plt.xlabel('No. of epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "    plt.grid(linestyle='dotted')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 / 1000  : Train-loss =  2.68108478757e+14 , Val-loss =  571.794580109 , Time for epoch =  0.7516875267028809 s\n",
      "Epoch  2 / 1000  : Train-loss =  603.05447488 , Val-loss =  569.780261682 , Time for epoch =  0.7436344623565674 s\n",
      "Epoch  3 / 1000  : Train-loss =  597.550734819 , Val-loss =  566.213829666 , Time for epoch =  0.7400805950164795 s\n",
      "Epoch  4 / 1000  : Train-loss =  589.939534494 , Val-loss =  554.597383723 , Time for epoch =  0.7374792098999023 s\n",
      "Epoch  5 / 1000  : Train-loss =  564.089108569 , Val-loss =  517.611783807 , Time for epoch =  0.7522895336151123 s\n",
      "Epoch  6 / 1000  : Train-loss =  418.345756526 , Val-loss =  200.243641196 , Time for epoch =  0.7561569213867188 s\n",
      "Epoch  7 / 1000  : Train-loss =  137.45181049 , Val-loss =  125.939670805 , Time for epoch =  0.7500648498535156 s\n",
      "Epoch  8 / 1000  : Train-loss =  111.730485966 , Val-loss =  129.444518409 , Time for epoch =  0.7469890117645264 s\n",
      "Epoch  9 / 1000  : Train-loss =  109.159064722 , Val-loss =  131.481009618 , Time for epoch =  0.748875617980957 s\n",
      "Epoch  10 / 1000  : Train-loss =  108.687199066 , Val-loss =  133.32376376 , Time for epoch =  0.7443957328796387 s\n",
      "Epoch  11 / 1000  : Train-loss =  109.147470174 , Val-loss =  134.371692042 , Time for epoch =  0.7530314922332764 s\n",
      "Epoch  12 / 1000  : Train-loss =  109.760665483 , Val-loss =  134.515855912 , Time for epoch =  0.7553691864013672 s\n",
      "Epoch  13 / 1000  : Train-loss =  110.055360174 , Val-loss =  134.158535769 , Time for epoch =  0.747025728225708 s\n",
      "Epoch  14 / 1000  : Train-loss =  110.036230209 , Val-loss =  133.522062226 , Time for epoch =  0.712672233581543 s\n",
      "Epoch  15 / 1000  : Train-loss =  109.767623226 , Val-loss =  132.65692191 , Time for epoch =  0.7146778106689453 s\n",
      "Epoch  16 / 1000  : Train-loss =  109.319642954 , Val-loss =  131.682180522 , Time for epoch =  0.7074630260467529 s\n",
      "Epoch  17 / 1000  : Train-loss =  108.784522628 , Val-loss =  130.654505825 , Time for epoch =  0.7129659652709961 s\n",
      "Epoch  18 / 1000  : Train-loss =  108.164846668 , Val-loss =  129.251993615 , Time for epoch =  0.7098109722137451 s\n",
      "Epoch  19 / 1000  : Train-loss =  107.346304753 , Val-loss =  128.014092429 , Time for epoch =  0.7124755382537842 s\n",
      "Epoch  20 / 1000  : Train-loss =  106.527344491 , Val-loss =  126.555911552 , Time for epoch =  0.7138311862945557 s\n",
      "Epoch  21 / 1000  : Train-loss =  105.688242842 , Val-loss =  125.391247523 , Time for epoch =  0.7135076522827148 s\n",
      "Epoch  22 / 1000  : Train-loss =  105.024482118 , Val-loss =  124.44306623 , Time for epoch =  0.7453258037567139 s\n",
      "Epoch  23 / 1000  : Train-loss =  104.530669651 , Val-loss =  123.64168661 , Time for epoch =  0.7265927791595459 s\n",
      "Epoch  24 / 1000  : Train-loss =  104.165401244 , Val-loss =  122.985460746 , Time for epoch =  0.7294983863830566 s\n",
      "Epoch  25 / 1000  : Train-loss =  103.847024495 , Val-loss =  122.171947474 , Time for epoch =  0.7152359485626221 s\n",
      "Epoch  26 / 1000  : Train-loss =  103.484558042 , Val-loss =  121.41573051 , Time for epoch =  0.709298849105835 s\n",
      "Epoch  27 / 1000  : Train-loss =  103.239881985 , Val-loss =  120.753712856 , Time for epoch =  0.7224855422973633 s\n",
      "Epoch  28 / 1000  : Train-loss =  103.055208447 , Val-loss =  120.160575156 , Time for epoch =  0.814749002456665 s\n",
      "Epoch  29 / 1000  : Train-loss =  102.900042198 , Val-loss =  119.614963934 , Time for epoch =  0.7633039951324463 s\n",
      "Epoch  30 / 1000  : Train-loss =  102.759390882 , Val-loss =  119.105278552 , Time for epoch =  0.7094690799713135 s\n",
      "Epoch  31 / 1000  : Train-loss =  102.625611726 , Val-loss =  118.6246613 , Time for epoch =  0.7263064384460449 s\n",
      "Epoch  32 / 1000  : Train-loss =  102.494683237 , Val-loss =  118.168653827 , Time for epoch =  0.7482559680938721 s\n",
      "Epoch  33 / 1000  : Train-loss =  102.364442108 , Val-loss =  117.73408303 , Time for epoch =  0.7509641647338867 s\n",
      "Epoch  34 / 1000  : Train-loss =  102.23372159 , Val-loss =  117.318517491 , Time for epoch =  0.7463932037353516 s\n",
      "Epoch  35 / 1000  : Train-loss =  102.101913259 , Val-loss =  116.919991762 , Time for epoch =  0.7548577785491943 s\n",
      "Epoch  36 / 1000  : Train-loss =  101.968733417 , Val-loss =  116.536858966 , Time for epoch =  0.7711119651794434 s\n",
      "Epoch  37 / 1000  : Train-loss =  101.834092268 , Val-loss =  116.167706701 , Time for epoch =  0.8088626861572266 s\n",
      "Epoch  38 / 1000  : Train-loss =  101.48355305 , Val-loss =  115.985059229 , Time for epoch =  0.7987995147705078 s\n",
      "Epoch  39 / 1000  : Train-loss =  100.964587178 , Val-loss =  115.27593759 , Time for epoch =  0.7463874816894531 s\n",
      "Epoch  40 / 1000  : Train-loss =  100.655104084 , Val-loss =  114.718863755 , Time for epoch =  0.8434202671051025 s\n",
      "Epoch  41 / 1000  : Train-loss =  100.460461092 , Val-loss =  114.239531542 , Time for epoch =  0.8997933864593506 s\n",
      "Epoch  42 / 1000  : Train-loss =  100.318372858 , Val-loss =  113.876017451 , Time for epoch =  0.8307151794433594 s\n",
      "Epoch  43 / 1000  : Train-loss =  100.23546057 , Val-loss =  113.512050893 , Time for epoch =  0.8176851272583008 s\n",
      "Epoch  44 / 1000  : Train-loss =  100.134683282 , Val-loss =  113.155066844 , Time for epoch =  0.7676229476928711 s\n",
      "Epoch  45 / 1000  : Train-loss =  100.024427996 , Val-loss =  112.807993377 , Time for epoch =  0.7610268592834473 s\n",
      "Epoch  46 / 1000  : Train-loss =  99.9088186901 , Val-loss =  112.47167462 , Time for epoch =  0.7427244186401367 s\n",
      "Epoch  47 / 1000  : Train-loss =  99.7899076641 , Val-loss =  112.146029413 , Time for epoch =  0.7195074558258057 s\n",
      "Epoch  48 / 1000  : Train-loss =  99.6687589464 , Val-loss =  111.830585799 , Time for epoch =  0.719897985458374 s\n",
      "Epoch  49 / 1000  : Train-loss =  99.5396376406 , Val-loss =  111.529827763 , Time for epoch =  0.717674970626831 s\n",
      "Epoch  50 / 1000  : Train-loss =  99.388797254 , Val-loss =  111.220469746 , Time for epoch =  0.7131538391113281 s\n",
      "Epoch  51 / 1000  : Train-loss =  99.2124846257 , Val-loss =  110.910182326 , Time for epoch =  0.7203826904296875 s\n",
      "Epoch  52 / 1000  : Train-loss =  99.0097957034 , Val-loss =  110.601941572 , Time for epoch =  0.7410647869110107 s\n",
      "Epoch  53 / 1000  : Train-loss =  98.8142369128 , Val-loss =  110.29937624 , Time for epoch =  0.754866361618042 s\n",
      "Epoch  54 / 1000  : Train-loss =  98.6094110066 , Val-loss =  110.00032641 , Time for epoch =  0.7118425369262695 s\n",
      "Epoch  55 / 1000  : Train-loss =  98.3981296762 , Val-loss =  109.676536993 , Time for epoch =  0.7144601345062256 s\n",
      "Epoch  56 / 1000  : Train-loss =  98.1940537249 , Val-loss =  109.328205754 , Time for epoch =  0.7171919345855713 s\n",
      "Epoch  57 / 1000  : Train-loss =  97.9984793142 , Val-loss =  108.991458131 , Time for epoch =  0.7110037803649902 s\n",
      "Epoch  58 / 1000  : Train-loss =  97.8111610258 , Val-loss =  108.66237193 , Time for epoch =  0.718815803527832 s\n",
      "Epoch  59 / 1000  : Train-loss =  97.6313276586 , Val-loss =  108.344292551 , Time for epoch =  0.7176780700683594 s\n",
      "Epoch  60 / 1000  : Train-loss =  97.4581080653 , Val-loss =  108.036391238 , Time for epoch =  0.7196319103240967 s\n",
      "Epoch  61 / 1000  : Train-loss =  97.2906905481 , Val-loss =  107.731518634 , Time for epoch =  0.7176079750061035 s\n",
      "Epoch  62 / 1000  : Train-loss =  97.1283696335 , Val-loss =  107.407565638 , Time for epoch =  0.8309440612792969 s\n",
      "Epoch  63 / 1000  : Train-loss =  96.970548903 , Val-loss =  107.094107826 , Time for epoch =  0.9117324352264404 s\n",
      "Epoch  64 / 1000  : Train-loss =  96.8167290074 , Val-loss =  106.790295441 , Time for epoch =  0.8915512561798096 s\n",
      "Epoch  65 / 1000  : Train-loss =  96.6664924658 , Val-loss =  106.495387003 , Time for epoch =  0.840538501739502 s\n",
      "Epoch  66 / 1000  : Train-loss =  96.5194894088 , Val-loss =  106.208731952 , Time for epoch =  0.8319501876831055 s\n",
      "Epoch  67 / 1000  : Train-loss =  96.3543195539 , Val-loss =  105.908300362 , Time for epoch =  0.8005552291870117 s\n",
      "Epoch  68 / 1000  : Train-loss =  96.188852962 , Val-loss =  105.612551659 , Time for epoch =  0.7686247825622559 s\n",
      "Epoch  69 / 1000  : Train-loss =  96.0263564854 , Val-loss =  105.323430443 , Time for epoch =  0.7552955150604248 s\n",
      "Epoch  70 / 1000  : Train-loss =  95.8678793062 , Val-loss =  105.041411551 , Time for epoch =  0.7725849151611328 s\n",
      "Epoch  71 / 1000  : Train-loss =  95.7008199081 , Val-loss =  104.749439518 , Time for epoch =  0.7831263542175293 s\n",
      "Epoch  72 / 1000  : Train-loss =  95.5261560488 , Val-loss =  104.460224993 , Time for epoch =  0.8540551662445068 s\n",
      "Epoch  73 / 1000  : Train-loss =  95.3545704126 , Val-loss =  104.176458932 , Time for epoch =  0.7402958869934082 s\n",
      "Epoch  74 / 1000  : Train-loss =  95.1878145082 , Val-loss =  103.899016496 , Time for epoch =  0.7626128196716309 s\n",
      "Epoch  75 / 1000  : Train-loss =  95.0263032545 , Val-loss =  103.628043404 , Time for epoch =  0.7743117809295654 s\n",
      "Epoch  76 / 1000  : Train-loss =  94.8699028401 , Val-loss =  103.363400467 , Time for epoch =  0.7670235633850098 s\n",
      "Epoch  77 / 1000  : Train-loss =  94.7182759349 , Val-loss =  103.104845495 , Time for epoch =  0.8274736404418945 s\n",
      "Epoch  78 / 1000  : Train-loss =  94.5710306053 , Val-loss =  102.852107862 , Time for epoch =  0.8633229732513428 s\n",
      "Epoch  79 / 1000  : Train-loss =  94.42778235 , Val-loss =  102.604918634 , Time for epoch =  0.8822824954986572 s\n",
      "Epoch  80 / 1000  : Train-loss =  94.2846643433 , Val-loss =  102.187449421 , Time for epoch =  0.71274733543396 s\n",
      "Epoch  81 / 1000  : Train-loss =  94.0087782338 , Val-loss =  101.842476614 , Time for epoch =  0.7153608798980713 s\n",
      "Epoch  82 / 1000  : Train-loss =  93.8082777731 , Val-loss =  101.536751489 , Time for epoch =  0.7196650505065918 s\n",
      "Epoch  83 / 1000  : Train-loss =  93.645233529 , Val-loss =  101.25432385 , Time for epoch =  0.7178256511688232 s\n",
      "Epoch  84 / 1000  : Train-loss =  93.5019224636 , Val-loss =  100.987168002 , Time for epoch =  0.7340610027313232 s\n",
      "Epoch  85 / 1000  : Train-loss =  93.3696272735 , Val-loss =  100.731084886 , Time for epoch =  0.7504081726074219 s\n",
      "Epoch  86 / 1000  : Train-loss =  93.2439186 , Val-loss =  100.483777643 , Time for epoch =  0.7430636882781982 s\n",
      "Epoch  87 / 1000  : Train-loss =  93.1224865566 , Val-loss =  100.24391388 , Time for epoch =  0.7457623481750488 s\n",
      "Epoch  88 / 1000  : Train-loss =  93.015381251 , Val-loss =  99.92783235 , Time for epoch =  0.7672154903411865 s\n",
      "Epoch  89 / 1000  : Train-loss =  92.8341479389 , Val-loss =  99.6457716387 , Time for epoch =  0.8268413543701172 s\n",
      "Epoch  90 / 1000  : Train-loss =  92.6896709123 , Val-loss =  99.389199125 , Time for epoch =  0.8297538757324219 s\n",
      "Epoch  91 / 1000  : Train-loss =  92.5650102255 , Val-loss =  99.1479949348 , Time for epoch =  0.8290238380432129 s\n",
      "Epoch  92 / 1000  : Train-loss =  92.450925254 , Val-loss =  98.9176637244 , Time for epoch =  0.8316624164581299 s\n",
      "Epoch  93 / 1000  : Train-loss =  92.3427439913 , Val-loss =  98.695705373 , Time for epoch =  0.7701668739318848 s\n",
      "Epoch  94 / 1000  : Train-loss =  92.2380292571 , Val-loss =  98.4806711451 , Time for epoch =  0.7652461528778076 s\n",
      "Epoch  95 / 1000  : Train-loss =  92.1354975162 , Val-loss =  98.2716692936 , Time for epoch =  0.7630932331085205 s\n",
      "Epoch  96 / 1000  : Train-loss =  92.0344679662 , Val-loss =  98.0646265768 , Time for epoch =  0.7588117122650146 s\n",
      "Epoch  97 / 1000  : Train-loss =  91.9345771135 , Val-loss =  97.8394414374 , Time for epoch =  0.75396728515625 s\n",
      "Epoch  98 / 1000  : Train-loss =  91.8356294819 , Val-loss =  97.6198972361 , Time for epoch =  0.7260947227478027 s\n",
      "Epoch  99 / 1000  : Train-loss =  91.7375190627 , Val-loss =  97.4056621246 , Time for epoch =  0.7647030353546143 s\n",
      "Epoch  100 / 1000  : Train-loss =  91.6401877991 , Val-loss =  97.1964453371 , Time for epoch =  0.8794171810150146 s\n",
      "Epoch  101 / 1000  : Train-loss =  91.5436035434 , Val-loss =  96.9919857295 , Time for epoch =  0.726891040802002 s\n",
      "Epoch  102 / 1000  : Train-loss =  91.4477482866 , Val-loss =  96.7920451222 , Time for epoch =  0.8118352890014648 s\n",
      "Epoch  103 / 1000  : Train-loss =  91.3526118233 , Val-loss =  96.596404214 , Time for epoch =  0.7036454677581787 s\n",
      "Epoch  104 / 1000  : Train-loss =  91.2581883032 , Val-loss =  96.4048598964 , Time for epoch =  0.7453246116638184 s\n",
      "Epoch  105 / 1000  : Train-loss =  91.1644743232 , Val-loss =  96.2172233499 , Time for epoch =  0.7621345520019531 s\n",
      "Epoch  106 / 1000  : Train-loss =  91.0714678507 , Val-loss =  96.0333185943 , Time for epoch =  0.8098313808441162 s\n",
      "Epoch  107 / 1000  : Train-loss =  90.9791675973 , Val-loss =  95.8529813169 , Time for epoch =  0.8143579959869385 s\n",
      "Epoch  108 / 1000  : Train-loss =  90.8875726439 , Val-loss =  95.6760578819 , Time for epoch =  0.8256988525390625 s\n",
      "Epoch  109 / 1000  : Train-loss =  90.7966822083 , Val-loss =  95.5024044672 , Time for epoch =  0.6922764778137207 s\n",
      "Epoch  110 / 1000  : Train-loss =  90.7064954951 , Val-loss =  95.3318862969 , Time for epoch =  0.8361737728118896 s\n",
      "Epoch  111 / 1000  : Train-loss =  90.6170115973 , Val-loss =  95.1643769506 , Time for epoch =  0.760690450668335 s\n",
      "Epoch  112 / 1000  : Train-loss =  90.5282294308 , Val-loss =  94.9997577367 , Time for epoch =  0.7488939762115479 s\n",
      "Epoch  113 / 1000  : Train-loss =  90.4401476899 , Val-loss =  94.8379171211 , Time for epoch =  0.7706444263458252 s\n",
      "Epoch  114 / 1000  : Train-loss =  90.3527648182 , Val-loss =  94.6787502038 , Time for epoch =  0.7543323040008545 s\n",
      "Epoch  115 / 1000  : Train-loss =  90.2660789916 , Val-loss =  94.5221582401 , Time for epoch =  0.7560861110687256 s\n",
      "Epoch  116 / 1000  : Train-loss =  90.1800881085 , Val-loss =  94.3680481996 , Time for epoch =  0.7534005641937256 s\n",
      "Epoch  117 / 1000  : Train-loss =  90.0947897866 , Val-loss =  94.2163323617 , Time for epoch =  0.7591941356658936 s\n",
      "Epoch  118 / 1000  : Train-loss =  90.0101813649 , Val-loss =  94.0669279422 , Time for epoch =  0.7546086311340332 s\n",
      "Epoch  119 / 1000  : Train-loss =  89.9262599092 , Val-loss =  93.91975675 , Time for epoch =  0.7523789405822754 s\n",
      "Epoch  120 / 1000  : Train-loss =  89.8430222205 , Val-loss =  93.7747448703 , Time for epoch =  0.7523550987243652 s\n",
      "Epoch  121 / 1000  : Train-loss =  89.760464845 , Val-loss =  93.6318223716 , Time for epoch =  0.7632744312286377 s\n",
      "Epoch  122 / 1000  : Train-loss =  89.6785840867 , Val-loss =  93.4909230362 , Time for epoch =  0.8314619064331055 s\n",
      "Epoch  123 / 1000  : Train-loss =  89.5973760202 , Val-loss =  93.3519841103 , Time for epoch =  0.8280761241912842 s\n",
      "Epoch  124 / 1000  : Train-loss =  89.5168365043 , Val-loss =  93.2149460729 , Time for epoch =  1.0612640380859375 s\n",
      "Epoch  125 / 1000  : Train-loss =  89.4369611966 , Val-loss =  93.0797524226 , Time for epoch =  0.9834356307983398 s\n",
      "Epoch  126 / 1000  : Train-loss =  89.3577455675 , Val-loss =  92.9463494795 , Time for epoch =  1.2021613121032715 s\n",
      "Epoch  127 / 1000  : Train-loss =  89.2791849147 , Val-loss =  92.8146862021 , Time for epoch =  0.9655575752258301 s\n",
      "Epoch  128 / 1000  : Train-loss =  89.2012743773 , Val-loss =  92.6847140176 , Time for epoch =  0.701716423034668 s\n",
      "Epoch  129 / 1000  : Train-loss =  89.1240089496 , Val-loss =  92.5563866639 , Time for epoch =  0.6872577667236328 s\n",
      "Epoch  130 / 1000  : Train-loss =  89.0473834946 , Val-loss =  92.429660044 , Time for epoch =  0.7002546787261963 s\n",
      "Epoch  131 / 1000  : Train-loss =  88.9713927567 , Val-loss =  92.30449209 , Time for epoch =  0.7318367958068848 s\n",
      "Epoch  132 / 1000  : Train-loss =  88.8960313743 , Val-loss =  92.1808426371 , Time for epoch =  0.8182883262634277 s\n",
      "Epoch  133 / 1000  : Train-loss =  88.8212938916 , Val-loss =  92.0586733065 , Time for epoch =  0.7571260929107666 s\n",
      "Epoch  134 / 1000  : Train-loss =  88.7471747697 , Val-loss =  91.9379473964 , Time for epoch =  0.8066024780273438 s\n",
      "Epoch  135 / 1000  : Train-loss =  88.6736683977 , Val-loss =  91.8186297807 , Time for epoch =  0.8085713386535645 s\n",
      "Epoch  136 / 1000  : Train-loss =  88.6007691022 , Val-loss =  91.7006868148 , Time for epoch =  0.7743830680847168 s\n",
      "Epoch  137 / 1000  : Train-loss =  88.5260371673 , Val-loss =  91.5768609784 , Time for epoch =  0.7966468334197998 s\n",
      "Epoch  138 / 1000  : Train-loss =  88.4481003453 , Val-loss =  91.45403578 , Time for epoch =  0.7519123554229736 s\n",
      "Epoch  139 / 1000  : Train-loss =  88.3708224011 , Val-loss =  91.3325588515 , Time for epoch =  0.8020720481872559 s\n",
      "Epoch  140 / 1000  : Train-loss =  88.2942811719 , Val-loss =  91.2124606408 , Time for epoch =  0.7647614479064941 s\n",
      "Epoch  141 / 1000  : Train-loss =  88.2185111291 , Val-loss =  91.0937394633 , Time for epoch =  0.7663946151733398 s\n",
      "Epoch  142 / 1000  : Train-loss =  88.1435222794 , Val-loss =  90.9763814641 , Time for epoch =  0.7631280422210693 s\n",
      "Epoch  143 / 1000  : Train-loss =  88.0693115459 , Val-loss =  90.8603665894 , Time for epoch =  0.807300329208374 s\n",
      "Epoch  144 / 1000  : Train-loss =  87.99586905 , Val-loss =  90.7456717062 , Time for epoch =  0.7530906200408936 s\n",
      "Epoch  145 / 1000  : Train-loss =  87.9231815485 , Val-loss =  90.6322723172 , Time for epoch =  0.8006322383880615 s\n",
      "Epoch  146 / 1000  : Train-loss =  87.8512343033 , Val-loss =  90.5201435003 , Time for epoch =  0.7578728199005127 s\n",
      "Epoch  147 / 1000  : Train-loss =  87.7800120882 , Val-loss =  90.4092604195 , Time for epoch =  0.7520492076873779 s\n",
      "Epoch  148 / 1000  : Train-loss =  87.7094997226 , Val-loss =  90.2995985966 , Time for epoch =  0.788679838180542 s\n",
      "Epoch  149 / 1000  : Train-loss =  87.6396823438 , Val-loss =  90.191134051 , Time for epoch =  0.7943520545959473 s\n",
      "Epoch  150 / 1000  : Train-loss =  87.5705455387 , Val-loss =  90.0838433664 , Time for epoch =  0.7904222011566162 s\n",
      "Epoch  151 / 1000  : Train-loss =  87.5020753973 , Val-loss =  89.9777037168 , Time for epoch =  0.7654449939727783 s\n",
      "Epoch  152 / 1000  : Train-loss =  87.4342585257 , Val-loss =  89.8726928709 , Time for epoch =  0.7711572647094727 s\n",
      "Epoch  153 / 1000  : Train-loss =  87.3670820374 , Val-loss =  89.7687891844 , Time for epoch =  0.7526180744171143 s\n",
      "Epoch  154 / 1000  : Train-loss =  87.3005335344 , Val-loss =  89.6659715865 , Time for epoch =  0.7965013980865479 s\n",
      "Epoch  155 / 1000  : Train-loss =  87.2346010825 , Val-loss =  89.5642195626 , Time for epoch =  0.9129996299743652 s\n",
      "Epoch  156 / 1000  : Train-loss =  87.1692731866 , Val-loss =  89.463513137 , Time for epoch =  1.1586337089538574 s\n",
      "Epoch  157 / 1000  : Train-loss =  87.1045387646 , Val-loss =  89.3638328541 , Time for epoch =  0.9973981380462646 s\n",
      "Epoch  158 / 1000  : Train-loss =  87.0403871232 , Val-loss =  89.2651597604 , Time for epoch =  0.76507568359375 s\n",
      "Epoch  159 / 1000  : Train-loss =  86.9768079349 , Val-loss =  89.1674753868 , Time for epoch =  0.7519614696502686 s\n",
      "Epoch  160 / 1000  : Train-loss =  86.9137912157 , Val-loss =  89.0707617315 , Time for epoch =  0.7063543796539307 s\n",
      "Epoch  161 / 1000  : Train-loss =  86.8513273057 , Val-loss =  88.9750012437 , Time for epoch =  0.7878680229187012 s\n",
      "Epoch  162 / 1000  : Train-loss =  86.7894068499 , Val-loss =  88.8801768075 , Time for epoch =  0.7706024646759033 s\n",
      "Epoch  163 / 1000  : Train-loss =  86.7280207806 , Val-loss =  88.7862717272 , Time for epoch =  0.8733513355255127 s\n",
      "Epoch  164 / 1000  : Train-loss =  86.6671603019 , Val-loss =  88.6932697124 , Time for epoch =  0.8249502182006836 s\n",
      "Epoch  165 / 1000  : Train-loss =  86.6068168745 , Val-loss =  88.6011548641 , Time for epoch =  0.7427229881286621 s\n",
      "Epoch  166 / 1000  : Train-loss =  86.5469822015 , Val-loss =  88.5099116619 , Time for epoch =  0.8135960102081299 s\n",
      "Epoch  167 / 1000  : Train-loss =  86.4876482164 , Val-loss =  88.4195249506 , Time for epoch =  0.7171308994293213 s\n",
      "Epoch  168 / 1000  : Train-loss =  86.4288070703 , Val-loss =  88.3299799284 , Time for epoch =  0.713202953338623 s\n",
      "Epoch  169 / 1000  : Train-loss =  86.3704511215 , Val-loss =  88.2412621348 , Time for epoch =  0.7144389152526855 s\n",
      "Epoch  170 / 1000  : Train-loss =  86.3096314839 , Val-loss =  88.1459175433 , Time for epoch =  0.7358782291412354 s\n",
      "Epoch  171 / 1000  : Train-loss =  86.2357364352 , Val-loss =  88.0525513629 , Time for epoch =  0.7600195407867432 s\n",
      "Epoch  172 / 1000  : Train-loss =  86.1623637964 , Val-loss =  87.9592047277 , Time for epoch =  0.7069692611694336 s\n",
      "Epoch  173 / 1000  : Train-loss =  86.0896135044 , Val-loss =  87.8662012936 , Time for epoch =  0.7099924087524414 s\n",
      "Epoch  174 / 1000  : Train-loss =  86.0176913612 , Val-loss =  87.7695254142 , Time for epoch =  0.7015578746795654 s\n",
      "Epoch  175 / 1000  : Train-loss =  85.9466955626 , Val-loss =  87.668897416 , Time for epoch =  0.730919599533081 s\n",
      "Epoch  176 / 1000  : Train-loss =  85.8766638123 , Val-loss =  87.5691795114 , Time for epoch =  0.7441427707672119 s\n",
      "Epoch  177 / 1000  : Train-loss =  85.8076004487 , Val-loss =  87.4703958752 , Time for epoch =  0.7155437469482422 s\n",
      "Epoch  178 / 1000  : Train-loss =  85.7394918493 , Val-loss =  87.3725558248 , Time for epoch =  0.7564041614532471 s\n",
      "Epoch  179 / 1000  : Train-loss =  85.6723151423 , Val-loss =  87.2756599223 , Time for epoch =  0.7689926624298096 s\n",
      "Epoch  180 / 1000  : Train-loss =  85.6060431018 , Val-loss =  87.1797035329 , Time for epoch =  0.7620506286621094 s\n",
      "Epoch  181 / 1000  : Train-loss =  85.5406468727 , Val-loss =  87.0846789095 , Time for epoch =  0.7588562965393066 s\n",
      "Epoch  182 / 1000  : Train-loss =  85.4760974603 , Val-loss =  86.9905764201 , Time for epoch =  0.7590320110321045 s\n",
      "Epoch  183 / 1000  : Train-loss =  85.4123665224 , Val-loss =  86.8973852766 , Time for epoch =  0.7583706378936768 s\n",
      "Epoch  184 / 1000  : Train-loss =  85.3647892464 , Val-loss =  86.7205707403 , Time for epoch =  0.7979922294616699 s\n",
      "Epoch  185 / 1000  : Train-loss =  85.2590744655 , Val-loss =  86.5730308798 , Time for epoch =  0.7777245044708252 s\n",
      "Epoch  186 / 1000  : Train-loss =  85.1743254793 , Val-loss =  86.4434083897 , Time for epoch =  0.9326777458190918 s\n",
      "Epoch  187 / 1000  : Train-loss =  85.1015229092 , Val-loss =  86.3250450766 , Time for epoch =  0.9707703590393066 s\n",
      "Epoch  188 / 1000  : Train-loss =  85.0357282519 , Val-loss =  86.2140140594 , Time for epoch =  0.9670975208282471 s\n",
      "Epoch  189 / 1000  : Train-loss =  84.9741630967 , Val-loss =  86.1079846473 , Time for epoch =  0.9560275077819824 s\n",
      "Epoch  190 / 1000  : Train-loss =  84.9109222556 , Val-loss =  86.0021994138 , Time for epoch =  1.0410020351409912 s\n",
      "Epoch  191 / 1000  : Train-loss =  84.8481847194 , Val-loss =  85.8989312759 , Time for epoch =  1.1784582138061523 s\n",
      "Epoch  192 / 1000  : Train-loss =  84.7866529247 , Val-loss =  85.7977950826 , Time for epoch =  1.116758108139038 s\n",
      "Epoch  193 / 1000  : Train-loss =  84.726083351 , Val-loss =  85.6985360607 , Time for epoch =  0.7110176086425781 s\n",
      "Epoch  194 / 1000  : Train-loss =  84.6663269418 , Val-loss =  85.6009823622 , Time for epoch =  0.7166032791137695 s\n",
      "Epoch  195 / 1000  : Train-loss =  84.6072899647 , Val-loss =  85.5050120027 , Time for epoch =  0.7153949737548828 s\n",
      "Epoch  196 / 1000  : Train-loss =  84.5489113616 , Val-loss =  85.4105333387 , Time for epoch =  0.711503267288208 s\n",
      "Epoch  197 / 1000  : Train-loss =  84.4911494973 , Val-loss =  85.3174735209 , Time for epoch =  0.7147083282470703 s\n",
      "Epoch  198 / 1000  : Train-loss =  84.4339743867 , Val-loss =  85.225771643 , Time for epoch =  0.713080883026123 s\n",
      "Epoch  199 / 1000  : Train-loss =  84.3773631248 , Val-loss =  85.1353746568 , Time for epoch =  0.7021405696868896 s\n",
      "Epoch  200 / 1000  : Train-loss =  84.32129719 , Val-loss =  85.0462349199 , Time for epoch =  0.7204864025115967 s\n",
      "Epoch  201 / 1000  : Train-loss =  84.2657608479 , Val-loss =  84.9583087074 , Time for epoch =  0.7115011215209961 s\n",
      "Epoch  202 / 1000  : Train-loss =  84.2107401993 , Val-loss =  84.8715552949 , Time for epoch =  0.7212967872619629 s\n",
      "Epoch  203 / 1000  : Train-loss =  84.1562226094 , Val-loss =  84.7859363796 , Time for epoch =  0.7360849380493164 s\n",
      "Epoch  204 / 1000  : Train-loss =  84.1021963589 , Val-loss =  84.7014157053 , Time for epoch =  0.7518036365509033 s\n",
      "Epoch  205 / 1000  : Train-loss =  84.0486504287 , Val-loss =  84.615348613 , Time for epoch =  0.7806386947631836 s\n",
      "Epoch  206 / 1000  : Train-loss =  83.9955743632 , Val-loss =  84.5258101145 , Time for epoch =  0.7683515548706055 s\n",
      "Epoch  207 / 1000  : Train-loss =  83.9429581797 , Val-loss =  84.4374230654 , Time for epoch =  0.7113029956817627 s\n",
      "Epoch  208 / 1000  : Train-loss =  83.8907923071 , Val-loss =  84.3501524655 , Time for epoch =  0.7004556655883789 s\n",
      "Epoch  209 / 1000  : Train-loss =  83.8390675416 , Val-loss =  84.263964814 , Time for epoch =  0.6945111751556396 s\n",
      "Epoch  210 / 1000  : Train-loss =  83.7877750133 , Val-loss =  84.1788280208 , Time for epoch =  0.6880781650543213 s\n",
      "Epoch  211 / 1000  : Train-loss =  83.7369061599 , Val-loss =  84.0947113272 , Time for epoch =  0.7100086212158203 s\n",
      "Epoch  212 / 1000  : Train-loss =  83.686452705 , Val-loss =  84.0115852342 , Time for epoch =  0.6889379024505615 s\n",
      "Epoch  213 / 1000  : Train-loss =  83.6364066395 , Val-loss =  83.9294214354 , Time for epoch =  0.7062344551086426 s\n",
      "Epoch  214 / 1000  : Train-loss =  83.5867602053 , Val-loss =  83.8481927561 , Time for epoch =  0.699009895324707 s\n",
      "Epoch  215 / 1000  : Train-loss =  83.5375058815 , Val-loss =  83.7678730955 , Time for epoch =  0.7076065540313721 s\n",
      "Epoch  216 / 1000  : Train-loss =  83.4886363706 , Val-loss =  83.6884373725 , Time for epoch =  0.7604873180389404 s\n",
      "Epoch  217 / 1000  : Train-loss =  83.4401445873 , Val-loss =  83.6098614754 , Time for epoch =  0.7568016052246094 s\n",
      "Epoch  218 / 1000  : Train-loss =  83.3920236472 , Val-loss =  83.5321222143 , Time for epoch =  0.7846267223358154 s\n",
      "Epoch  219 / 1000  : Train-loss =  83.3442668575 , Val-loss =  83.455197276 , Time for epoch =  0.7762770652770996 s\n",
      "Epoch  220 / 1000  : Train-loss =  83.2968677069 , Val-loss =  83.3790651815 , Time for epoch =  0.7725112438201904 s\n",
      "Epoch  221 / 1000  : Train-loss =  83.2498198579 , Val-loss =  83.3037052464 , Time for epoch =  0.7512972354888916 s\n",
      "Epoch  222 / 1000  : Train-loss =  83.2031171384 , Val-loss =  83.2290975429 , Time for epoch =  0.7509057521820068 s\n",
      "Epoch  223 / 1000  : Train-loss =  83.1567535343 , Val-loss =  83.1552228641 , Time for epoch =  3.95162034034729 s\n",
      "Epoch  224 / 1000  : Train-loss =  83.1107231829 , Val-loss =  83.0820626905 , Time for epoch =  0.7457361221313477 s\n",
      "Epoch  225 / 1000  : Train-loss =  83.0650203663 , Val-loss =  83.0095991578 , Time for epoch =  0.7488715648651123 s\n",
      "Epoch  226 / 1000  : Train-loss =  83.0196395054 , Val-loss =  82.937815027 , Time for epoch =  0.7460331916809082 s\n",
      "Epoch  227 / 1000  : Train-loss =  82.9745751545 , Val-loss =  82.8666936559 , Time for epoch =  0.7541866302490234 s\n",
      "Epoch  228 / 1000  : Train-loss =  82.9298219957 , Val-loss =  82.7962189715 , Time for epoch =  0.7474484443664551 s\n",
      "Epoch  229 / 1000  : Train-loss =  82.8853748341 , Val-loss =  82.7263754453 , Time for epoch =  0.7637276649475098 s\n",
      "Epoch  230 / 1000  : Train-loss =  82.8412285932 , Val-loss =  82.6571480683 , Time for epoch =  0.751708984375 s\n",
      "Epoch  231 / 1000  : Train-loss =  82.7973783106 , Val-loss =  82.5885223283 , Time for epoch =  0.7511892318725586 s\n",
      "Epoch  232 / 1000  : Train-loss =  82.7538191337 , Val-loss =  82.5204841881 , Time for epoch =  0.7534537315368652 s\n",
      "Epoch  233 / 1000  : Train-loss =  82.7105463159 , Val-loss =  82.4530200648 , Time for epoch =  0.7479007244110107 s\n",
      "Epoch  234 / 1000  : Train-loss =  82.6675552127 , Val-loss =  82.3861168103 , Time for epoch =  0.7474513053894043 s\n",
      "Epoch  235 / 1000  : Train-loss =  82.6248412789 , Val-loss =  82.3197616926 , Time for epoch =  0.7461395263671875 s\n",
      "Epoch  236 / 1000  : Train-loss =  82.5824000643 , Val-loss =  82.2539423782 , Time for epoch =  0.7491636276245117 s\n",
      "Epoch  237 / 1000  : Train-loss =  82.5402272116 , Val-loss =  82.188646915 , Time for epoch =  0.7467701435089111 s\n",
      "Epoch  238 / 1000  : Train-loss =  82.4983184525 , Val-loss =  82.1238637171 , Time for epoch =  0.742032527923584 s\n",
      "Epoch  239 / 1000  : Train-loss =  82.4566696057 , Val-loss =  82.0595815489 , Time for epoch =  0.7605791091918945 s\n",
      "Epoch  240 / 1000  : Train-loss =  82.4152765736 , Val-loss =  81.9957895112 , Time for epoch =  0.748732328414917 s\n",
      "Epoch  241 / 1000  : Train-loss =  82.3741353402 , Val-loss =  81.9324770273 , Time for epoch =  0.7434241771697998 s\n",
      "Epoch  242 / 1000  : Train-loss =  82.3332419684 , Val-loss =  81.8696338302 , Time for epoch =  0.7084856033325195 s\n",
      "Epoch  243 / 1000  : Train-loss =  82.2925925976 , Val-loss =  81.80724995 , Time for epoch =  0.7424581050872803 s\n",
      "Epoch  244 / 1000  : Train-loss =  82.2521834418 , Val-loss =  81.7453157021 , Time for epoch =  0.7194187641143799 s\n",
      "Epoch  245 / 1000  : Train-loss =  82.2120107873 , Val-loss =  81.6838216764 , Time for epoch =  0.7088119983673096 s\n",
      "Epoch  246 / 1000  : Train-loss =  82.1720709906 , Val-loss =  81.6227587261 , Time for epoch =  0.7085661888122559 s\n",
      "Epoch  247 / 1000  : Train-loss =  82.1323604767 , Val-loss =  81.5621179579 , Time for epoch =  0.7083907127380371 s\n",
      "Epoch  248 / 1000  : Train-loss =  82.0927433059 , Val-loss =  81.5013473221 , Time for epoch =  0.6998729705810547 s\n",
      "Epoch  249 / 1000  : Train-loss =  82.0535127767 , Val-loss =  81.4407303605 , Time for epoch =  0.7001705169677734 s\n",
      "Epoch  250 / 1000  : Train-loss =  82.0144130011 , Val-loss =  81.3803617974 , Time for epoch =  0.6923356056213379 s\n",
      "Epoch  251 / 1000  : Train-loss =  81.9754814371 , Val-loss =  81.3202955205 , Time for epoch =  0.6893565654754639 s\n",
      "Epoch  252 / 1000  : Train-loss =  81.936738666 , Val-loss =  81.2605610581 , Time for epoch =  0.6887133121490479 s\n",
      "Epoch  253 / 1000  : Train-loss =  81.8981953073 , Val-loss =  81.2011734284 , Time for epoch =  0.6915407180786133 s\n",
      "Epoch  254 / 1000  : Train-loss =  81.8598561178 , Val-loss =  81.1421390259 , Time for epoch =  0.8597805500030518 s\n",
      "Epoch  255 / 1000  : Train-loss =  81.8217224197 , Val-loss =  81.0834591386 , Time for epoch =  0.978459358215332 s\n",
      "Epoch  256 / 1000  : Train-loss =  81.7837935382 , Val-loss =  81.02513205 , Time for epoch =  1.0294067859649658 s\n",
      "Epoch  257 / 1000  : Train-loss =  81.7460676512 , Val-loss =  80.9671542938 , Time for epoch =  1.1689159870147705 s\n",
      "Epoch  258 / 1000  : Train-loss =  81.7085422913 , Val-loss =  80.9095214031 , Time for epoch =  0.8111362457275391 s\n",
      "Epoch  259 / 1000  : Train-loss =  81.6712146419 , Val-loss =  80.8522283577 , Time for epoch =  0.7612142562866211 s\n",
      "Epoch  260 / 1000  : Train-loss =  81.6340817112 , Val-loss =  80.7952698497 , Time for epoch =  0.7136938571929932 s\n",
      "Epoch  261 / 1000  : Train-loss =  81.5971404342 , Val-loss =  80.7386404422 , Time for epoch =  0.7228176593780518 s\n",
      "Epoch  262 / 1000  : Train-loss =  81.5603877322 , Val-loss =  80.6823346627 , Time for epoch =  0.7453992366790771 s\n",
      "Epoch  263 / 1000  : Train-loss =  81.5238205464 , Val-loss =  80.6263470583 , Time for epoch =  0.771350622177124 s\n",
      "Epoch  264 / 1000  : Train-loss =  81.4874358571 , Val-loss =  80.5706722278 , Time for epoch =  0.7564918994903564 s\n",
      "Epoch  265 / 1000  : Train-loss =  81.4512306945 , Val-loss =  80.5153048402 , Time for epoch =  0.7529680728912354 s\n",
      "Epoch  266 / 1000  : Train-loss =  81.4152021437 , Val-loss =  80.4602396446 , Time for epoch =  0.7545032501220703 s\n",
      "Epoch  267 / 1000  : Train-loss =  81.3793473466 , Val-loss =  80.4054714757 , Time for epoch =  0.754413366317749 s\n",
      "Epoch  268 / 1000  : Train-loss =  81.3436635031 , Val-loss =  80.3509952564 , Time for epoch =  0.7543654441833496 s\n",
      "Epoch  269 / 1000  : Train-loss =  81.30814787 , Val-loss =  80.296805998 , Time for epoch =  0.7551195621490479 s\n",
      "Epoch  270 / 1000  : Train-loss =  81.2727977604 , Val-loss =  80.2428988001 , Time for epoch =  0.7584755420684814 s\n",
      "Epoch  271 / 1000  : Train-loss =  81.237610542 , Val-loss =  80.1892688497 , Time for epoch =  0.7553310394287109 s\n",
      "Epoch  272 / 1000  : Train-loss =  81.2025836358 , Val-loss =  80.1359114201 , Time for epoch =  0.7661495208740234 s\n",
      "Epoch  273 / 1000  : Train-loss =  81.1677145148 , Val-loss =  80.0828218691 , Time for epoch =  0.7517786026000977 s\n",
      "Epoch  274 / 1000  : Train-loss =  81.1330007021 , Val-loss =  80.0299956376 , Time for epoch =  0.7222597599029541 s\n",
      "Epoch  275 / 1000  : Train-loss =  81.0984397695 , Val-loss =  79.975241217 , Time for epoch =  0.7076601982116699 s\n",
      "Epoch  276 / 1000  : Train-loss =  81.0640293362 , Val-loss =  79.9196291158 , Time for epoch =  0.7046833038330078 s\n",
      "Epoch  277 / 1000  : Train-loss =  81.0297670673 , Val-loss =  79.8643093022 , Time for epoch =  0.7211618423461914 s\n",
      "Epoch  278 / 1000  : Train-loss =  80.9956506724 , Val-loss =  79.8092767176 , Time for epoch =  0.7282712459564209 s\n",
      "Epoch  279 / 1000  : Train-loss =  80.9616779043 , Val-loss =  79.7545263999 , Time for epoch =  0.7109251022338867 s\n",
      "Epoch  280 / 1000  : Train-loss =  80.9278465579 , Val-loss =  79.7000534812 , Time for epoch =  0.7480928897857666 s\n",
      "Epoch  281 / 1000  : Train-loss =  80.8941544688 , Val-loss =  79.6458531859 , Time for epoch =  0.7663581371307373 s\n",
      "Epoch  282 / 1000  : Train-loss =  80.860599512 , Val-loss =  79.5919208283 , Time for epoch =  0.727792501449585 s\n",
      "Epoch  283 / 1000  : Train-loss =  80.8271796015 , Val-loss =  79.5382518108 , Time for epoch =  0.7153339385986328 s\n",
      "Epoch  284 / 1000  : Train-loss =  80.7938926884 , Val-loss =  79.4848416215 , Time for epoch =  0.7401642799377441 s\n",
      "Epoch  285 / 1000  : Train-loss =  80.7607367602 , Val-loss =  79.4316858329 , Time for epoch =  0.7494332790374756 s\n",
      "Epoch  286 / 1000  : Train-loss =  80.72770984 , Val-loss =  79.3787800994 , Time for epoch =  0.74538254737854 s\n",
      "Epoch  287 / 1000  : Train-loss =  80.6948099855 , Val-loss =  79.3261201559 , Time for epoch =  0.7329061031341553 s\n",
      "Epoch  288 / 1000  : Train-loss =  80.6620352877 , Val-loss =  79.2737018155 , Time for epoch =  0.7197728157043457 s\n",
      "Epoch  289 / 1000  : Train-loss =  80.6293838707 , Val-loss =  79.2215209685 , Time for epoch =  0.7583370208740234 s\n",
      "Epoch  290 / 1000  : Train-loss =  80.5968538901 , Val-loss =  79.16957358 , Time for epoch =  0.7628946304321289 s\n",
      "Epoch  291 / 1000  : Train-loss =  80.5644435329 , Val-loss =  79.1178556887 , Time for epoch =  0.7462422847747803 s\n",
      "Epoch  292 / 1000  : Train-loss =  80.5321510163 , Val-loss =  79.0663634051 , Time for epoch =  0.7744286060333252 s\n",
      "Epoch  293 / 1000  : Train-loss =  80.499974587 , Val-loss =  79.0150929099 , Time for epoch =  0.744044303894043 s\n",
      "Epoch  294 / 1000  : Train-loss =  80.4679125206 , Val-loss =  78.9640404527 , Time for epoch =  0.7502679824829102 s\n",
      "Epoch  295 / 1000  : Train-loss =  80.4359631208 , Val-loss =  78.9132023503 , Time for epoch =  0.7437741756439209 s\n",
      "Epoch  296 / 1000  : Train-loss =  80.4041247189 , Val-loss =  78.8625749853 , Time for epoch =  0.7429370880126953 s\n",
      "Epoch  297 / 1000  : Train-loss =  80.3723956728 , Val-loss =  78.8121548049 , Time for epoch =  0.7438218593597412 s\n",
      "Epoch  298 / 1000  : Train-loss =  80.3407743669 , Val-loss =  78.7619383192 , Time for epoch =  0.8722054958343506 s\n",
      "Epoch  299 / 1000  : Train-loss =  80.309259211 , Val-loss =  78.7119221001 , Time for epoch =  0.8803048133850098 s\n",
      "Epoch  300 / 1000  : Train-loss =  80.2778486397 , Val-loss =  78.6621027799 , Time for epoch =  0.8550264835357666 s\n",
      "Epoch  301 / 1000  : Train-loss =  80.2465411126 , Val-loss =  78.6124770502 , Time for epoch =  0.8640954494476318 s\n",
      "Epoch  302 / 1000  : Train-loss =  80.2153351127 , Val-loss =  78.5630416603 , Time for epoch =  0.7993161678314209 s\n",
      "Epoch  303 / 1000  : Train-loss =  80.1842291466 , Val-loss =  78.5137934166 , Time for epoch =  0.6992275714874268 s\n",
      "Epoch  304 / 1000  : Train-loss =  80.1472656598 , Val-loss =  78.5006389127 , Time for epoch =  0.6983838081359863 s\n",
      "Epoch  305 / 1000  : Train-loss =  80.1305231637 , Val-loss =  78.4361605353 , Time for epoch =  0.7151265144348145 s\n",
      "Epoch  306 / 1000  : Train-loss =  80.0905427468 , Val-loss =  78.4143239856 , Time for epoch =  0.7430665493011475 s\n",
      "Epoch  307 / 1000  : Train-loss =  80.065842814 , Val-loss =  78.3823358449 , Time for epoch =  0.7407436370849609 s\n",
      "Epoch  308 / 1000  : Train-loss =  80.0388691231 , Val-loss =  78.3442699484 , Time for epoch =  0.7522962093353271 s\n",
      "Epoch  309 / 1000  : Train-loss =  80.0105660267 , Val-loss =  78.3025753953 , Time for epoch =  0.7393178939819336 s\n",
      "Epoch  310 / 1000  : Train-loss =  79.9815093647 , Val-loss =  78.2587229428 , Time for epoch =  0.7396423816680908 s\n",
      "Epoch  311 / 1000  : Train-loss =  79.9520454914 , Val-loss =  78.2135953423 , Time for epoch =  0.7418084144592285 s\n",
      "Epoch  312 / 1000  : Train-loss =  79.9223808434 , Val-loss =  78.1677225174 , Time for epoch =  0.7391645908355713 s\n",
      "Epoch  313 / 1000  : Train-loss =  79.892637574 , Val-loss =  78.1214228845 , Time for epoch =  0.7423415184020996 s\n",
      "Epoch  314 / 1000  : Train-loss =  79.8628874299 , Val-loss =  78.0748881343 , Time for epoch =  0.739983081817627 s\n",
      "Epoch  315 / 1000  : Train-loss =  79.8331721463 , Val-loss =  78.0282340392 , Time for epoch =  0.7400069236755371 s\n",
      "Epoch  316 / 1000  : Train-loss =  79.8035156468 , Val-loss =  77.9815308798 , Time for epoch =  0.7364993095397949 s\n",
      "Epoch  317 / 1000  : Train-loss =  79.7739313104 , Val-loss =  77.9348216554 , Time for epoch =  0.7416322231292725 s\n",
      "Epoch  318 / 1000  : Train-loss =  79.7444262899 , Val-loss =  77.8881329811 , Time for epoch =  0.7393338680267334 s\n",
      "Epoch  319 / 1000  : Train-loss =  79.7150040738 , Val-loss =  77.8414816061 , Time for epoch =  0.741161584854126 s\n",
      "Epoch  320 / 1000  : Train-loss =  79.6856660038 , Val-loss =  77.7948783141 , Time for epoch =  0.7382626533508301 s\n",
      "Epoch  321 / 1000  : Train-loss =  79.6564121732 , Val-loss =  77.7483302571 , Time for epoch =  0.7371642589569092 s\n",
      "Epoch  322 / 1000  : Train-loss =  79.6272419577 , Val-loss =  77.7018423533 , Time for epoch =  0.7538313865661621 s\n",
      "Epoch  323 / 1000  : Train-loss =  79.5981543282 , Val-loss =  77.6554181252 , Time for epoch =  0.7399744987487793 s\n",
      "Epoch  324 / 1000  : Train-loss =  79.5691480353 , Val-loss =  77.6090602036 , Time for epoch =  0.7381818294525146 s\n",
      "Epoch  325 / 1000  : Train-loss =  79.5402217179 , Val-loss =  77.5627706318 , Time for epoch =  0.7766284942626953 s\n",
      "Epoch  326 / 1000  : Train-loss =  79.5113739654 , Val-loss =  77.5165510497 , Time for epoch =  0.7469689846038818 s\n",
      "Epoch  327 / 1000  : Train-loss =  79.4826033551 , Val-loss =  77.4704028077 , Time for epoch =  0.7468018531799316 s\n",
      "Epoch  328 / 1000  : Train-loss =  79.4539084721 , Val-loss =  77.4243270362 , Time for epoch =  0.7381246089935303 s\n",
      "Epoch  329 / 1000  : Train-loss =  79.4252879217 , Val-loss =  77.3783246913 , Time for epoch =  0.736710786819458 s\n",
      "Epoch  330 / 1000  : Train-loss =  79.3967403349 , Val-loss =  77.3323965832 , Time for epoch =  0.7075786590576172 s\n",
      "Epoch  331 / 1000  : Train-loss =  79.3682643716 , Val-loss =  77.2865433964 , Time for epoch =  0.7129216194152832 s\n",
      "Epoch  332 / 1000  : Train-loss =  79.3398587221 , Val-loss =  77.2407657044 , Time for epoch =  0.7066254615783691 s\n",
      "Epoch  333 / 1000  : Train-loss =  79.3115221065 , Val-loss =  77.1950639795 , Time for epoch =  0.7075297832489014 s\n",
      "Epoch  334 / 1000  : Train-loss =  79.2832532748 , Val-loss =  77.1499205284 , Time for epoch =  0.7159571647644043 s\n",
      "Epoch  335 / 1000  : Train-loss =  79.2550510059 , Val-loss =  77.1049023965 , Time for epoch =  0.7027580738067627 s\n",
      "Epoch  336 / 1000  : Train-loss =  79.2269141069 , Val-loss =  77.0599605683 , Time for epoch =  0.7562544345855713 s\n",
      "Epoch  337 / 1000  : Train-loss =  79.1988414116 , Val-loss =  77.0150951428 , Time for epoch =  0.7480628490447998 s\n",
      "Epoch  338 / 1000  : Train-loss =  79.1708317799 , Val-loss =  76.9703061517 , Time for epoch =  0.750410795211792 s\n",
      "Epoch  339 / 1000  : Train-loss =  79.1428840967 , Val-loss =  76.9255935635 , Time for epoch =  0.7404346466064453 s\n",
      "Epoch  340 / 1000  : Train-loss =  79.1149972709 , Val-loss =  76.8809572884 , Time for epoch =  0.749626874923706 s\n",
      "Epoch  341 / 1000  : Train-loss =  79.0871702344 , Val-loss =  76.8363971813 , Time for epoch =  0.7431013584136963 s\n",
      "Epoch  342 / 1000  : Train-loss =  79.0594019415 , Val-loss =  76.7919130461 , Time for epoch =  0.7628586292266846 s\n",
      "Epoch  343 / 1000  : Train-loss =  79.0316913675 , Val-loss =  76.7475046388 , Time for epoch =  0.7506823539733887 s\n",
      "Epoch  344 / 1000  : Train-loss =  79.0040375087 , Val-loss =  76.7031716708 , Time for epoch =  0.7451176643371582 s\n",
      "Epoch  345 / 1000  : Train-loss =  78.9764393808 , Val-loss =  76.6589138119 , Time for epoch =  0.7456402778625488 s\n",
      "Epoch  346 / 1000  : Train-loss =  78.9488960188 , Val-loss =  76.6147306932 , Time for epoch =  0.7474288940429688 s\n",
      "Epoch  347 / 1000  : Train-loss =  78.9214064761 , Val-loss =  76.57062191 , Time for epoch =  0.7864491939544678 s\n",
      "Epoch  348 / 1000  : Train-loss =  78.8939698237 , Val-loss =  76.5265870239 , Time for epoch =  0.7935771942138672 s\n",
      "Epoch  349 / 1000  : Train-loss =  78.8665851499 , Val-loss =  76.4826255657 , Time for epoch =  0.7871930599212646 s\n",
      "Epoch  350 / 1000  : Train-loss =  78.8392515595 , Val-loss =  76.4387370373 , Time for epoch =  0.7952799797058105 s\n",
      "Epoch  351 / 1000  : Train-loss =  78.8119681733 , Val-loss =  76.3949209144 , Time for epoch =  0.750324010848999 s\n",
      "Epoch  352 / 1000  : Train-loss =  78.7847341277 , Val-loss =  76.3511766478 , Time for epoch =  0.7510275840759277 s\n",
      "Epoch  353 / 1000  : Train-loss =  78.7575485738 , Val-loss =  76.3075036659 , Time for epoch =  0.7410707473754883 s\n",
      "Epoch  354 / 1000  : Train-loss =  78.7304106775 , Val-loss =  76.2639013763 , Time for epoch =  0.7386510372161865 s\n",
      "Epoch  355 / 1000  : Train-loss =  78.7033196188 , Val-loss =  76.2203691676 , Time for epoch =  0.7426810264587402 s\n",
      "Epoch  356 / 1000  : Train-loss =  78.676274591 , Val-loss =  76.1769064109 , Time for epoch =  0.7299907207489014 s\n",
      "Epoch  357 / 1000  : Train-loss =  78.649274801 , Val-loss =  76.1335124613 , Time for epoch =  0.7136654853820801 s\n",
      "Epoch  358 / 1000  : Train-loss =  78.6223194684 , Val-loss =  76.0901866597 , Time for epoch =  0.7144877910614014 s\n",
      "Epoch  359 / 1000  : Train-loss =  78.5954078252 , Val-loss =  76.0469283338 , Time for epoch =  0.7075912952423096 s\n",
      "Epoch  360 / 1000  : Train-loss =  78.5685391156 , Val-loss =  76.0037367994 , Time for epoch =  0.7117598056793213 s\n",
      "Epoch  361 / 1000  : Train-loss =  78.5417125956 , Val-loss =  75.9606113617 , Time for epoch =  0.715811014175415 s\n",
      "Epoch  362 / 1000  : Train-loss =  78.5149275325 , Val-loss =  75.9175513166 , Time for epoch =  0.6981306076049805 s\n",
      "Epoch  363 / 1000  : Train-loss =  78.488183205 , Val-loss =  75.8745559516 , Time for epoch =  0.7504565715789795 s\n",
      "Epoch  364 / 1000  : Train-loss =  78.4614789024 , Val-loss =  75.8316245466 , Time for epoch =  0.7157084941864014 s\n",
      "Epoch  365 / 1000  : Train-loss =  78.4348139248 , Val-loss =  75.7887563754 , Time for epoch =  0.7081518173217773 s\n",
      "Epoch  366 / 1000  : Train-loss =  78.4081875825 , Val-loss =  75.7459507062 , Time for epoch =  0.6948239803314209 s\n",
      "Epoch  367 / 1000  : Train-loss =  78.3815991958 , Val-loss =  75.7032068024 , Time for epoch =  0.7031383514404297 s\n",
      "Epoch  368 / 1000  : Train-loss =  78.3545924373 , Val-loss =  75.6573234422 , Time for epoch =  0.7066652774810791 s\n",
      "Epoch  369 / 1000  : Train-loss =  78.3254461765 , Val-loss =  75.6111572256 , Time for epoch =  0.7123904228210449 s\n",
      "Epoch  370 / 1000  : Train-loss =  78.2963056018 , Val-loss =  75.5648602361 , Time for epoch =  0.7228007316589355 s\n",
      "Epoch  371 / 1000  : Train-loss =  78.2671961649 , Val-loss =  75.5185320806 , Time for epoch =  0.7120406627655029 s\n",
      "Epoch  372 / 1000  : Train-loss =  78.2381326804 , Val-loss =  75.4722306393 , Time for epoch =  0.7177817821502686 s\n",
      "Epoch  373 / 1000  : Train-loss =  78.2091232795 , Val-loss =  75.4259893562 , Time for epoch =  0.7630531787872314 s\n",
      "Epoch  374 / 1000  : Train-loss =  78.1801722108 , Val-loss =  75.3798273679 , Time for epoch =  0.7899391651153564 s\n",
      "Epoch  375 / 1000  : Train-loss =  78.1512814742 , Val-loss =  75.3337554359 , Time for epoch =  0.7887718677520752 s\n",
      "Epoch  376 / 1000  : Train-loss =  78.1224517714 , Val-loss =  75.2877794185 , Time for epoch =  0.8053827285766602 s\n",
      "Epoch  377 / 1000  : Train-loss =  78.0936830603 , Val-loss =  75.2419023017 , Time for epoch =  0.7953715324401855 s\n",
      "Epoch  378 / 1000  : Train-loss =  78.0649748761 , Val-loss =  75.1961253882 , Time for epoch =  0.7598061561584473 s\n",
      "Epoch  379 / 1000  : Train-loss =  78.0363265183 , Val-loss =  75.150448992 , Time for epoch =  0.747800350189209 s\n",
      "Epoch  380 / 1000  : Train-loss =  78.0077371587 , Val-loss =  75.1048728453 , Time for epoch =  0.7326796054840088 s\n",
      "Epoch  381 / 1000  : Train-loss =  77.9792059041 , Val-loss =  75.0593963366 , Time for epoch =  0.7454938888549805 s\n",
      "Epoch  382 / 1000  : Train-loss =  77.9507318321 , Val-loss =  75.01401865 , Time for epoch =  0.7323024272918701 s\n",
      "Epoch  383 / 1000  : Train-loss =  77.9223140119 , Val-loss =  74.9687388477 , Time for epoch =  0.7561366558074951 s\n",
      "Epoch  384 / 1000  : Train-loss =  77.8939515159 , Val-loss =  74.9235559176 , Time for epoch =  0.7096550464630127 s\n",
      "Epoch  385 / 1000  : Train-loss =  77.865643426 , Val-loss =  74.8784688029 , Time for epoch =  0.7074346542358398 s\n",
      "Epoch  386 / 1000  : Train-loss =  77.8373888373 , Val-loss =  74.8334764185 , Time for epoch =  0.7096805572509766 s\n",
      "Epoch  387 / 1000  : Train-loss =  77.80918686 , Val-loss =  74.7885776622 , Time for epoch =  0.7095239162445068 s\n",
      "Epoch  388 / 1000  : Train-loss =  77.78103662 , Val-loss =  74.7437714207 , Time for epoch =  0.7066113948822021 s\n",
      "Epoch  389 / 1000  : Train-loss =  77.7529372591 , Val-loss =  74.6990565743 , Time for epoch =  0.7028560638427734 s\n",
      "Epoch  390 / 1000  : Train-loss =  77.7248879353 , Val-loss =  74.6544319992 , Time for epoch =  0.6882579326629639 s\n",
      "Epoch  391 / 1000  : Train-loss =  77.6968878218 , Val-loss =  74.6098965699 , Time for epoch =  0.7004396915435791 s\n",
      "Epoch  392 / 1000  : Train-loss =  77.6689361077 , Val-loss =  74.5654491608 , Time for epoch =  0.7508666515350342 s\n",
      "Epoch  393 / 1000  : Train-loss =  77.6410319965 , Val-loss =  74.5210886469 , Time for epoch =  0.8027417659759521 s\n",
      "Epoch  394 / 1000  : Train-loss =  77.6131747065 , Val-loss =  74.4768139053 , Time for epoch =  0.7327439785003662 s\n",
      "Epoch  395 / 1000  : Train-loss =  77.5853634702 , Val-loss =  74.4325639767 , Time for epoch =  0.7054202556610107 s\n",
      "Epoch  396 / 1000  : Train-loss =  77.5575975337 , Val-loss =  74.3880639322 , Time for epoch =  0.6971664428710938 s\n",
      "Epoch  397 / 1000  : Train-loss =  77.5306298031 , Val-loss =  74.3438807391 , Time for epoch =  0.7030010223388672 s\n",
      "Epoch  398 / 1000  : Train-loss =  77.5039605799 , Val-loss =  74.2995800227 , Time for epoch =  0.6871383190155029 s\n",
      "Epoch  399 / 1000  : Train-loss =  77.4774150023 , Val-loss =  74.2551601567 , Time for epoch =  0.6891376972198486 s\n",
      "Epoch  400 / 1000  : Train-loss =  77.4508837522 , Val-loss =  74.2107082434 , Time for epoch =  0.691870927810669 s\n",
      "Epoch  401 / 1000  : Train-loss =  77.4243768791 , Val-loss =  74.1662740419 , Time for epoch =  0.6991918087005615 s\n",
      "Epoch  402 / 1000  : Train-loss =  77.3978998287 , Val-loss =  74.1218857616 , Time for epoch =  0.7010526657104492 s\n",
      "Epoch  403 / 1000  : Train-loss =  77.3714554049 , Val-loss =  74.0775591843 , Time for epoch =  0.7424848079681396 s\n",
      "Epoch  404 / 1000  : Train-loss =  77.3450448995 , Val-loss =  74.0333029291 , Time for epoch =  0.7397339344024658 s\n",
      "Epoch  405 / 1000  : Train-loss =  77.3186687412 , Val-loss =  73.9891214904 , Time for epoch =  0.7458615303039551 s\n",
      "Epoch  406 / 1000  : Train-loss =  77.2923268695 , Val-loss =  73.9450169891 , Time for epoch =  0.7346541881561279 s\n",
      "Epoch  407 / 1000  : Train-loss =  77.2660189484 , Val-loss =  73.9009842971 , Time for epoch =  0.7429177761077881 s\n",
      "Epoch  408 / 1000  : Train-loss =  77.2397444893 , Val-loss =  73.8564943484 , Time for epoch =  0.7446296215057373 s\n",
      "Epoch  409 / 1000  : Train-loss =  77.2135029213 , Val-loss =  73.8120899442 , Time for epoch =  0.7439596652984619 s\n",
      "Epoch  410 / 1000  : Train-loss =  77.1872936317 , Val-loss =  73.7677702089 , Time for epoch =  0.7556695938110352 s\n",
      "Epoch  411 / 1000  : Train-loss =  77.1606585419 , Val-loss =  73.7212572528 , Time for epoch =  0.7943527698516846 s\n",
      "Epoch  412 / 1000  : Train-loss =  77.1333000738 , Val-loss =  73.6725029079 , Time for epoch =  0.8655788898468018 s\n",
      "Epoch  413 / 1000  : Train-loss =  77.1058682463 , Val-loss =  73.6237526136 , Time for epoch =  0.8675732612609863 s\n",
      "Epoch  414 / 1000  : Train-loss =  77.0784677149 , Val-loss =  73.5750699193 , Time for epoch =  0.8690011501312256 s\n",
      "Epoch  415 / 1000  : Train-loss =  77.0511051503 , Val-loss =  73.5264903167 , Time for epoch =  0.917107343673706 s\n",
      "Epoch  416 / 1000  : Train-loss =  77.0237839561 , Val-loss =  73.4780332477 , Time for epoch =  0.707359790802002 s\n",
      "Epoch  417 / 1000  : Train-loss =  76.9965056797 , Val-loss =  73.4297089803 , Time for epoch =  0.7014575004577637 s\n",
      "Epoch  418 / 1000  : Train-loss =  76.9692708176 , Val-loss =  73.3815225459 , Time for epoch =  0.7043261528015137 s\n",
      "Epoch  419 / 1000  : Train-loss =  76.9420792748 , Val-loss =  73.3334759908 , Time for epoch =  0.7041366100311279 s\n",
      "Epoch  420 / 1000  : Train-loss =  76.9149306269 , Val-loss =  73.285569664 , Time for epoch =  0.7056467533111572 s\n",
      "Epoch  421 / 1000  : Train-loss =  76.887824269 , Val-loss =  73.2378029532 , Time for epoch =  0.7008318901062012 s\n",
      "Epoch  422 / 1000  : Train-loss =  76.8607595003 , Val-loss =  73.1901747052 , Time for epoch =  0.700998067855835 s\n",
      "Epoch  423 / 1000  : Train-loss =  76.8337355723 , Val-loss =  73.1426834658 , Time for epoch =  0.6999416351318359 s\n",
      "Epoch  424 / 1000  : Train-loss =  76.8067517154 , Val-loss =  73.0953276174 , Time for epoch =  0.7405531406402588 s\n",
      "Epoch  425 / 1000  : Train-loss =  76.7798071548 , Val-loss =  73.0481054569 , Time for epoch =  0.7393908500671387 s\n",
      "Epoch  426 / 1000  : Train-loss =  76.7529011182 , Val-loss =  73.0006921142 , Time for epoch =  0.7366600036621094 s\n",
      "Epoch  427 / 1000  : Train-loss =  76.726032841 , Val-loss =  72.9524856234 , Time for epoch =  0.7334432601928711 s\n",
      "Epoch  428 / 1000  : Train-loss =  76.6992015683 , Val-loss =  72.9044211353 , Time for epoch =  0.7417478561401367 s\n",
      "Epoch  429 / 1000  : Train-loss =  76.6724065566 , Val-loss =  72.8564966791 , Time for epoch =  0.7356548309326172 s\n",
      "Epoch  430 / 1000  : Train-loss =  76.6456470738 , Val-loss =  72.8087103013 , Time for epoch =  0.7536208629608154 s\n",
      "Epoch  431 / 1000  : Train-loss =  76.6189223998 , Val-loss =  72.7610600683 , Time for epoch =  0.7101469039916992 s\n",
      "Epoch  432 / 1000  : Train-loss =  76.5922318262 , Val-loss =  72.7135440679 , Time for epoch =  0.7293529510498047 s\n",
      "Epoch  433 / 1000  : Train-loss =  76.565574656 , Val-loss =  72.6661604104 , Time for epoch =  0.7343919277191162 s\n",
      "Epoch  434 / 1000  : Train-loss =  76.5392482557 , Val-loss =  72.6217761961 , Time for epoch =  0.7319226264953613 s\n",
      "Epoch  435 / 1000  : Train-loss =  76.513787174 , Val-loss =  72.5776653359 , Time for epoch =  0.733468770980835 s\n",
      "Epoch  436 / 1000  : Train-loss =  76.4883629581 , Val-loss =  72.5337412532 , Time for epoch =  0.7342424392700195 s\n",
      "Epoch  437 / 1000  : Train-loss =  76.4629659016 , Val-loss =  72.4899553388 , Time for epoch =  0.737234354019165 s\n",
      "Epoch  438 / 1000  : Train-loss =  76.4375903827 , Val-loss =  72.4462796848 , Time for epoch =  0.7302200794219971 s\n",
      "Epoch  439 / 1000  : Train-loss =  76.4110495069 , Val-loss =  72.3982693412 , Time for epoch =  0.7394647598266602 s\n",
      "Epoch  440 / 1000  : Train-loss =  76.3837303243 , Val-loss =  72.3500172483 , Time for epoch =  0.7345590591430664 s\n",
      "Epoch  441 / 1000  : Train-loss =  76.3564179647 , Val-loss =  72.3016977662 , Time for epoch =  0.73606276512146 s\n",
      "Epoch  442 / 1000  : Train-loss =  76.3291302446 , Val-loss =  72.2534081535 , Time for epoch =  0.7389636039733887 s\n",
      "Epoch  443 / 1000  : Train-loss =  76.3018766934 , Val-loss =  72.2052022123 , Time for epoch =  3.080507516860962 s\n",
      "Epoch  444 / 1000  : Train-loss =  76.2746621879 , Val-loss =  72.1571092729 , Time for epoch =  2.0340442657470703 s\n",
      "Epoch  445 / 1000  : Train-loss =  76.2474000381 , Val-loss =  72.1086467996 , Time for epoch =  0.6916828155517578 s\n",
      "Epoch  446 / 1000  : Train-loss =  76.2201210272 , Val-loss =  72.0601563494 , Time for epoch =  0.7197659015655518 s\n",
      "Epoch  447 / 1000  : Train-loss =  76.1928697914 , Val-loss =  72.0117215347 , Time for epoch =  0.6915409564971924 s\n",
      "Epoch  448 / 1000  : Train-loss =  76.1656542378 , Val-loss =  71.9633882204 , Time for epoch =  0.6975667476654053 s\n",
      "Epoch  449 / 1000  : Train-loss =  76.1384783086 , Val-loss =  71.9151810864 , Time for epoch =  0.6970877647399902 s\n",
      "Epoch  450 / 1000  : Train-loss =  76.1113437352 , Val-loss =  71.8671129361 , Time for epoch =  0.6984267234802246 s\n",
      "Epoch  451 / 1000  : Train-loss =  76.0846151905 , Val-loss =  71.821802993 , Time for epoch =  0.69856858253479 s\n",
      "Epoch  452 / 1000  : Train-loss =  76.0592018312 , Val-loss =  71.7795759498 , Time for epoch =  0.7015402317047119 s\n",
      "Epoch  453 / 1000  : Train-loss =  76.0339601745 , Val-loss =  71.7376296084 , Time for epoch =  0.7002043724060059 s\n",
      "Epoch  454 / 1000  : Train-loss =  76.0087538515 , Val-loss =  71.695865962 , Time for epoch =  0.6972823143005371 s\n",
      "Epoch  455 / 1000  : Train-loss =  75.9835723522 , Val-loss =  71.6542298262 , Time for epoch =  0.6973304748535156 s\n",
      "Epoch  456 / 1000  : Train-loss =  75.9584096233 , Val-loss =  71.6126899635 , Time for epoch =  0.6999661922454834 s\n",
      "Epoch  457 / 1000  : Train-loss =  75.9329853696 , Val-loss =  71.5703161172 , Time for epoch =  0.6995227336883545 s\n",
      "Epoch  458 / 1000  : Train-loss =  75.9071161071 , Val-loss =  71.5277314303 , Time for epoch =  0.702383279800415 s\n",
      "Epoch  459 / 1000  : Train-loss =  75.8812422976 , Val-loss =  71.4850631 , Time for epoch =  0.7130579948425293 s\n",
      "Epoch  460 / 1000  : Train-loss =  75.8553762657 , Val-loss =  71.4423815429 , Time for epoch =  0.7078824043273926 s\n",
      "Epoch  461 / 1000  : Train-loss =  75.8295245659 , Val-loss =  71.399725517 , Time for epoch =  0.7081952095031738 s\n",
      "Epoch  462 / 1000  : Train-loss =  75.8036905455 , Val-loss =  71.3571161434 , Time for epoch =  0.7053227424621582 s\n",
      "Epoch  463 / 1000  : Train-loss =  75.7778757738 , Val-loss =  71.3145647248 , Time for epoch =  0.6964492797851562 s\n",
      "Epoch  464 / 1000  : Train-loss =  75.7520808369 , Val-loss =  71.2720771029 , Time for epoch =  0.6952099800109863 s\n",
      "Epoch  465 / 1000  : Train-loss =  75.7263057802 , Val-loss =  71.2293266968 , Time for epoch =  0.6965599060058594 s\n",
      "Epoch  466 / 1000  : Train-loss =  75.7005503538 , Val-loss =  71.186513326 , Time for epoch =  0.6945946216583252 s\n",
      "Epoch  467 / 1000  : Train-loss =  75.6748141482 , Val-loss =  71.143768454 , Time for epoch =  0.72267746925354 s\n",
      "Epoch  468 / 1000  : Train-loss =  75.6490966698 , Val-loss =  71.1010917834 , Time for epoch =  0.7343845367431641 s\n",
      "Epoch  469 / 1000  : Train-loss =  75.6233973827 , Val-loss =  71.0584827159 , Time for epoch =  0.734201192855835 s\n",
      "Epoch  470 / 1000  : Train-loss =  75.5977157316 , Val-loss =  71.0159404891 , Time for epoch =  0.7452189922332764 s\n",
      "Epoch  471 / 1000  : Train-loss =  75.5720511542 , Val-loss =  70.9734642528 , Time for epoch =  0.7464573383331299 s\n",
      "Epoch  472 / 1000  : Train-loss =  75.5464030882 , Val-loss =  70.9310531109 , Time for epoch =  4.179332971572876 s\n",
      "Epoch  473 / 1000  : Train-loss =  75.5207709749 , Val-loss =  70.8887061456 , Time for epoch =  0.8065962791442871 s\n",
      "Epoch  474 / 1000  : Train-loss =  75.4951542613 , Val-loss =  70.846528709 , Time for epoch =  0.7026746273040771 s\n",
      "Epoch  475 / 1000  : Train-loss =  75.469552401 , Val-loss =  70.8050809019 , Time for epoch =  0.6928012371063232 s\n",
      "Epoch  476 / 1000  : Train-loss =  75.4439648543 , Val-loss =  70.7636887536 , Time for epoch =  0.6963589191436768 s\n",
      "Epoch  477 / 1000  : Train-loss =  75.4183910891 , Val-loss =  70.7223513961 , Time for epoch =  0.6949148178100586 s\n",
      "Epoch  478 / 1000  : Train-loss =  75.39283058 , Val-loss =  70.6810679672 , Time for epoch =  0.692136287689209 s\n",
      "Epoch  479 / 1000  : Train-loss =  75.367282809 , Val-loss =  70.6398376119 , Time for epoch =  0.7163605690002441 s\n",
      "Epoch  480 / 1000  : Train-loss =  75.341747265 , Val-loss =  70.5986594822 , Time for epoch =  0.7290852069854736 s\n",
      "Epoch  481 / 1000  : Train-loss =  75.316358808 , Val-loss =  70.5568080467 , Time for epoch =  0.7335042953491211 s\n",
      "Epoch  482 / 1000  : Train-loss =  75.289778061 , Val-loss =  70.5146978882 , Time for epoch =  0.7361946105957031 s\n",
      "Epoch  483 / 1000  : Train-loss =  75.2599058548 , Val-loss =  70.4579339867 , Time for epoch =  0.7382569313049316 s\n",
      "Epoch  484 / 1000  : Train-loss =  75.2318081754 , Val-loss =  70.4074259551 , Time for epoch =  0.7306504249572754 s\n",
      "Epoch  485 / 1000  : Train-loss =  75.2043321192 , Val-loss =  70.3604017882 , Time for epoch =  0.7339119911193848 s\n",
      "Epoch  486 / 1000  : Train-loss =  75.1772068695 , Val-loss =  70.315333458 , Time for epoch =  0.7093870639801025 s\n",
      "Epoch  487 / 1000  : Train-loss =  75.1502858338 , Val-loss =  70.2713779892 , Time for epoch =  0.6896858215332031 s\n",
      "Epoch  488 / 1000  : Train-loss =  75.1234888427 , Val-loss =  70.2280700761 , Time for epoch =  0.6975595951080322 s\n",
      "Epoch  489 / 1000  : Train-loss =  75.0967716966 , Val-loss =  70.1851526935 , Time for epoch =  0.6888582706451416 s\n",
      "Epoch  490 / 1000  : Train-loss =  75.0701098142 , Val-loss =  70.1424836715 , Time for epoch =  0.6927144527435303 s\n",
      "Epoch  491 / 1000  : Train-loss =  75.0434893569 , Val-loss =  70.099984158 , Time for epoch =  0.6970999240875244 s\n",
      "Epoch  492 / 1000  : Train-loss =  75.0169023821 , Val-loss =  70.0576101933 , Time for epoch =  0.7290923595428467 s\n",
      "Epoch  493 / 1000  : Train-loss =  74.9903441877 , Val-loss =  70.0153370379 , Time for epoch =  0.7399623394012451 s\n",
      "Epoch  494 / 1000  : Train-loss =  74.9638118552 , Val-loss =  69.9731505374 , Time for epoch =  0.7290253639221191 s\n",
      "Epoch  495 / 1000  : Train-loss =  74.93730345 , Val-loss =  69.9310423659 , Time for epoch =  0.7215194702148438 s\n",
      "Epoch  496 / 1000  : Train-loss =  74.9108175823 , Val-loss =  69.889007409 , Time for epoch =  0.6960234642028809 s\n",
      "Epoch  497 / 1000  : Train-loss =  74.8843531663 , Val-loss =  69.8470423235 , Time for epoch =  0.7071323394775391 s\n",
      "Epoch  498 / 1000  : Train-loss =  74.8579092873 , Val-loss =  69.8051447461 , Time for epoch =  0.7014899253845215 s\n",
      "Epoch  499 / 1000  : Train-loss =  74.8314851293 , Val-loss =  69.7633128586 , Time for epoch =  0.7159333229064941 s\n",
      "Epoch  500 / 1000  : Train-loss =  74.8051430501 , Val-loss =  69.7205894976 , Time for epoch =  0.7428011894226074 s\n",
      "Epoch  501 / 1000  : Train-loss =  74.777762339 , Val-loss =  69.6775284928 , Time for epoch =  0.7319214344024658 s\n",
      "Epoch  502 / 1000  : Train-loss =  74.7503850474 , Val-loss =  69.6343247694 , Time for epoch =  0.737067461013794 s\n",
      "Epoch  503 / 1000  : Train-loss =  74.7230275572 , Val-loss =  69.5910839823 , Time for epoch =  0.7358078956604004 s\n",
      "Epoch  504 / 1000  : Train-loss =  74.6956982914 , Val-loss =  69.5478631316 , Time for epoch =  0.7372949123382568 s\n",
      "Epoch  505 / 1000  : Train-loss =  74.6684013419 , Val-loss =  69.5046926429 , Time for epoch =  0.71622633934021 s\n",
      "Epoch  506 / 1000  : Train-loss =  74.641138448 , Val-loss =  69.4615884426 , Time for epoch =  0.7057514190673828 s\n",
      "Epoch  507 / 1000  : Train-loss =  74.613910076 , Val-loss =  69.418558556 , Time for epoch =  0.713768482208252 s\n",
      "Epoch  508 / 1000  : Train-loss =  74.5867160082 , Val-loss =  69.3729364815 , Time for epoch =  0.6925046443939209 s\n",
      "Epoch  509 / 1000  : Train-loss =  74.5593719936 , Val-loss =  69.3262127343 , Time for epoch =  0.7523238658905029 s\n",
      "Epoch  510 / 1000  : Train-loss =  74.5312355984 , Val-loss =  69.2793555761 , Time for epoch =  0.7306857109069824 s\n",
      "Epoch  511 / 1000  : Train-loss =  74.5031270351 , Val-loss =  69.2324902183 , Time for epoch =  0.7378666400909424 s\n",
      "Epoch  512 / 1000  : Train-loss =  74.4746187417 , Val-loss =  69.1841416241 , Time for epoch =  0.7313153743743896 s\n",
      "Epoch  513 / 1000  : Train-loss =  74.4453838912 , Val-loss =  69.1356095326 , Time for epoch =  0.7375397682189941 s\n",
      "Epoch  514 / 1000  : Train-loss =  74.4162011799 , Val-loss =  69.0870607975 , Time for epoch =  0.7339303493499756 s\n",
      "Epoch  515 / 1000  : Train-loss =  74.3870830593 , Val-loss =  69.0385842207 , Time for epoch =  0.7360339164733887 s\n",
      "Epoch  516 / 1000  : Train-loss =  74.3580352645 , Val-loss =  68.9902263295 , Time for epoch =  0.732496976852417 s\n",
      "Epoch  517 / 1000  : Train-loss =  74.3290599214 , Val-loss =  68.9420107894 , Time for epoch =  0.7339303493499756 s\n",
      "Epoch  518 / 1000  : Train-loss =  74.3001572305 , Val-loss =  68.8939489258 , Time for epoch =  0.7438788414001465 s\n",
      "Epoch  519 / 1000  : Train-loss =  74.2713263766 , Val-loss =  68.8460454231 , Time for epoch =  0.7363424301147461 s\n",
      "Epoch  520 / 1000  : Train-loss =  74.2425660199 , Val-loss =  68.798301407 , Time for epoch =  0.7361173629760742 s\n",
      "Epoch  521 / 1000  : Train-loss =  74.2138745596 , Val-loss =  68.7507161108 , Time for epoch =  0.7054393291473389 s\n",
      "Epoch  522 / 1000  : Train-loss =  74.1852502751 , Val-loss =  68.7032877748 , Time for epoch =  0.7075037956237793 s\n",
      "Epoch  523 / 1000  : Train-loss =  74.1566914014 , Val-loss =  68.656014131 , Time for epoch =  0.6940453052520752 s\n",
      "Epoch  524 / 1000  : Train-loss =  74.1281961688 , Val-loss =  68.6088926641 , Time for epoch =  0.723926305770874 s\n",
      "Epoch  525 / 1000  : Train-loss =  74.0997628236 , Val-loss =  68.5619207515 , Time for epoch =  0.7167837619781494 s\n",
      "Epoch  526 / 1000  : Train-loss =  74.0713896384 , Val-loss =  68.5150957379 , Time for epoch =  0.761481523513794 s\n",
      "Epoch  527 / 1000  : Train-loss =  74.0430749174 , Val-loss =  68.4684149749 , Time for epoch =  0.7369654178619385 s\n",
      "Epoch  528 / 1000  : Train-loss =  74.0148169976 , Val-loss =  68.421875842 , Time for epoch =  0.7432198524475098 s\n",
      "Epoch  529 / 1000  : Train-loss =  73.9866142502 , Val-loss =  68.375475757 , Time for epoch =  0.7381784915924072 s\n",
      "Epoch  530 / 1000  : Train-loss =  73.9588228287 , Val-loss =  68.3297365917 , Time for epoch =  0.7407450675964355 s\n",
      "Epoch  531 / 1000  : Train-loss =  73.931126379 , Val-loss =  68.2841876309 , Time for epoch =  0.7403678894042969 s\n",
      "Epoch  532 / 1000  : Train-loss =  73.9034772194 , Val-loss =  68.2387940542 , Time for epoch =  0.7397134304046631 s\n",
      "Epoch  533 / 1000  : Train-loss =  73.8758713938 , Val-loss =  68.1935363333 , Time for epoch =  0.7426018714904785 s\n",
      "Epoch  534 / 1000  : Train-loss =  73.8483062177 , Val-loss =  68.1484031274 , Time for epoch =  0.738745927810669 s\n",
      "Epoch  535 / 1000  : Train-loss =  73.8162267788 , Val-loss =  68.094524796 , Time for epoch =  0.7150237560272217 s\n",
      "Epoch  536 / 1000  : Train-loss =  73.7748727254 , Val-loss =  68.0387015992 , Time for epoch =  0.73600172996521 s\n",
      "Epoch  537 / 1000  : Train-loss =  73.7331380785 , Val-loss =  67.9808786299 , Time for epoch =  0.742929220199585 s\n",
      "Epoch  538 / 1000  : Train-loss =  73.69175067 , Val-loss =  67.921823005 , Time for epoch =  0.7422547340393066 s\n",
      "Epoch  539 / 1000  : Train-loss =  73.622882369 , Val-loss =  67.7861278594 , Time for epoch =  0.7341644763946533 s\n",
      "Epoch  540 / 1000  : Train-loss =  73.5715208776 , Val-loss =  67.6855103628 , Time for epoch =  0.7136540412902832 s\n",
      "Epoch  541 / 1000  : Train-loss =  73.5242315014 , Val-loss =  67.6034019198 , Time for epoch =  0.7354929447174072 s\n",
      "Epoch  542 / 1000  : Train-loss =  73.4793404206 , Val-loss =  67.5312606131 , Time for epoch =  0.7338166236877441 s\n",
      "Epoch  543 / 1000  : Train-loss =  73.4321582741 , Val-loss =  67.4623743823 , Time for epoch =  0.7303314208984375 s\n",
      "Epoch  544 / 1000  : Train-loss =  73.3852222116 , Val-loss =  67.3965457669 , Time for epoch =  0.7206828594207764 s\n",
      "Epoch  545 / 1000  : Train-loss =  73.3417224471 , Val-loss =  67.3321653953 , Time for epoch =  0.7335422039031982 s\n",
      "Epoch  546 / 1000  : Train-loss =  73.2998334973 , Val-loss =  67.2660606838 , Time for epoch =  0.7307584285736084 s\n",
      "Epoch  547 / 1000  : Train-loss =  73.2547449529 , Val-loss =  67.1983278397 , Time for epoch =  0.7282729148864746 s\n",
      "Epoch  548 / 1000  : Train-loss =  73.2111010318 , Val-loss =  67.1291809742 , Time for epoch =  0.740379810333252 s\n",
      "Epoch  549 / 1000  : Train-loss =  73.1689087423 , Val-loss =  67.06056677 , Time for epoch =  0.7480101585388184 s\n",
      "Epoch  550 / 1000  : Train-loss =  73.1283841304 , Val-loss =  66.9884643282 , Time for epoch =  0.7147948741912842 s\n",
      "Epoch  551 / 1000  : Train-loss =  73.0880036477 , Val-loss =  66.9165043244 , Time for epoch =  0.7027573585510254 s\n",
      "Epoch  552 / 1000  : Train-loss =  73.0492151643 , Val-loss =  66.8451427818 , Time for epoch =  0.7441384792327881 s\n",
      "Epoch  553 / 1000  : Train-loss =  73.0120070518 , Val-loss =  66.7745613522 , Time for epoch =  0.7357516288757324 s\n",
      "Epoch  554 / 1000  : Train-loss =  72.9780759001 , Val-loss =  66.7049580005 , Time for epoch =  0.7360363006591797 s\n",
      "Epoch  555 / 1000  : Train-loss =  72.947395245 , Val-loss =  66.6347247437 , Time for epoch =  0.7419037818908691 s\n",
      "Epoch  556 / 1000  : Train-loss =  72.9177293961 , Val-loss =  66.5655802252 , Time for epoch =  0.7400856018066406 s\n",
      "Epoch  557 / 1000  : Train-loss =  72.8894003516 , Val-loss =  66.4962686965 , Time for epoch =  0.7445969581604004 s\n",
      "Epoch  558 / 1000  : Train-loss =  72.8632531405 , Val-loss =  66.417323228 , Time for epoch =  0.7486827373504639 s\n",
      "Epoch  559 / 1000  : Train-loss =  72.83304865 , Val-loss =  66.3355153824 , Time for epoch =  0.7466704845428467 s\n",
      "Epoch  560 / 1000  : Train-loss =  72.8058640172 , Val-loss =  66.2519495016 , Time for epoch =  0.754204273223877 s\n",
      "Epoch  561 / 1000  : Train-loss =  72.7819520952 , Val-loss =  66.1719419414 , Time for epoch =  0.8395435810089111 s\n",
      "Epoch  562 / 1000  : Train-loss =  72.762263993 , Val-loss =  66.0986893744 , Time for epoch =  0.8755447864532471 s\n",
      "Epoch  563 / 1000  : Train-loss =  72.7475413327 , Val-loss =  66.0320697117 , Time for epoch =  0.9420995712280273 s\n",
      "Epoch  564 / 1000  : Train-loss =  72.7351177011 , Val-loss =  65.9647416608 , Time for epoch =  0.8708534240722656 s\n",
      "Epoch  565 / 1000  : Train-loss =  72.7236480459 , Val-loss =  65.8975638744 , Time for epoch =  0.8250236511230469 s\n",
      "Epoch  566 / 1000  : Train-loss =  72.713750009 , Val-loss =  65.8282103289 , Time for epoch =  0.7014265060424805 s\n",
      "Epoch  567 / 1000  : Train-loss =  72.7054269307 , Val-loss =  65.7619003005 , Time for epoch =  0.6921303272247314 s\n",
      "Epoch  568 / 1000  : Train-loss =  72.7037485744 , Val-loss =  65.6935200468 , Time for epoch =  0.7009644508361816 s\n",
      "Epoch  569 / 1000  : Train-loss =  72.7064082866 , Val-loss =  65.629504221 , Time for epoch =  0.7026674747467041 s\n",
      "Epoch  570 / 1000  : Train-loss =  72.7189768593 , Val-loss =  65.561072805 , Time for epoch =  0.701880693435669 s\n",
      "Epoch  571 / 1000  : Train-loss =  72.7359852257 , Val-loss =  65.4953462751 , Time for epoch =  0.6991136074066162 s\n",
      "Epoch  572 / 1000  : Train-loss =  72.7576844324 , Val-loss =  65.4340625041 , Time for epoch =  0.7137224674224854 s\n",
      "Epoch  573 / 1000  : Train-loss =  72.7749506967 , Val-loss =  65.374002753 , Time for epoch =  0.7293844223022461 s\n",
      "Epoch  574 / 1000  : Train-loss =  72.7841601119 , Val-loss =  65.3211091897 , Time for epoch =  0.7305557727813721 s\n",
      "Epoch  575 / 1000  : Train-loss =  72.7836826977 , Val-loss =  65.2738523308 , Time for epoch =  0.7301537990570068 s\n",
      "Epoch  576 / 1000  : Train-loss =  72.7679923155 , Val-loss =  65.2292359125 , Time for epoch =  0.7309629917144775 s\n",
      "Epoch  577 / 1000  : Train-loss =  72.7524006138 , Val-loss =  65.1881325861 , Time for epoch =  0.7339591979980469 s\n",
      "Epoch  578 / 1000  : Train-loss =  72.7363486428 , Val-loss =  65.1488612611 , Time for epoch =  0.7068803310394287 s\n",
      "Epoch  579 / 1000  : Train-loss =  72.7207945179 , Val-loss =  65.1106696752 , Time for epoch =  0.6932415962219238 s\n",
      "Epoch  580 / 1000  : Train-loss =  72.7050100664 , Val-loss =  65.0729147405 , Time for epoch =  0.7064311504364014 s\n",
      "Epoch  581 / 1000  : Train-loss =  72.688846555 , Val-loss =  65.0354703628 , Time for epoch =  0.7091410160064697 s\n",
      "Epoch  582 / 1000  : Train-loss =  72.6686532467 , Val-loss =  64.9980904483 , Time for epoch =  0.704918622970581 s\n",
      "Epoch  583 / 1000  : Train-loss =  72.6485555631 , Val-loss =  64.9612346491 , Time for epoch =  0.7029988765716553 s\n",
      "Epoch  584 / 1000  : Train-loss =  72.6281941775 , Val-loss =  64.9247146667 , Time for epoch =  0.6954946517944336 s\n",
      "Epoch  585 / 1000  : Train-loss =  72.6075638871 , Val-loss =  64.8884349157 , Time for epoch =  0.6990389823913574 s\n",
      "Epoch  586 / 1000  : Train-loss =  72.5866648177 , Val-loss =  64.8524542505 , Time for epoch =  0.6959912776947021 s\n",
      "Epoch  587 / 1000  : Train-loss =  72.5635140106 , Val-loss =  64.8158776141 , Time for epoch =  0.6951615810394287 s\n",
      "Epoch  588 / 1000  : Train-loss =  72.5352544705 , Val-loss =  64.7793446659 , Time for epoch =  0.7037243843078613 s\n",
      "Epoch  589 / 1000  : Train-loss =  72.5041352678 , Val-loss =  64.7440212073 , Time for epoch =  0.6875488758087158 s\n",
      "Epoch  590 / 1000  : Train-loss =  72.473217283 , Val-loss =  64.709316859 , Time for epoch =  0.7086920738220215 s\n",
      "Epoch  591 / 1000  : Train-loss =  72.4424585131 , Val-loss =  64.6749427289 , Time for epoch =  0.6957182884216309 s\n",
      "Epoch  592 / 1000  : Train-loss =  72.4118342963 , Val-loss =  64.6407557068 , Time for epoch =  0.7130005359649658 s\n",
      "Epoch  593 / 1000  : Train-loss =  72.3813283322 , Val-loss =  64.6066842848 , Time for epoch =  0.7055008411407471 s\n",
      "Epoch  594 / 1000  : Train-loss =  72.3509286265 , Val-loss =  64.5726921432 , Time for epoch =  0.6936948299407959 s\n",
      "Epoch  595 / 1000  : Train-loss =  72.3206255208 , Val-loss =  64.5387602709 , Time for epoch =  0.7030174732208252 s\n",
      "Epoch  596 / 1000  : Train-loss =  72.2904107195 , Val-loss =  64.5048781878 , Time for epoch =  0.6958925724029541 s\n",
      "Epoch  597 / 1000  : Train-loss =  72.2602768014 , Val-loss =  64.471039637 , Time for epoch =  0.7130930423736572 s\n",
      "Epoch  598 / 1000  : Train-loss =  72.230216968 , Val-loss =  64.4372404708 , Time for epoch =  0.7293210029602051 s\n",
      "Epoch  599 / 1000  : Train-loss =  72.2002249074 , Val-loss =  64.4034776136 , Time for epoch =  0.7195916175842285 s\n",
      "Epoch  600 / 1000  : Train-loss =  72.1702947171 , Val-loss =  64.3697485521 , Time for epoch =  0.7006375789642334 s\n",
      "Epoch  601 / 1000  : Train-loss =  72.1404208548 , Val-loss =  64.3360510847 , Time for epoch =  0.6943068504333496 s\n",
      "Epoch  602 / 1000  : Train-loss =  72.1105981048 , Val-loss =  64.3023831973 , Time for epoch =  0.6984257698059082 s\n",
      "Epoch  603 / 1000  : Train-loss =  72.0808215517 , Val-loss =  64.2687430014 , Time for epoch =  0.7053267955780029 s\n",
      "Epoch  604 / 1000  : Train-loss =  72.0510865598 , Val-loss =  64.2351287024 , Time for epoch =  0.700913667678833 s\n",
      "Epoch  605 / 1000  : Train-loss =  72.0213887534 , Val-loss =  64.2015385832 , Time for epoch =  0.7276515960693359 s\n",
      "Epoch  606 / 1000  : Train-loss =  71.9917240012 , Val-loss =  64.1679709944 , Time for epoch =  0.7412621974945068 s\n",
      "Epoch  607 / 1000  : Train-loss =  71.9620884003 , Val-loss =  64.1344243492 , Time for epoch =  0.7455704212188721 s\n",
      "Epoch  608 / 1000  : Train-loss =  71.9324782628 , Val-loss =  64.1008971191 , Time for epoch =  0.7021794319152832 s\n",
      "Epoch  609 / 1000  : Train-loss =  71.9028901019 , Val-loss =  64.0673878311 , Time for epoch =  0.721583366394043 s\n",
      "Epoch  610 / 1000  : Train-loss =  71.8733206206 , Val-loss =  64.0338950654 , Time for epoch =  0.7048423290252686 s\n",
      "Epoch  611 / 1000  : Train-loss =  71.8437666999 , Val-loss =  64.0004174531 , Time for epoch =  0.7181203365325928 s\n",
      "Epoch  612 / 1000  : Train-loss =  71.8142253879 , Val-loss =  63.9669536743 , Time for epoch =  0.806358814239502 s\n",
      "Epoch  613 / 1000  : Train-loss =  71.7846938906 , Val-loss =  63.9335024562 , Time for epoch =  0.8051578998565674 s\n",
      "Epoch  614 / 1000  : Train-loss =  71.7551695617 , Val-loss =  63.9000625715 , Time for epoch =  0.7810776233673096 s\n",
      "Epoch  615 / 1000  : Train-loss =  71.7256498948 , Val-loss =  63.8666499325 , Time for epoch =  0.8026716709136963 s\n",
      "Epoch  616 / 1000  : Train-loss =  71.6961325143 , Val-loss =  63.8332590348 , Time for epoch =  0.7374403476715088 s\n",
      "Epoch  617 / 1000  : Train-loss =  71.6666151684 , Val-loss =  63.7998985648 , Time for epoch =  0.7404370307922363 s\n",
      "Epoch  618 / 1000  : Train-loss =  71.6370957215 , Val-loss =  63.7665657883 , Time for epoch =  0.7305505275726318 s\n",
      "Epoch  619 / 1000  : Train-loss =  71.6075721475 , Val-loss =  63.7332397797 , Time for epoch =  0.7577996253967285 s\n",
      "Epoch  620 / 1000  : Train-loss =  71.5780425235 , Val-loss =  63.6999194681 , Time for epoch =  0.7359371185302734 s\n",
      "Epoch  621 / 1000  : Train-loss =  71.548505024 , Val-loss =  63.6666038219 , Time for epoch =  0.7178378105163574 s\n",
      "Epoch  622 / 1000  : Train-loss =  71.5189579147 , Val-loss =  63.6332918481 , Time for epoch =  0.7197799682617188 s\n",
      "Epoch  623 / 1000  : Train-loss =  71.4893995478 , Val-loss =  63.5999825905 , Time for epoch =  0.7193284034729004 s\n",
      "Epoch  624 / 1000  : Train-loss =  71.4598283567 , Val-loss =  63.5666751285 , Time for epoch =  0.7116079330444336 s\n",
      "Epoch  625 / 1000  : Train-loss =  71.4302428513 , Val-loss =  63.5333685754 , Time for epoch =  0.7181317806243896 s\n",
      "Epoch  626 / 1000  : Train-loss =  71.4006416138 , Val-loss =  63.5000620781 , Time for epoch =  0.7123064994812012 s\n",
      "Epoch  627 / 1000  : Train-loss =  71.3710232943 , Val-loss =  63.4667548151 , Time for epoch =  0.7338991165161133 s\n",
      "Epoch  628 / 1000  : Train-loss =  71.3413866069 , Val-loss =  63.4334459956 , Time for epoch =  0.7572245597839355 s\n",
      "Epoch  629 / 1000  : Train-loss =  71.3117303262 , Val-loss =  63.4001348585 , Time for epoch =  0.7340419292449951 s\n",
      "Epoch  630 / 1000  : Train-loss =  71.2820532835 , Val-loss =  63.3668206714 , Time for epoch =  0.7535090446472168 s\n",
      "Epoch  631 / 1000  : Train-loss =  71.2523543639 , Val-loss =  63.3335027296 , Time for epoch =  0.7427182197570801 s\n",
      "Epoch  632 / 1000  : Train-loss =  71.2226325028 , Val-loss =  63.3001803547 , Time for epoch =  0.7268118858337402 s\n",
      "Epoch  633 / 1000  : Train-loss =  71.1928866831 , Val-loss =  63.2668528946 , Time for epoch =  0.7109813690185547 s\n",
      "Epoch  634 / 1000  : Train-loss =  71.1631159326 , Val-loss =  63.2335197215 , Time for epoch =  0.722759485244751 s\n",
      "Epoch  635 / 1000  : Train-loss =  71.1333193212 , Val-loss =  63.2002054873 , Time for epoch =  0.7142815589904785 s\n",
      "Epoch  636 / 1000  : Train-loss =  71.1034959584 , Val-loss =  63.1668851804 , Time for epoch =  0.7056326866149902 s\n",
      "Epoch  637 / 1000  : Train-loss =  71.0736449913 , Val-loss =  63.1335606962 , Time for epoch =  0.7137348651885986 s\n",
      "Epoch  638 / 1000  : Train-loss =  71.0437656018 , Val-loss =  63.1002674257 , Time for epoch =  0.7328691482543945 s\n",
      "Epoch  639 / 1000  : Train-loss =  71.0138570052 , Val-loss =  63.0669655394 , Time for epoch =  0.7375130653381348 s\n",
      "Epoch  640 / 1000  : Train-loss =  70.9839184478 , Val-loss =  63.0336545176 , Time for epoch =  0.7501542568206787 s\n",
      "Epoch  641 / 1000  : Train-loss =  70.953949205 , Val-loss =  63.0003338618 , Time for epoch =  0.769721508026123 s\n",
      "Epoch  642 / 1000  : Train-loss =  70.92394858 , Val-loss =  62.9670030933 , Time for epoch =  0.7344577312469482 s\n",
      "Epoch  643 / 1000  : Train-loss =  70.8939159014 , Val-loss =  62.9336617529 , Time for epoch =  0.746842622756958 s\n",
      "Epoch  644 / 1000  : Train-loss =  70.8638505225 , Val-loss =  62.9003094002 , Time for epoch =  0.738236665725708 s\n",
      "Epoch  645 / 1000  : Train-loss =  70.8337518191 , Val-loss =  62.8669456129 , Time for epoch =  0.7490444183349609 s\n",
      "Epoch  646 / 1000  : Train-loss =  70.8036191883 , Val-loss =  62.8335699862 , Time for epoch =  0.7402322292327881 s\n",
      "Epoch  647 / 1000  : Train-loss =  70.7734520476 , Val-loss =  62.8001821322 , Time for epoch =  0.7434813976287842 s\n",
      "Epoch  648 / 1000  : Train-loss =  70.743249833 , Val-loss =  62.7667816793 , Time for epoch =  0.7393064498901367 s\n",
      "Epoch  649 / 1000  : Train-loss =  70.7080913254 , Val-loss =  62.7296912354 , Time for epoch =  0.8320403099060059 s\n",
      "Epoch  650 / 1000  : Train-loss =  70.6729711917 , Val-loss =  62.6976335256 , Time for epoch =  0.9242773056030273 s\n",
      "Epoch  651 / 1000  : Train-loss =  70.6430826526 , Val-loss =  62.6648359497 , Time for epoch =  0.8666749000549316 s\n",
      "Epoch  652 / 1000  : Train-loss =  70.6131161067 , Val-loss =  62.6316984031 , Time for epoch =  0.870295524597168 s\n",
      "Epoch  653 / 1000  : Train-loss =  70.5830864817 , Val-loss =  62.5983950412 , Time for epoch =  0.8139634132385254 s\n",
      "Epoch  654 / 1000  : Train-loss =  70.5529998747 , Val-loss =  62.5650063666 , Time for epoch =  0.7422528266906738 s\n",
      "Epoch  655 / 1000  : Train-loss =  70.5228591536 , Val-loss =  62.5315694803 , Time for epoch =  0.7367420196533203 s\n",
      "Epoch  656 / 1000  : Train-loss =  70.4877606807 , Val-loss =  62.4942802255 , Time for epoch =  0.7244651317596436 s\n",
      "Epoch  657 / 1000  : Train-loss =  70.4528466747 , Val-loss =  62.4621397098 , Time for epoch =  0.7345566749572754 s\n",
      "Epoch  658 / 1000  : Train-loss =  70.4229530618 , Val-loss =  62.4292616762 , Time for epoch =  0.7429840564727783 s\n",
      "Epoch  659 / 1000  : Train-loss =  70.3929699475 , Val-loss =  62.3960456048 , Time for epoch =  0.7749528884887695 s\n",
      "Epoch  660 / 1000  : Train-loss =  70.3629118036 , Val-loss =  62.3626641623 , Time for epoch =  0.7748267650604248 s\n",
      "Epoch  661 / 1000  : Train-loss =  70.3327847195 , Val-loss =  62.3291965758 , Time for epoch =  0.7611570358276367 s\n",
      "Epoch  662 / 1000  : Train-loss =  70.2976974396 , Val-loss =  62.2917365167 , Time for epoch =  0.7599606513977051 s\n",
      "Epoch  663 / 1000  : Train-loss =  70.2629228748 , Val-loss =  62.2595199574 , Time for epoch =  0.7780039310455322 s\n",
      "Epoch  664 / 1000  : Train-loss =  70.2329908594 , Val-loss =  62.2265684663 , Time for epoch =  0.7362027168273926 s\n",
      "Epoch  665 / 1000  : Train-loss =  70.2029608741 , Val-loss =  62.1932808619 , Time for epoch =  0.7304186820983887 s\n",
      "Epoch  666 / 1000  : Train-loss =  70.1728468636 , Val-loss =  62.159828516 , Time for epoch =  0.7260251045227051 s\n",
      "Epoch  667 / 1000  : Train-loss =  70.1426548239 , Val-loss =  62.1262895472 , Time for epoch =  0.7306041717529297 s\n",
      "Epoch  668 / 1000  : Train-loss =  70.1075078255 , Val-loss =  62.0886379649 , Time for epoch =  0.7281911373138428 s\n",
      "Epoch  669 / 1000  : Train-loss =  70.0727878668 , Val-loss =  62.0563302381 , Time for epoch =  0.7527613639831543 s\n",
      "Epoch  670 / 1000  : Train-loss =  70.0427503951 , Val-loss =  62.0232938932 , Time for epoch =  0.7687122821807861 s\n",
      "Epoch  671 / 1000  : Train-loss =  70.0126089806 , Val-loss =  61.9899256806 , Time for epoch =  0.7739391326904297 s\n",
      "Epoch  672 / 1000  : Train-loss =  69.9823768921 , Val-loss =  61.956394619 , Time for epoch =  0.7697296142578125 s\n",
      "Epoch  673 / 1000  : Train-loss =  69.9471849722 , Val-loss =  61.9186176876 , Time for epoch =  0.8016974925994873 s\n",
      "Epoch  674 / 1000  : Train-loss =  69.9125299407 , Val-loss =  61.8862499111 , Time for epoch =  0.7520534992218018 s\n",
      "Epoch  675 / 1000  : Train-loss =  69.8824134982 , Val-loss =  61.8531532337 , Time for epoch =  0.73282790184021 s\n",
      "Epoch  676 / 1000  : Train-loss =  69.8521875943 , Val-loss =  61.8197251595 , Time for epoch =  0.7295794486999512 s\n",
      "Epoch  677 / 1000  : Train-loss =  69.8218649335 , Val-loss =  61.7861343616 , Time for epoch =  0.732903003692627 s\n",
      "Epoch  678 / 1000  : Train-loss =  69.7865835797 , Val-loss =  61.7482009501 , Time for epoch =  0.7283647060394287 s\n",
      "Epoch  679 / 1000  : Train-loss =  69.7519406263 , Val-loss =  61.7157560646 , Time for epoch =  0.7303194999694824 s\n",
      "Epoch  680 / 1000  : Train-loss =  69.7217036487 , Val-loss =  61.6825891717 , Time for epoch =  0.7508704662322998 s\n",
      "Epoch  681 / 1000  : Train-loss =  69.6913531975 , Val-loss =  61.6490949596 , Time for epoch =  0.7572457790374756 s\n",
      "Epoch  682 / 1000  : Train-loss =  69.6609012628 , Val-loss =  61.6154398917 , Time for epoch =  0.763505220413208 s\n",
      "Epoch  683 / 1000  : Train-loss =  69.6254947333 , Val-loss =  61.5773480339 , Time for epoch =  0.7764647006988525 s\n",
      "Epoch  684 / 1000  : Train-loss =  69.5908204219 , Val-loss =  61.5448226858 , Time for epoch =  0.7595579624176025 s\n",
      "Epoch  685 / 1000  : Train-loss =  69.5604283235 , Val-loss =  61.5115825686 , Time for epoch =  0.8748748302459717 s\n",
      "Epoch  686 / 1000  : Train-loss =  69.5299197455 , Val-loss =  61.4780196279 , Time for epoch =  0.9367775917053223 s\n",
      "Epoch  687 / 1000  : Train-loss =  69.4993059178 , Val-loss =  61.4442979811 , Time for epoch =  0.9330639839172363 s\n",
      "Epoch  688 / 1000  : Train-loss =  69.4637437719 , Val-loss =  61.4060472074 , Time for epoch =  0.9452271461486816 s\n",
      "Epoch  689 / 1000  : Train-loss =  69.4290013613 , Val-loss =  61.3734391991 , Time for epoch =  0.9133718013763428 s\n",
      "Epoch  690 / 1000  : Train-loss =  69.3984246284 , Val-loss =  61.3401239513 , Time for epoch =  0.8372883796691895 s\n",
      "Epoch  691 / 1000  : Train-loss =  69.3677291749 , Val-loss =  61.3064906964 , Time for epoch =  0.8308219909667969 s\n",
      "Epoch  692 / 1000  : Train-loss =  69.3369254288 , Val-loss =  61.272701106 , Time for epoch =  0.8359966278076172 s\n",
      "Epoch  693 / 1000  : Train-loss =  69.3011811848 , Val-loss =  61.2342918051 , Time for epoch =  0.8385450839996338 s\n",
      "Epoch  694 / 1000  : Train-loss =  69.2663391391 , Val-loss =  61.2015997604 , Time for epoch =  0.7397682666778564 s\n",
      "Epoch  695 / 1000  : Train-loss =  69.2355521261 , Val-loss =  61.1682083528 , Time for epoch =  0.6902172565460205 s\n",
      "Epoch  696 / 1000  : Train-loss =  69.2046447345 , Val-loss =  61.1345040406 , Time for epoch =  0.6920766830444336 s\n",
      "Epoch  697 / 1000  : Train-loss =  69.1687865892 , Val-loss =  61.0960381193 , Time for epoch =  0.7028446197509766 s\n",
      "Epoch  698 / 1000  : Train-loss =  69.133926103 , Val-loss =  61.0633164206 , Time for epoch =  0.6940124034881592 s\n",
      "Epoch  699 / 1000  : Train-loss =  69.1030088961 , Val-loss =  61.0298899455 , Time for epoch =  0.7094810009002686 s\n",
      "Epoch  700 / 1000  : Train-loss =  69.0719683935 , Val-loss =  60.9961484827 , Time for epoch =  0.6973803043365479 s\n",
      "Epoch  701 / 1000  : Train-loss =  69.0408135391 , Val-loss =  60.962252321 , Time for epoch =  0.6988394260406494 s\n",
      "Epoch  702 / 1000  : Train-loss =  69.0047217518 , Val-loss =  60.9235750987 , Time for epoch =  0.7262358665466309 s\n",
      "Epoch  703 / 1000  : Train-loss =  68.9697004132 , Val-loss =  60.8907432199 , Time for epoch =  0.7254102230072021 s\n",
      "Epoch  704 / 1000  : Train-loss =  68.9385239865 , Val-loss =  60.8572278697 , Time for epoch =  0.7344961166381836 s\n",
      "Epoch  705 / 1000  : Train-loss =  68.907223833 , Val-loss =  60.8234087984 , Time for epoch =  0.7543272972106934 s\n",
      "Epoch  706 / 1000  : Train-loss =  68.8709786818 , Val-loss =  60.7846732188 , Time for epoch =  0.731285810470581 s\n",
      "Epoch  707 / 1000  : Train-loss =  68.8358940736 , Val-loss =  60.7518097039 , Time for epoch =  0.7323272228240967 s\n",
      "Epoch  708 / 1000  : Train-loss =  68.8045502471 , Val-loss =  60.7182588096 , Time for epoch =  0.7330117225646973 s\n",
      "Epoch  709 / 1000  : Train-loss =  68.7730806034 , Val-loss =  60.6844030083 , Time for epoch =  0.7038750648498535 s\n",
      "Epoch  710 / 1000  : Train-loss =  68.7414923477 , Val-loss =  60.6503974752 , Time for epoch =  0.7198660373687744 s\n",
      "Epoch  711 / 1000  : Train-loss =  68.7049737066 , Val-loss =  60.6114561984 , Time for epoch =  0.7360663414001465 s\n",
      "Epoch  712 / 1000  : Train-loss =  68.6696811153 , Val-loss =  60.5784834409 , Time for epoch =  0.7325649261474609 s\n",
      "Epoch  713 / 1000  : Train-loss =  68.6380388145 , Val-loss =  60.5448444749 , Time for epoch =  0.7380826473236084 s\n",
      "Epoch  714 / 1000  : Train-loss =  68.6062708064 , Val-loss =  60.5109119046 , Time for epoch =  0.7316558361053467 s\n",
      "Epoch  715 / 1000  : Train-loss =  68.5695652196 , Val-loss =  60.4719133746 , Time for epoch =  0.733715295791626 s\n",
      "Epoch  716 / 1000  : Train-loss =  68.5341730496 , Val-loss =  60.4389093732 , Time for epoch =  0.7604234218597412 s\n",
      "Epoch  717 / 1000  : Train-loss =  68.5023321169 , Val-loss =  60.405236623 , Time for epoch =  0.7322547435760498 s\n",
      "Epoch  718 / 1000  : Train-loss =  68.4703639115 , Val-loss =  60.3712698141 , Time for epoch =  0.7308876514434814 s\n",
      "Epoch  719 / 1000  : Train-loss =  68.433453057 , Val-loss =  60.3321692208 , Time for epoch =  0.7348344326019287 s\n",
      "Epoch  720 / 1000  : Train-loss =  68.39794076 , Val-loss =  60.2991144737 , Time for epoch =  0.7472574710845947 s\n",
      "Epoch  721 / 1000  : Train-loss =  68.3658843083 , Val-loss =  60.2653997604 , Time for epoch =  0.75522780418396 s\n",
      "Epoch  722 / 1000  : Train-loss =  68.3336993804 , Val-loss =  60.231395472 , Time for epoch =  4.368598461151123 s\n",
      "Epoch  723 / 1000  : Train-loss =  68.30139046 , Val-loss =  60.1972490665 , Time for epoch =  0.7300558090209961 s\n",
      "Epoch  724 / 1000  : Train-loss =  68.2641532089 , Val-loss =  60.1579516176 , Time for epoch =  0.7256767749786377 s\n",
      "Epoch  725 / 1000  : Train-loss =  68.2283723694 , Val-loss =  60.1247907428 , Time for epoch =  0.729454517364502 s\n",
      "Epoch  726 / 1000  : Train-loss =  68.1959655965 , Val-loss =  60.0909913426 , Time for epoch =  0.7267031669616699 s\n",
      "Epoch  727 / 1000  : Train-loss =  68.1634309145 , Val-loss =  60.0569138718 , Time for epoch =  0.741448163986206 s\n",
      "Epoch  728 / 1000  : Train-loss =  68.1259609062 , Val-loss =  60.01756275 , Time for epoch =  0.7296812534332275 s\n",
      "Epoch  729 / 1000  : Train-loss =  68.0900328504 , Val-loss =  59.9843734184 , Time for epoch =  0.7304210662841797 s\n",
      "Epoch  730 / 1000  : Train-loss =  68.0573852251 , Val-loss =  59.9505450337 , Time for epoch =  0.730851411819458 s\n",
      "Epoch  731 / 1000  : Train-loss =  68.0246086449 , Val-loss =  59.9164391552 , Time for epoch =  0.7325055599212646 s\n",
      "Epoch  732 / 1000  : Train-loss =  67.9868901869 , Val-loss =  59.8770123072 , Time for epoch =  0.7288937568664551 s\n",
      "Epoch  733 / 1000  : Train-loss =  67.9507978617 , Val-loss =  59.8437614473 , Time for epoch =  0.6909410953521729 s\n",
      "Epoch  734 / 1000  : Train-loss =  67.9178946488 , Val-loss =  59.809898285 , Time for epoch =  0.6947805881500244 s\n",
      "Epoch  735 / 1000  : Train-loss =  67.8848615976 , Val-loss =  59.7757625547 , Time for epoch =  0.6953363418579102 s\n",
      "Epoch  736 / 1000  : Train-loss =  67.8517002671 , Val-loss =  59.7414934597 , Time for epoch =  0.6926307678222656 s\n",
      "Epoch  737 / 1000  : Train-loss =  67.813608416 , Val-loss =  59.7018684224 , Time for epoch =  0.6928324699401855 s\n",
      "Epoch  738 / 1000  : Train-loss =  67.777198488 , Val-loss =  59.6685378644 , Time for epoch =  0.6977283954620361 s\n",
      "Epoch  739 / 1000  : Train-loss =  67.7439000314 , Val-loss =  59.6345994231 , Time for epoch =  0.6994378566741943 s\n",
      "Epoch  740 / 1000  : Train-loss =  67.7104723618 , Val-loss =  59.6003999373 , Time for epoch =  0.7170240879058838 s\n",
      "Epoch  741 / 1000  : Train-loss =  67.6721054694 , Val-loss =  59.5607429766 , Time for epoch =  0.7326462268829346 s\n",
      "Epoch  742 / 1000  : Train-loss =  67.6355083055 , Val-loss =  59.5273780051 , Time for epoch =  0.7373449802398682 s\n",
      "Epoch  743 / 1000  : Train-loss =  67.6019311115 , Val-loss =  59.4934202818 , Time for epoch =  0.730665922164917 s\n",
      "Epoch  744 / 1000  : Train-loss =  67.5682238812 , Val-loss =  59.4592030773 , Time for epoch =  0.7331099510192871 s\n",
      "Epoch  745 / 1000  : Train-loss =  67.5343860566 , Val-loss =  59.4248596028 , Time for epoch =  0.7324309349060059 s\n",
      "Epoch  746 / 1000  : Train-loss =  67.4956163251 , Val-loss =  59.3850249803 , Time for epoch =  0.7369036674499512 s\n",
      "Epoch  747 / 1000  : Train-loss =  67.4586752286 , Val-loss =  59.3515890781 , Time for epoch =  0.7325935363769531 s\n",
      "Epoch  748 / 1000  : Train-loss =  67.4246759502 , Val-loss =  59.3175682515 , Time for epoch =  0.7409732341766357 s\n",
      "Epoch  749 / 1000  : Train-loss =  67.3905470804 , Val-loss =  59.2832993756 , Time for epoch =  0.7384271621704102 s\n",
      "Epoch  750 / 1000  : Train-loss =  67.351474928 , Val-loss =  59.2434610355 , Time for epoch =  0.7402257919311523 s\n",
      "Epoch  751 / 1000  : Train-loss =  67.3143240891 , Val-loss =  59.209982405 , Time for epoch =  0.6977105140686035 s\n",
      "Epoch  752 / 1000  : Train-loss =  67.2800225949 , Val-loss =  59.1759536271 , Time for epoch =  0.6965031623840332 s\n",
      "Epoch  753 / 1000  : Train-loss =  67.2455906685 , Val-loss =  59.1416790244 , Time for epoch =  0.6983740329742432 s\n",
      "Epoch  754 / 1000  : Train-loss =  67.2110255544 , Val-loss =  59.1072857773 , Time for epoch =  0.6938247680664062 s\n",
      "Epoch  755 / 1000  : Train-loss =  67.1715196925 , Val-loss =  59.067278792 , Time for epoch =  0.6920087337493896 s\n",
      "Epoch  756 / 1000  : Train-loss =  67.1339987352 , Val-loss =  59.0337547327 , Time for epoch =  0.6928761005401611 s\n",
      "Epoch  757 / 1000  : Train-loss =  67.0992475141 , Val-loss =  58.9996785546 , Time for epoch =  0.7052628993988037 s\n",
      "Epoch  758 / 1000  : Train-loss =  67.0643660834 , Val-loss =  58.9653681265 , Time for epoch =  0.6936731338500977 s\n",
      "Epoch  759 / 1000  : Train-loss =  67.0293505563 , Val-loss =  58.9309453985 , Time for epoch =  0.7053282260894775 s\n",
      "Epoch  760 / 1000  : Train-loss =  66.9893948222 , Val-loss =  58.890832158 , Time for epoch =  0.7034564018249512 s\n",
      "Epoch  761 / 1000  : Train-loss =  66.9514947054 , Val-loss =  58.8572727658 , Time for epoch =  0.695772647857666 s\n",
      "Epoch  762 / 1000  : Train-loss =  66.9162816712 , Val-loss =  58.8231677063 , Time for epoch =  0.7103292942047119 s\n",
      "Epoch  763 / 1000  : Train-loss =  66.8809385572 , Val-loss =  58.7888370443 , Time for epoch =  0.7399826049804688 s\n",
      "Epoch  764 / 1000  : Train-loss =  66.8454602357 , Val-loss =  58.7543993867 , Time for epoch =  0.748225212097168 s\n",
      "Epoch  765 / 1000  : Train-loss =  66.8050384219 , Val-loss =  58.7142050327 , Time for epoch =  0.7574553489685059 s\n",
      "Epoch  766 / 1000  : Train-loss =  66.7667494149 , Val-loss =  58.6806147419 , Time for epoch =  0.7554476261138916 s\n",
      "Epoch  767 / 1000  : Train-loss =  66.7310615729 , Val-loss =  58.646495612 , Time for epoch =  4.335634231567383 s\n",
      "Epoch  768 / 1000  : Train-loss =  66.6952435628 , Val-loss =  58.6121597958 , Time for epoch =  0.7282454967498779 s\n",
      "Epoch  769 / 1000  : Train-loss =  66.6592889977 , Val-loss =  58.5777225721 , Time for epoch =  0.726421594619751 s\n",
      "Epoch  770 / 1000  : Train-loss =  66.6183836912 , Val-loss =  58.5374787573 , Time for epoch =  0.7301187515258789 s\n",
      "Epoch  771 / 1000  : Train-loss =  66.579695062 , Val-loss =  58.5038586118 , Time for epoch =  0.7282559871673584 s\n",
      "Epoch  772 / 1000  : Train-loss =  66.5435183963 , Val-loss =  58.4697420888 , Time for epoch =  0.7331807613372803 s\n",
      "Epoch  773 / 1000  : Train-loss =  66.5072112692 , Val-loss =  58.4354181773 , Time for epoch =  0.7312812805175781 s\n",
      "Epoch  774 / 1000  : Train-loss =  66.4707660208 , Val-loss =  58.4009988322 , Time for epoch =  0.7285556793212891 s\n",
      "Epoch  775 / 1000  : Train-loss =  66.4293586663 , Val-loss =  58.3607373939 , Time for epoch =  0.7150487899780273 s\n",
      "Epoch  776 / 1000  : Train-loss =  66.3902588386 , Val-loss =  58.3270927711 , Time for epoch =  0.7245864868164062 s\n",
      "Epoch  777 / 1000  : Train-loss =  66.353578417 , Val-loss =  58.2929979406 , Time for epoch =  0.7379288673400879 s\n",
      "Epoch  778 / 1000  : Train-loss =  66.3167670492 , Val-loss =  58.2587054962 , Time for epoch =  0.7337157726287842 s\n",
      "Epoch  779 / 1000  : Train-loss =  66.2798157927 , Val-loss =  58.2243240805 , Time for epoch =  0.7266814708709717 s\n",
      "Epoch  780 / 1000  : Train-loss =  66.2427212223 , Val-loss =  58.1898963135 , Time for epoch =  0.7290997505187988 s\n",
      "Epoch  781 / 1000  : Train-loss =  66.2006624931 , Val-loss =  58.14954691 , Time for epoch =  0.73445725440979 s\n",
      "Epoch  782 / 1000  : Train-loss =  66.1609857926 , Val-loss =  58.1159261395 , Time for epoch =  0.7292783260345459 s\n",
      "Epoch  783 / 1000  : Train-loss =  66.1236417485 , Val-loss =  58.081856679 , Time for epoch =  0.7307536602020264 s\n",
      "Epoch  784 / 1000  : Train-loss =  66.0861669269 , Val-loss =  58.0476041025 , Time for epoch =  0.731935977935791 s\n",
      "Epoch  785 / 1000  : Train-loss =  66.0485509192 , Val-loss =  58.0132725292 , Time for epoch =  0.7301769256591797 s\n",
      "Epoch  786 / 1000  : Train-loss =  66.0059492198 , Val-loss =  57.9730094249 , Time for epoch =  0.7277421951293945 s\n",
      "Epoch  787 / 1000  : Train-loss =  65.9658427266 , Val-loss =  57.9394069029 , Time for epoch =  0.7274289131164551 s\n",
      "Epoch  788 / 1000  : Train-loss =  65.927965157 , Val-loss =  57.9053987846 , Time for epoch =  0.7303073406219482 s\n",
      "Epoch  789 / 1000  : Train-loss =  65.8899558645 , Val-loss =  57.8712379413 , Time for epoch =  0.7291722297668457 s\n",
      "Epoch  790 / 1000  : Train-loss =  65.8518030895 , Val-loss =  57.8370055746 , Time for epoch =  0.7310664653778076 s\n",
      "Epoch  791 / 1000  : Train-loss =  65.8135023906 , Val-loss =  57.802741003 , Time for epoch =  0.7279055118560791 s\n",
      "Epoch  792 / 1000  : Train-loss =  65.7750524564 , Val-loss =  57.7684603953 , Time for epoch =  0.7419006824493408 s\n",
      "Epoch  793 / 1000  : Train-loss =  65.7316131278 , Val-loss =  57.728155913 , Time for epoch =  0.7389936447143555 s\n",
      "Epoch  794 / 1000  : Train-loss =  65.6907490181 , Val-loss =  57.6946384849 , Time for epoch =  0.7358129024505615 s\n",
      "Epoch  795 / 1000  : Train-loss =  65.6520212241 , Val-loss =  57.6607529664 , Time for epoch =  0.7141475677490234 s\n",
      "Epoch  796 / 1000  : Train-loss =  65.6131619685 , Val-loss =  57.6267149433 , Time for epoch =  0.711172342300415 s\n",
      "Epoch  797 / 1000  : Train-loss =  65.574157818 , Val-loss =  57.592620601 , Time for epoch =  0.7150888442993164 s\n",
      "Epoch  798 / 1000  : Train-loss =  65.5350037126 , Val-loss =  57.5585070306 , Time for epoch =  0.7034914493560791 s\n",
      "Epoch  799 / 1000  : Train-loss =  65.4956980748 , Val-loss =  57.5243895987 , Time for epoch =  0.7186524868011475 s\n",
      "Epoch  800 / 1000  : Train-loss =  65.4513874173 , Val-loss =  57.4841874215 , Time for epoch =  0.698460578918457 s\n",
      "Epoch  801 / 1000  : Train-loss =  65.4097613591 , Val-loss =  57.4508303888 , Time for epoch =  0.6983070373535156 s\n",
      "Epoch  802 / 1000  : Train-loss =  65.3701632708 , Val-loss =  57.4171219667 , Time for epoch =  0.6991581916809082 s\n",
      "Epoch  803 / 1000  : Train-loss =  65.3304334366 , Val-loss =  57.3832962539 , Time for epoch =  0.6966629028320312 s\n",
      "Epoch  804 / 1000  : Train-loss =  65.2905566807 , Val-loss =  57.3494309818 , Time for epoch =  0.6906263828277588 s\n",
      "Epoch  805 / 1000  : Train-loss =  65.2505273521 , Val-loss =  57.3155614171 , Time for epoch =  0.727109432220459 s\n",
      "Epoch  806 / 1000  : Train-loss =  65.2103436397 , Val-loss =  57.2817023637 , Time for epoch =  0.755415678024292 s\n",
      "Epoch  807 / 1000  : Train-loss =  65.1700051185 , Val-loss =  57.2478612472 , Time for epoch =  0.736177921295166 s\n",
      "Epoch  808 / 1000  : Train-loss =  65.1246404008 , Val-loss =  57.2078766097 , Time for epoch =  0.6982879638671875 s\n",
      "Epoch  809 / 1000  : Train-loss =  65.0820865686 , Val-loss =  57.1748118162 , Time for epoch =  0.7520623207092285 s\n",
      "Epoch  810 / 1000  : Train-loss =  65.0414397266 , Val-loss =  57.1414013899 , Time for epoch =  0.7329316139221191 s\n",
      "Epoch  811 / 1000  : Train-loss =  65.0006608826 , Val-loss =  57.1079277086 , Time for epoch =  0.7346198558807373 s\n",
      "Epoch  812 / 1000  : Train-loss =  64.9597329362 , Val-loss =  57.0744378457 , Time for epoch =  0.7812986373901367 s\n",
      "Epoch  813 / 1000  : Train-loss =  64.9186496155 , Val-loss =  57.0409651476 , Time for epoch =  0.795886754989624 s\n",
      "Epoch  814 / 1000  : Train-loss =  64.8774088771 , Val-loss =  57.0075239357 , Time for epoch =  0.7832784652709961 s\n",
      "Epoch  815 / 1000  : Train-loss =  64.836010183 , Val-loss =  56.9741217994 , Time for epoch =  0.7851433753967285 s\n",
      "Epoch  816 / 1000  : Train-loss =  64.794453521 , Val-loss =  56.9407639738 , Time for epoch =  0.7895257472991943 s\n",
      "Epoch  817 / 1000  : Train-loss =  64.7527390526 , Val-loss =  56.9074548956 , Time for epoch =  0.7438013553619385 s\n",
      "Epoch  818 / 1000  : Train-loss =  64.7059683445 , Val-loss =  56.8679403752 , Time for epoch =  0.7431735992431641 s\n",
      "Epoch  819 / 1000  : Train-loss =  64.662167439 , Val-loss =  56.8354442087 , Time for epoch =  0.7329897880554199 s\n",
      "Epoch  820 / 1000  : Train-loss =  64.6201279655 , Val-loss =  56.8026526864 , Time for epoch =  0.7347688674926758 s\n",
      "Epoch  821 / 1000  : Train-loss =  64.5779565486 , Val-loss =  56.7698041331 , Time for epoch =  0.7391901016235352 s\n",
      "Epoch  822 / 1000  : Train-loss =  64.5356338072 , Val-loss =  56.737004909 , Time for epoch =  0.721393346786499 s\n",
      "Epoch  823 / 1000  : Train-loss =  64.4931527848 , Val-loss =  56.7042623579 , Time for epoch =  0.7186174392700195 s\n",
      "Epoch  824 / 1000  : Train-loss =  64.4505112093 , Val-loss =  56.6715862 , Time for epoch =  0.7265126705169678 s\n",
      "Epoch  825 / 1000  : Train-loss =  64.4077084458 , Val-loss =  56.6389844835 , Time for epoch =  0.7166585922241211 s\n",
      "Epoch  826 / 1000  : Train-loss =  64.3647444253 , Val-loss =  56.6064632405 , Time for epoch =  0.715984582901001 s\n",
      "Epoch  827 / 1000  : Train-loss =  64.3216192669 , Val-loss =  56.5740278598 , Time for epoch =  0.712655782699585 s\n",
      "Epoch  828 / 1000  : Train-loss =  64.2783331457 , Val-loss =  56.5416835659 , Time for epoch =  0.713360071182251 s\n",
      "Epoch  829 / 1000  : Train-loss =  64.2348862469 , Val-loss =  56.5094355869 , Time for epoch =  0.7477457523345947 s\n",
      "Epoch  830 / 1000  : Train-loss =  64.1912787504 , Val-loss =  56.4772892165 , Time for epoch =  0.7364883422851562 s\n",
      "Epoch  831 / 1000  : Train-loss =  64.147510827 , Val-loss =  56.4452498371 , Time for epoch =  0.7012245655059814 s\n",
      "Epoch  832 / 1000  : Train-loss =  64.1035826372 , Val-loss =  56.4133229316 , Time for epoch =  0.727989912033081 s\n",
      "Epoch  833 / 1000  : Train-loss =  64.0594943323 , Val-loss =  56.3815140896 , Time for epoch =  0.7299931049346924 s\n",
      "Epoch  834 / 1000  : Train-loss =  64.0152460556 , Val-loss =  56.3498290128 , Time for epoch =  0.7285261154174805 s\n",
      "Epoch  835 / 1000  : Train-loss =  63.9708379438 , Val-loss =  56.3182735195 , Time for epoch =  0.7293522357940674 s\n",
      "Epoch  836 / 1000  : Train-loss =  63.9213047987 , Val-loss =  56.2804597245 , Time for epoch =  0.7142059803009033 s\n",
      "Epoch  837 / 1000  : Train-loss =  63.8750440212 , Val-loss =  56.2498503479 , Time for epoch =  0.7061436176300049 s\n",
      "Epoch  838 / 1000  : Train-loss =  63.8302930449 , Val-loss =  56.2190784963 , Time for epoch =  0.706775426864624 s\n",
      "Epoch  839 / 1000  : Train-loss =  63.7854123861 , Val-loss =  56.1883567827 , Time for epoch =  0.6958799362182617 s\n",
      "Epoch  840 / 1000  : Train-loss =  63.7403791502 , Val-loss =  56.1577560928 , Time for epoch =  0.698347806930542 s\n",
      "Epoch  841 / 1000  : Train-loss =  63.6951855212 , Val-loss =  56.1273048626 , Time for epoch =  0.698577880859375 s\n",
      "Epoch  842 / 1000  : Train-loss =  63.6498290594 , Val-loss =  56.0970173672 , Time for epoch =  0.7063891887664795 s\n",
      "Epoch  843 / 1000  : Train-loss =  63.6043091451 , Val-loss =  56.0669032341 , Time for epoch =  0.7162842750549316 s\n",
      "Epoch  844 / 1000  : Train-loss =  63.5586257768 , Val-loss =  56.0369706328 , Time for epoch =  0.7208330631256104 s\n",
      "Epoch  845 / 1000  : Train-loss =  63.5127791652 , Val-loss =  56.0072273429 , Time for epoch =  0.718961238861084 s\n",
      "Epoch  846 / 1000  : Train-loss =  63.4667695967 , Val-loss =  55.9776811125 , Time for epoch =  0.7317826747894287 s\n",
      "Epoch  847 / 1000  : Train-loss =  63.4205973884 , Val-loss =  55.9483397801 , Time for epoch =  0.706953763961792 s\n",
      "Epoch  848 / 1000  : Train-loss =  63.3742628742 , Val-loss =  55.9192113171 , Time for epoch =  0.7158260345458984 s\n",
      "Epoch  849 / 1000  : Train-loss =  63.3277664015 , Val-loss =  55.8903038439 , Time for epoch =  0.6970162391662598 s\n",
      "Epoch  850 / 1000  : Train-loss =  63.2811083309 , Val-loss =  55.8616256375 , Time for epoch =  0.7040231227874756 s\n",
      "Epoch  851 / 1000  : Train-loss =  63.234289038 , Val-loss =  55.8331851353 , Time for epoch =  0.7077319622039795 s\n",
      "Epoch  852 / 1000  : Train-loss =  63.1873089142 , Val-loss =  55.8049909386 , Time for epoch =  0.6948938369750977 s\n",
      "Epoch  853 / 1000  : Train-loss =  63.1401683697 , Val-loss =  55.7770518147 , Time for epoch =  0.6993618011474609 s\n",
      "Epoch  854 / 1000  : Train-loss =  63.0928678342 , Val-loss =  55.7493809528 , Time for epoch =  0.7449829578399658 s\n",
      "Epoch  855 / 1000  : Train-loss =  63.0454077597 , Val-loss =  55.7219885823 , Time for epoch =  0.7173798084259033 s\n",
      "Epoch  856 / 1000  : Train-loss =  62.9977886219 , Val-loss =  55.6948793644 , Time for epoch =  0.7342476844787598 s\n",
      "Epoch  857 / 1000  : Train-loss =  62.9500109223 , Val-loss =  55.6680627296 , Time for epoch =  0.7458856105804443 s\n",
      "Epoch  858 / 1000  : Train-loss =  62.9020751902 , Val-loss =  55.6415482815 , Time for epoch =  0.7443585395812988 s\n",
      "Epoch  859 / 1000  : Train-loss =  62.8539819844 , Val-loss =  55.6153457974 , Time for epoch =  0.7363858222961426 s\n",
      "Epoch  860 / 1000  : Train-loss =  62.8057318956 , Val-loss =  55.5894652285 , Time for epoch =  0.7372846603393555 s\n",
      "Epoch  861 / 1000  : Train-loss =  62.7573255479 , Val-loss =  55.5639166988 , Time for epoch =  0.720604658126831 s\n",
      "Epoch  862 / 1000  : Train-loss =  62.7087636012 , Val-loss =  55.5387105046 , Time for epoch =  0.7318398952484131 s\n",
      "Epoch  863 / 1000  : Train-loss =  62.6600467529 , Val-loss =  55.513857113 , Time for epoch =  0.742668867111206 s\n",
      "Epoch  864 / 1000  : Train-loss =  62.6111757399 , Val-loss =  55.4893671598 , Time for epoch =  0.7156164646148682 s\n",
      "Epoch  865 / 1000  : Train-loss =  62.5621513408 , Val-loss =  55.4652514474 , Time for epoch =  0.7120046615600586 s\n",
      "Epoch  866 / 1000  : Train-loss =  62.5129743779 , Val-loss =  55.4415209415 , Time for epoch =  0.7205753326416016 s\n",
      "Epoch  867 / 1000  : Train-loss =  62.4636457189 , Val-loss =  55.4181867677 , Time for epoch =  0.735060453414917 s\n",
      "Epoch  868 / 1000  : Train-loss =  62.4141662793 , Val-loss =  55.3952602071 , Time for epoch =  0.7526412010192871 s\n",
      "Epoch  869 / 1000  : Train-loss =  62.3645370241 , Val-loss =  55.3727526919 , Time for epoch =  0.7351827621459961 s\n",
      "Epoch  870 / 1000  : Train-loss =  62.3147589701 , Val-loss =  55.3506757997 , Time for epoch =  0.7482483386993408 s\n",
      "Epoch  871 / 1000  : Train-loss =  62.2648331877 , Val-loss =  55.3290412471 , Time for epoch =  0.734083890914917 s\n",
      "Epoch  872 / 1000  : Train-loss =  62.2147608029 , Val-loss =  55.3078783536 , Time for epoch =  0.72977614402771 s\n",
      "Epoch  873 / 1000  : Train-loss =  62.1645429996 , Val-loss =  55.287207734 , Time for epoch =  0.7330648899078369 s\n",
      "Epoch  874 / 1000  : Train-loss =  62.114181021 , Val-loss =  55.2670164395 , Time for epoch =  0.7281973361968994 s\n",
      "Epoch  875 / 1000  : Train-loss =  62.0636761723 , Val-loss =  55.2473166435 , Time for epoch =  0.7298579216003418 s\n",
      "Epoch  876 / 1000  : Train-loss =  62.0130298219 , Val-loss =  55.2281206168 , Time for epoch =  0.7352406978607178 s\n",
      "Epoch  877 / 1000  : Train-loss =  61.9622434041 , Val-loss =  55.2094407162 , Time for epoch =  0.7165672779083252 s\n",
      "Epoch  878 / 1000  : Train-loss =  61.9113184202 , Val-loss =  55.1912893731 , Time for epoch =  0.7420637607574463 s\n",
      "Epoch  879 / 1000  : Train-loss =  61.8602564412 , Val-loss =  55.1736790806 , Time for epoch =  0.7508912086486816 s\n",
      "Epoch  880 / 1000  : Train-loss =  61.809059109 , Val-loss =  55.1566223804 , Time for epoch =  0.7465426921844482 s\n",
      "Epoch  881 / 1000  : Train-loss =  61.5540728361 , Val-loss =  54.9651472908 , Time for epoch =  0.7535481452941895 s\n",
      "Epoch  882 / 1000  : Train-loss =  61.4906613967 , Val-loss =  54.952311154 , Time for epoch =  0.7441310882568359 s\n",
      "Epoch  883 / 1000  : Train-loss =  61.4445630463 , Val-loss =  54.9428441409 , Time for epoch =  0.8577659130096436 s\n",
      "Epoch  884 / 1000  : Train-loss =  61.3963874179 , Val-loss =  54.9337672679 , Time for epoch =  0.8864076137542725 s\n",
      "Epoch  885 / 1000  : Train-loss =  61.3413203067 , Val-loss =  54.9166004195 , Time for epoch =  0.8680408000946045 s\n",
      "Epoch  886 / 1000  : Train-loss =  61.2840509769 , Val-loss =  54.9001845165 , Time for epoch =  0.8684682846069336 s\n",
      "Epoch  887 / 1000  : Train-loss =  61.2281201237 , Val-loss =  54.8872211472 , Time for epoch =  0.837996244430542 s\n",
      "Epoch  888 / 1000  : Train-loss =  61.175165238 , Val-loss =  54.8782501466 , Time for epoch =  0.6961617469787598 s\n",
      "Epoch  889 / 1000  : Train-loss =  61.1182434209 , Val-loss =  54.8642170962 , Time for epoch =  0.7090933322906494 s\n",
      "Epoch  890 / 1000  : Train-loss =  61.0600626848 , Val-loss =  54.8511257646 , Time for epoch =  0.6936969757080078 s\n",
      "Epoch  891 / 1000  : Train-loss =  61.0059126153 , Val-loss =  54.8447312825 , Time for epoch =  0.7109541893005371 s\n",
      "Epoch  892 / 1000  : Train-loss =  60.9484962372 , Val-loss =  54.8333785973 , Time for epoch =  0.694521427154541 s\n",
      "Epoch  893 / 1000  : Train-loss =  60.8898681821 , Val-loss =  54.8230193174 , Time for epoch =  0.6952536106109619 s\n",
      "Epoch  894 / 1000  : Train-loss =  60.8353010949 , Val-loss =  54.8193608806 , Time for epoch =  0.700585126876831 s\n",
      "Epoch  895 / 1000  : Train-loss =  60.777451223 , Val-loss =  54.8108316439 , Time for epoch =  0.7249822616577148 s\n",
      "Epoch  896 / 1000  : Train-loss =  60.7184261311 , Val-loss =  54.8033416171 , Time for epoch =  0.7331805229187012 s\n",
      "Epoch  897 / 1000  : Train-loss =  60.6634990147 , Val-loss =  54.8025476662 , Time for epoch =  0.7268023490905762 s\n",
      "Epoch  898 / 1000  : Train-loss =  60.6052453152 , Val-loss =  54.796973637 , Time for epoch =  0.7291069030761719 s\n",
      "Epoch  899 / 1000  : Train-loss =  60.5458676137 , Val-loss =  54.7924843526 , Time for epoch =  0.7274856567382812 s\n",
      "Epoch  900 / 1000  : Train-loss =  60.4880613077 , Val-loss =  54.7916245785 , Time for epoch =  0.7265739440917969 s\n",
      "Epoch  901 / 1000  : Train-loss =  60.4334206513 , Val-loss =  54.7948165416 , Time for epoch =  0.7062230110168457 s\n",
      "Epoch  902 / 1000  : Train-loss =  60.3747237063 , Val-loss =  54.7933355727 , Time for epoch =  0.741572380065918 s\n",
      "Epoch  903 / 1000  : Train-loss =  60.3149417827 , Val-loss =  54.7929932654 , Time for epoch =  0.7343077659606934 s\n",
      "Epoch  904 / 1000  : Train-loss =  60.2568000166 , Val-loss =  54.796299013 , Time for epoch =  0.7277007102966309 s\n",
      "Epoch  905 / 1000  : Train-loss =  60.199208506 , Val-loss =  54.8006617852 , Time for epoch =  0.7295503616333008 s\n",
      "Epoch  906 / 1000  : Train-loss =  60.1415600989 , Val-loss =  54.8060709561 , Time for epoch =  0.727924108505249 s\n",
      "Epoch  907 / 1000  : Train-loss =  60.0838407831 , Val-loss =  54.8125305505 , Time for epoch =  0.7279617786407471 s\n",
      "Epoch  908 / 1000  : Train-loss =  60.0260516019 , Val-loss =  54.8200487207 , Time for epoch =  0.7309455871582031 s\n",
      "Epoch  909 / 1000  : Train-loss =  59.9681981204 , Val-loss =  54.8286340871 , Time for epoch =  0.755936861038208 s\n",
      "Epoch  910 / 1000  : Train-loss =  59.9102873169 , Val-loss =  54.8382946294 , Time for epoch =  0.7321822643280029 s\n",
      "Epoch  911 / 1000  : Train-loss =  59.8523266722 , Val-loss =  54.8490373233 , Time for epoch =  0.7295405864715576 s\n",
      "Epoch  912 / 1000  : Train-loss =  59.7943238999 , Val-loss =  54.8608679957 , Time for epoch =  0.7323148250579834 s\n",
      "Epoch  913 / 1000  : Train-loss =  59.7362868646 , Val-loss =  54.8737912431 , Time for epoch =  0.7367875576019287 s\n",
      "Epoch  914 / 1000  : Train-loss =  59.6782235535 , Val-loss =  54.8829342697 , Time for epoch =  0.7442159652709961 s\n",
      "Epoch  915 / 1000  : Train-loss =  59.620142063 , Val-loss =  54.8924697127 , Time for epoch =  0.716012716293335 s\n",
      "Epoch  916 / 1000  : Train-loss =  59.562050591 , Val-loss =  54.9030525499 , Time for epoch =  0.7180066108703613 s\n",
      "Epoch  917 / 1000  : Train-loss =  59.5039574278 , Val-loss =  54.9146825051 , Time for epoch =  0.7145192623138428 s\n",
      "Epoch  918 / 1000  : Train-loss =  59.5096015695 , Val-loss =  54.9351478321 , Time for epoch =  0.7152457237243652 s\n",
      "Epoch  919 / 1000  : Train-loss =  59.4563476391 , Val-loss =  54.9677251004 , Time for epoch =  0.7080526351928711 s\n",
      "Epoch  920 / 1000  : Train-loss =  59.3968232769 , Val-loss =  54.9978901191 , Time for epoch =  0.7127962112426758 s\n",
      "Epoch  921 / 1000  : Train-loss =  59.3354720691 , Val-loss =  55.0291300692 , Time for epoch =  0.6973404884338379 s\n",
      "Epoch  922 / 1000  : Train-loss =  59.2737320789 , Val-loss =  55.0609516837 , Time for epoch =  0.7362942695617676 s\n",
      "Epoch  923 / 1000  : Train-loss =  59.2118669827 , Val-loss =  55.093245379 , Time for epoch =  0.7002787590026855 s\n",
      "Epoch  924 / 1000  : Train-loss =  59.1499635405 , Val-loss =  55.126004247 , Time for epoch =  0.6887214183807373 s\n",
      "Epoch  925 / 1000  : Train-loss =  59.0880561691 , Val-loss =  55.1592484682 , Time for epoch =  0.7044904232025146 s\n",
      "Epoch  926 / 1000  : Train-loss =  59.0261644612 , Val-loss =  55.193003505 , Time for epoch =  0.7034215927124023 s\n",
      "Epoch  927 / 1000  : Train-loss =  58.9685820996 , Val-loss =  55.2309296224 , Time for epoch =  0.7061116695404053 s\n",
      "Epoch  928 / 1000  : Train-loss =  58.9075783479 , Val-loss =  55.2649192203 , Time for epoch =  0.7189579010009766 s\n",
      "Epoch  929 / 1000  : Train-loss =  58.8457609568 , Val-loss =  55.2997028314 , Time for epoch =  0.7103385925292969 s\n",
      "Epoch  930 / 1000  : Train-loss =  58.7839616117 , Val-loss =  55.3351604027 , Time for epoch =  0.7180116176605225 s\n",
      "Epoch  931 / 1000  : Train-loss =  58.7222368221 , Val-loss =  55.3712635065 , Time for epoch =  0.7116315364837646 s\n",
      "Epoch  932 / 1000  : Train-loss =  58.6606116709 , Val-loss =  55.4080063925 , Time for epoch =  0.7008743286132812 s\n",
      "Epoch  933 / 1000  : Train-loss =  58.603444098 , Val-loss =  55.4485312098 , Time for epoch =  0.7026655673980713 s\n",
      "Epoch  934 / 1000  : Train-loss =  58.5428450636 , Val-loss =  55.4855561273 , Time for epoch =  0.7266058921813965 s\n",
      "Epoch  935 / 1000  : Train-loss =  58.4815346283 , Val-loss =  55.5234911472 , Time for epoch =  0.7438464164733887 s\n",
      "Epoch  936 / 1000  : Train-loss =  58.4203296071 , Val-loss =  55.5621525042 , Time for epoch =  0.7249455451965332 s\n",
      "Epoch  937 / 1000  : Train-loss =  58.3636196369 , Val-loss =  55.6042506643 , Time for epoch =  0.6976630687713623 s\n",
      "Epoch  938 / 1000  : Train-loss =  58.3035738533 , Val-loss =  55.6431163283 , Time for epoch =  0.698206901550293 s\n",
      "Epoch  939 / 1000  : Train-loss =  58.2428184997 , Val-loss =  55.6829199423 , Time for epoch =  0.7319164276123047 s\n",
      "Epoch  940 / 1000  : Train-loss =  58.1822238202 , Val-loss =  55.7234289969 , Time for epoch =  0.7178423404693604 s\n",
      "Epoch  941 / 1000  : Train-loss =  58.1262214149 , Val-loss =  55.7669284045 , Time for epoch =  0.7081522941589355 s\n",
      "Epoch  942 / 1000  : Train-loss =  58.0668760922 , Val-loss =  55.8074268892 , Time for epoch =  0.7188918590545654 s\n",
      "Epoch  943 / 1000  : Train-loss =  58.006871533 , Val-loss =  55.8488453454 , Time for epoch =  0.7565722465515137 s\n",
      "Epoch  944 / 1000  : Train-loss =  57.9514353813 , Val-loss =  55.8929481732 , Time for epoch =  0.8058342933654785 s\n",
      "Epoch  945 / 1000  : Train-loss =  57.8927731331 , Val-loss =  55.9342426144 , Time for epoch =  0.7883193492889404 s\n",
      "Epoch  946 / 1000  : Train-loss =  57.833410059 , Val-loss =  55.9764365934 , Time for epoch =  0.7930488586425781 s\n",
      "Epoch  947 / 1000  : Train-loss =  57.7786522215 , Val-loss =  56.0209107538 , Time for epoch =  0.8040719032287598 s\n",
      "Epoch  948 / 1000  : Train-loss =  57.7207345171 , Val-loss =  56.0627421459 , Time for epoch =  0.7631816864013672 s\n",
      "Epoch  949 / 1000  : Train-loss =  57.6621020403 , Val-loss =  56.1054311635 , Time for epoch =  0.7588343620300293 s\n",
      "Epoch  950 / 1000  : Train-loss =  57.6081370031 , Val-loss =  56.1499612496 , Time for epoch =  0.7399463653564453 s\n",
      "Epoch  951 / 1000  : Train-loss =  57.551016315 , Val-loss =  56.1920076371 , Time for epoch =  0.749793529510498 s\n",
      "Epoch  952 / 1000  : Train-loss =  57.4931923873 , Val-loss =  56.234857745 , Time for epoch =  0.7482521533966064 s\n",
      "Epoch  953 / 1000  : Train-loss =  57.4372195076 , Val-loss =  56.2784125847 , Time for epoch =  0.7358205318450928 s\n",
      "Epoch  954 / 1000  : Train-loss =  57.3852566689 , Val-loss =  56.3220949151 , Time for epoch =  0.7094326019287109 s\n",
      "Epoch  955 / 1000  : Train-loss =  57.3293107334 , Val-loss =  56.363691474 , Time for epoch =  0.7144138813018799 s\n",
      "Epoch  956 / 1000  : Train-loss =  57.2769883955 , Val-loss =  56.406667621 , Time for epoch =  0.7108533382415771 s\n",
      "Epoch  957 / 1000  : Train-loss =  57.2217280531 , Val-loss =  56.4476348685 , Time for epoch =  0.7161006927490234 s\n",
      "Epoch  958 / 1000  : Train-loss =  57.1656313211 , Val-loss =  56.4893418476 , Time for epoch =  0.7141396999359131 s\n",
      "Epoch  959 / 1000  : Train-loss =  57.1114110863 , Val-loss =  56.5312187945 , Time for epoch =  0.7003591060638428 s\n",
      "Epoch  960 / 1000  : Train-loss =  57.0583896391 , Val-loss =  56.5712805793 , Time for epoch =  0.6915390491485596 s\n",
      "Epoch  961 / 1000  : Train-loss =  57.0057011007 , Val-loss =  56.6076683547 , Time for epoch =  0.7003762722015381 s\n",
      "Epoch  962 / 1000  : Train-loss =  56.9533379065 , Val-loss =  56.6434579521 , Time for epoch =  0.7121965885162354 s\n",
      "Epoch  963 / 1000  : Train-loss =  56.901300256 , Val-loss =  56.6786309736 , Time for epoch =  0.7098836898803711 s\n",
      "Epoch  964 / 1000  : Train-loss =  56.8495898301 , Val-loss =  56.7131574742 , Time for epoch =  0.7104833126068115 s\n",
      "Epoch  965 / 1000  : Train-loss =  56.7982082942 , Val-loss =  56.7470049746 , Time for epoch =  0.7257189750671387 s\n",
      "Epoch  966 / 1000  : Train-loss =  56.747156945 , Val-loss =  56.7801408943 , Time for epoch =  0.7301120758056641 s\n",
      "Epoch  967 / 1000  : Train-loss =  56.7728648977 , Val-loss =  56.7957832521 , Time for epoch =  0.7305662631988525 s\n",
      "Epoch  968 / 1000  : Train-loss =  56.7278515934 , Val-loss =  56.8369623694 , Time for epoch =  0.7272329330444336 s\n",
      "Epoch  969 / 1000  : Train-loss =  56.6784451841 , Val-loss =  56.883280049 , Time for epoch =  0.7304577827453613 s\n",
      "Epoch  970 / 1000  : Train-loss =  56.6276642147 , Val-loss =  56.9309982416 , Time for epoch =  0.7282664775848389 s\n",
      "Epoch  971 / 1000  : Train-loss =  56.577046956 , Val-loss =  56.9779317453 , Time for epoch =  0.7263646125793457 s\n",
      "Epoch  972 / 1000  : Train-loss =  56.530898539 , Val-loss =  57.0214396686 , Time for epoch =  0.732100248336792 s\n",
      "Epoch  973 / 1000  : Train-loss =  56.4821352751 , Val-loss =  57.0631441362 , Time for epoch =  0.7296037673950195 s\n",
      "Epoch  974 / 1000  : Train-loss =  56.4321911096 , Val-loss =  57.104615071 , Time for epoch =  0.7280950546264648 s\n",
      "Epoch  975 / 1000  : Train-loss =  56.3825121004 , Val-loss =  57.1450365 , Time for epoch =  0.727961540222168 s\n",
      "Epoch  976 / 1000  : Train-loss =  56.3373880174 , Val-loss =  57.1814519323 , Time for epoch =  0.7200069427490234 s\n",
      "Epoch  977 / 1000  : Train-loss =  56.2896413725 , Val-loss =  57.2166941555 , Time for epoch =  0.7401597499847412 s\n",
      "Epoch  978 / 1000  : Train-loss =  56.240694669 , Val-loss =  57.251975469 , Time for epoch =  0.7604901790618896 s\n",
      "Epoch  979 / 1000  : Train-loss =  56.1962267366 , Val-loss =  57.2831446682 , Time for epoch =  0.7365150451660156 s\n",
      "Epoch  980 / 1000  : Train-loss =  56.1493376959 , Val-loss =  57.3136734005 , Time for epoch =  0.7622678279876709 s\n",
      "Epoch  981 / 1000  : Train-loss =  56.1011121492 , Val-loss =  57.3444549486 , Time for epoch =  0.74465012550354 s\n",
      "Epoch  982 / 1000  : Train-loss =  56.0531803944 , Val-loss =  57.3744837028 , Time for epoch =  0.7277011871337891 s\n",
      "Epoch  983 / 1000  : Train-loss =  56.0069308321 , Val-loss =  57.4016455672 , Time for epoch =  0.7181355953216553 s\n",
      "Epoch  984 / 1000  : Train-loss =  55.9649545005 , Val-loss =  57.4244445734 , Time for epoch =  0.7124249935150146 s\n",
      "Epoch  985 / 1000  : Train-loss =  55.9192865827 , Val-loss =  57.4477503692 , Time for epoch =  0.7108111381530762 s\n",
      "Epoch  986 / 1000  : Train-loss =  55.8723056317 , Val-loss =  57.471682015 , Time for epoch =  0.7049612998962402 s\n",
      "Epoch  987 / 1000  : Train-loss =  55.8269207461 , Val-loss =  57.4929195116 , Time for epoch =  0.7222373485565186 s\n",
      "Epoch  988 / 1000  : Train-loss =  55.78597227 , Val-loss =  57.5095882545 , Time for epoch =  0.6898632049560547 s\n",
      "Epoch  989 / 1000  : Train-loss =  55.7413162892 , Val-loss =  57.5273710255 , Time for epoch =  0.7018833160400391 s\n",
      "Epoch  990 / 1000  : Train-loss =  55.6952482849 , Val-loss =  57.5460081867 , Time for epoch =  0.7139675617218018 s\n",
      "Epoch  991 / 1000  : Train-loss =  55.6507962035 , Val-loss =  57.5618239808 , Time for epoch =  0.734748363494873 s\n",
      "Epoch  992 / 1000  : Train-loss =  55.6108154066 , Val-loss =  57.5727943062 , Time for epoch =  0.7442090511322021 s\n",
      "Epoch  993 / 1000  : Train-loss =  55.5670652707 , Val-loss =  57.585433805 , Time for epoch =  0.7288649082183838 s\n",
      "Epoch  994 / 1000  : Train-loss =  55.5260874364 , Val-loss =  57.5936681408 , Time for epoch =  0.7448883056640625 s\n",
      "Epoch  995 / 1000  : Train-loss =  55.4829061149 , Val-loss =  57.6036792518 , Time for epoch =  0.729006290435791 s\n",
      "Epoch  996 / 1000  : Train-loss =  55.4380792658 , Val-loss =  57.6148364339 , Time for epoch =  0.7822623252868652 s\n",
      "Epoch  997 / 1000  : Train-loss =  55.3979282875 , Val-loss =  57.6198426469 , Time for epoch =  0.7308721542358398 s\n",
      "Epoch  998 / 1000  : Train-loss =  55.3552759491 , Val-loss =  57.6266314445 , Time for epoch =  0.7762365341186523 s\n",
      "Epoch  999 / 1000  : Train-loss =  55.3026393511 , Val-loss =  57.7395401104 , Time for epoch =  0.7847845554351807 s\n",
      "Epoch  1000 / 1000  : Train-loss =  55.2695433465 , Val-loss =  57.8700451939 , Time for epoch =  0.7892556190490723 s\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAd4AAAEWCAYAAADIJfYaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa27bcb0748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12min 41s, sys: 980 ms, total: 12min 42s\n",
      "Wall time: 12min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train([X_train, y_train, X_test, y_test], batch_size, [w1, w2], n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
