{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"/home/rakesh47/NNfromScratch/PreparedData/X_train.csv\", header=None)\n",
    "y_train = pd.read_csv(\"/home/rakesh47/NNfromScratch/PreparedData/y_train.csv\", header=None)\n",
    "\n",
    "X_test = pd.read_csv(\"/home/rakesh47/NNfromScratch/PreparedData/X_test.csv\", header=None)\n",
    "y_test = pd.read_csv(\"/home/rakesh47/NNfromScratch/PreparedData/y_test.csv\", header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. User-defined hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((354, 13), (354, 1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D_in is input dimension; H is hidden dimension; D_out is output dimension.\n",
    "\n",
    "batch_size, D_in, H, D_out = 16, 13, 100, 1\n",
    "learning_rate = 1e-6\n",
    "n_epochs = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Random weight initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. Perform training : CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, batch_size, model, n_epochs):\n",
    "    \n",
    "    X_train, y_train, X_test, y_test = data\n",
    "    w1, w2 = model\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for t in range(n_epochs):\n",
    "\n",
    "        epoch_start = time.time()\n",
    "        train_loss = 0.\n",
    "        val_loss = 0. \n",
    "        for state in ['train', 'val']:        \n",
    "\n",
    "            batch_start_idx = 0\n",
    "            n_samples = len(X_train) if state=='train' else len(X_test)        \n",
    "            while batch_start_idx < n_samples:\n",
    "\n",
    "                # Get next batch of data \n",
    "                batch_end_idx = batch_start_idx + batch_size\n",
    "                if batch_end_idx > n_samples:  batch_end_idx = n_samples\n",
    "                if state == 'train':  x, y = np.array(X_train.iloc[batch_start_idx:batch_end_idx, :]), np.array(y_train.iloc[batch_start_idx:batch_end_idx, :])\n",
    "                else:  x, y = np.array(X_test.iloc[batch_start_idx:batch_end_idx, :]), np.array(y_test.iloc[batch_start_idx:batch_end_idx, :])\n",
    "\n",
    "                # Forward pass: Compute output and loss\n",
    "                h = x.dot(w1)\n",
    "                h_relu = np.maximum(h, 0)\n",
    "                y_pred = h_relu.dot(w2)\n",
    "                loss = np.square(y_pred - y).sum()\n",
    "                if state == 'train':  train_loss += loss\n",
    "                else:  val_loss += loss\n",
    "\n",
    "                # Backward pass: Compute gradients\n",
    "                if state == 'train':\n",
    "                    grad_y_pred = 2.0 * (y_pred - y)\n",
    "                    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "                    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "                    grad_h = grad_h_relu.copy()\n",
    "                    grad_h[h < 0] = 0\n",
    "                    grad_w1 = x.T.dot(grad_h)\n",
    "\n",
    "                # Update weights using gradient descent\n",
    "                    w1 -= learning_rate * grad_w1\n",
    "                    w2 -= learning_rate * grad_w2\n",
    "\n",
    "                batch_start_idx = batch_end_idx\n",
    "\n",
    "        # Print statistics\n",
    "        train_loss /= len(X_train)\n",
    "        train_losses.append(train_loss)\n",
    "        val_loss /= len(X_test)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        epoch_end = time.time()\n",
    "        epoch_time = epoch_end - epoch_start\n",
    "        print(\"Epoch \", (t+1), \"/\", n_epochs, \" : Train-loss = \", train_loss, \", Val-loss = \", val_loss, \", Time for epoch = \", epoch_time, \"s\")\n",
    "        \n",
    "    # Plot loss-curves\n",
    "    plt.figure()\n",
    "    plt.plot(range(2, n_epochs+1), train_losses[1:], label='Train-loss')\n",
    "    plt.plot(range(2, n_epochs+1), val_losses[1:], label='Val-loss')\n",
    "    plt.title('Loss curves')\n",
    "    plt.xlabel('No. of epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "    plt.grid(linestyle='dotted')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 / 1000  : Train-loss =  5.17944884589e+15 , Val-loss =  567.614394973 , Time for epoch =  0.044110774993896484 s\n",
      "Epoch  2 / 1000  : Train-loss =  595.078694865 , Val-loss =  566.159541659 , Time for epoch =  0.017217636108398438 s\n",
      "Epoch  3 / 1000  : Train-loss =  593.635212304 , Val-loss =  566.004848171 , Time for epoch =  0.012881040573120117 s\n",
      "Epoch  4 / 1000  : Train-loss =  593.185688686 , Val-loss =  565.62247275 , Time for epoch =  0.014408588409423828 s\n",
      "Epoch  5 / 1000  : Train-loss =  592.979044842 , Val-loss =  565.25105168 , Time for epoch =  0.013808488845825195 s\n",
      "Epoch  6 / 1000  : Train-loss =  592.838824644 , Val-loss =  564.963003004 , Time for epoch =  0.01249241828918457 s\n",
      "Epoch  7 / 1000  : Train-loss =  592.74124556 , Val-loss =  564.733318837 , Time for epoch =  0.011033296585083008 s\n",
      "Epoch  8 / 1000  : Train-loss =  592.670909441 , Val-loss =  564.544713789 , Time for epoch =  0.010937213897705078 s\n",
      "Epoch  9 / 1000  : Train-loss =  592.618304634 , Val-loss =  564.385871197 , Time for epoch =  0.01032567024230957 s\n",
      "Epoch  10 / 1000  : Train-loss =  592.577497311 , Val-loss =  564.249270287 , Time for epoch =  0.010342121124267578 s\n",
      "Epoch  11 / 1000  : Train-loss =  592.544715519 , Val-loss =  564.129779946 , Time for epoch =  0.010642290115356445 s\n",
      "Epoch  12 / 1000  : Train-loss =  592.517516274 , Val-loss =  564.023802344 , Time for epoch =  0.010385990142822266 s\n",
      "Epoch  13 / 1000  : Train-loss =  592.494289387 , Val-loss =  563.928746888 , Time for epoch =  0.010282516479492188 s\n",
      "Epoch  14 / 1000  : Train-loss =  592.473955835 , Val-loss =  563.84270037 , Time for epoch =  0.010312080383300781 s\n",
      "Epoch  15 / 1000  : Train-loss =  592.455780807 , Val-loss =  563.764215613 , Time for epoch =  0.01028895378112793 s\n",
      "Epoch  16 / 1000  : Train-loss =  592.439255844 , Val-loss =  563.692173245 , Time for epoch =  0.01167917251586914 s\n",
      "Epoch  17 / 1000  : Train-loss =  592.424023395 , Val-loss =  563.62568948 , Time for epoch =  0.01062464714050293 s\n",
      "Epoch  18 / 1000  : Train-loss =  592.409827868 , Val-loss =  563.564053435 , Time for epoch =  0.011704206466674805 s\n",
      "Epoch  19 / 1000  : Train-loss =  592.396466007 , Val-loss =  563.505790197 , Time for epoch =  0.012479066848754883 s\n",
      "Epoch  20 / 1000  : Train-loss =  592.384059428 , Val-loss =  563.451786775 , Time for epoch =  0.014372825622558594 s\n",
      "Epoch  21 / 1000  : Train-loss =  592.37225414 , Val-loss =  563.40128302 , Time for epoch =  0.014275312423706055 s\n",
      "Epoch  22 / 1000  : Train-loss =  592.360940303 , Val-loss =  563.353859597 , Time for epoch =  0.012626409530639648 s\n",
      "Epoch  23 / 1000  : Train-loss =  592.350055878 , Val-loss =  563.30919937 , Time for epoch =  0.014532089233398438 s\n",
      "Epoch  24 / 1000  : Train-loss =  592.339556317 , Val-loss =  563.267035803 , Time for epoch =  0.014155864715576172 s\n",
      "Epoch  25 / 1000  : Train-loss =  592.329406771 , Val-loss =  563.227137997 , Time for epoch =  0.014090776443481445 s\n",
      "Epoch  26 / 1000  : Train-loss =  592.319579071 , Val-loss =  563.189303736 , Time for epoch =  0.01150965690612793 s\n",
      "Epoch  27 / 1000  : Train-loss =  592.310050002 , Val-loss =  563.153354847 , Time for epoch =  0.010405540466308594 s\n",
      "Epoch  28 / 1000  : Train-loss =  592.300800138 , Val-loss =  563.119133623 , Time for epoch =  0.010408401489257812 s\n",
      "Epoch  29 / 1000  : Train-loss =  592.291812993 , Val-loss =  563.086499923 , Time for epoch =  0.010600805282592773 s\n",
      "Epoch  30 / 1000  : Train-loss =  592.283074382 , Val-loss =  563.056555295 , Time for epoch =  0.010261774063110352 s\n",
      "Epoch  31 / 1000  : Train-loss =  592.274571948 , Val-loss =  563.028033259 , Time for epoch =  0.010440587997436523 s\n",
      "Epoch  32 / 1000  : Train-loss =  592.266294796 , Val-loss =  563.000684344 , Time for epoch =  0.01069021224975586 s\n",
      "Epoch  33 / 1000  : Train-loss =  592.258233212 , Val-loss =  562.974426193 , Time for epoch =  0.013847589492797852 s\n",
      "Epoch  34 / 1000  : Train-loss =  592.250378446 , Val-loss =  562.949184298 , Time for epoch =  0.011725902557373047 s\n",
      "Epoch  35 / 1000  : Train-loss =  592.242722542 , Val-loss =  562.924891089 , Time for epoch =  0.012954950332641602 s\n",
      "Epoch  36 / 1000  : Train-loss =  592.235258207 , Val-loss =  562.901485154 , Time for epoch =  0.014461517333984375 s\n",
      "Epoch  37 / 1000  : Train-loss =  592.227978704 , Val-loss =  562.878910555 , Time for epoch =  0.012612581253051758 s\n",
      "Epoch  38 / 1000  : Train-loss =  592.220877766 , Val-loss =  562.857116238 , Time for epoch =  0.010540008544921875 s\n",
      "Epoch  39 / 1000  : Train-loss =  592.213949529 , Val-loss =  562.836055511 , Time for epoch =  0.010459661483764648 s\n",
      "Epoch  40 / 1000  : Train-loss =  592.20718848 , Val-loss =  562.815685591 , Time for epoch =  0.01030278205871582 s\n",
      "Epoch  41 / 1000  : Train-loss =  592.20058941 , Val-loss =  562.795967203 , Time for epoch =  0.010465860366821289 s\n",
      "Epoch  42 / 1000  : Train-loss =  592.194147375 , Val-loss =  562.776864228 , Time for epoch =  0.010367631912231445 s\n",
      "Epoch  43 / 1000  : Train-loss =  592.187857672 , Val-loss =  562.758343389 , Time for epoch =  0.010293960571289062 s\n",
      "Epoch  44 / 1000  : Train-loss =  592.181715809 , Val-loss =  562.740373974 , Time for epoch =  0.010523557662963867 s\n",
      "Epoch  45 / 1000  : Train-loss =  592.175593848 , Val-loss =  562.712720753 , Time for epoch =  0.010479211807250977 s\n",
      "Epoch  46 / 1000  : Train-loss =  592.167486196 , Val-loss =  562.689774095 , Time for epoch =  0.010329246520996094 s\n",
      "Epoch  47 / 1000  : Train-loss =  592.160023044 , Val-loss =  562.668372046 , Time for epoch =  0.01048588752746582 s\n",
      "Epoch  48 / 1000  : Train-loss =  592.1528889 , Val-loss =  562.647866961 , Time for epoch =  0.010480165481567383 s\n",
      "Epoch  49 / 1000  : Train-loss =  592.146014012 , Val-loss =  562.628098393 , Time for epoch =  0.010394811630249023 s\n",
      "Epoch  50 / 1000  : Train-loss =  592.139375113 , Val-loss =  562.609003155 , Time for epoch =  0.010497808456420898 s\n",
      "Epoch  51 / 1000  : Train-loss =  592.132958551 , Val-loss =  562.590539348 , Time for epoch =  0.010751008987426758 s\n",
      "Epoch  52 / 1000  : Train-loss =  592.126753129 , Val-loss =  562.5726714 , Time for epoch =  0.012263059616088867 s\n",
      "Epoch  53 / 1000  : Train-loss =  592.120748677 , Val-loss =  562.555366972 , Time for epoch =  0.018494844436645508 s\n",
      "Epoch  54 / 1000  : Train-loss =  592.114935728 , Val-loss =  562.538596188 , Time for epoch =  0.010880470275878906 s\n",
      "Epoch  55 / 1000  : Train-loss =  592.109305412 , Val-loss =  562.522331323 , Time for epoch =  0.010699748992919922 s\n",
      "Epoch  56 / 1000  : Train-loss =  592.103849404 , Val-loss =  562.506546599 , Time for epoch =  0.010609626770019531 s\n",
      "Epoch  57 / 1000  : Train-loss =  592.09855988 , Val-loss =  562.491218015 , Time for epoch =  0.011409997940063477 s\n",
      "Epoch  58 / 1000  : Train-loss =  592.093429476 , Val-loss =  562.476323199 , Time for epoch =  0.010875701904296875 s\n",
      "Epoch  59 / 1000  : Train-loss =  592.088451254 , Val-loss =  562.461841273 , Time for epoch =  0.010729789733886719 s\n",
      "Epoch  60 / 1000  : Train-loss =  592.083618673 , Val-loss =  562.44775273 , Time for epoch =  0.011047601699829102 s\n",
      "Epoch  61 / 1000  : Train-loss =  592.078925559 , Val-loss =  562.434039325 , Time for epoch =  0.010782718658447266 s\n",
      "Epoch  62 / 1000  : Train-loss =  592.07436608 , Val-loss =  562.420683975 , Time for epoch =  0.010714530944824219 s\n",
      "Epoch  63 / 1000  : Train-loss =  592.069934721 , Val-loss =  562.40767067 , Time for epoch =  0.010668516159057617 s\n",
      "Epoch  64 / 1000  : Train-loss =  592.065626262 , Val-loss =  562.394984385 , Time for epoch =  0.010493755340576172 s\n",
      "Epoch  65 / 1000  : Train-loss =  592.06143576 , Val-loss =  562.382611012 , Time for epoch =  0.01082754135131836 s\n",
      "Epoch  66 / 1000  : Train-loss =  592.057358531 , Val-loss =  562.370537287 , Time for epoch =  0.01127004623413086 s\n",
      "Epoch  67 / 1000  : Train-loss =  592.053390131 , Val-loss =  562.358750731 , Time for epoch =  0.013316631317138672 s\n",
      "Epoch  68 / 1000  : Train-loss =  592.049526342 , Val-loss =  562.347239589 , Time for epoch =  0.010680198669433594 s\n",
      "Epoch  69 / 1000  : Train-loss =  592.045763158 , Val-loss =  562.335992779 , Time for epoch =  0.010858297348022461 s\n",
      "Epoch  70 / 1000  : Train-loss =  592.04209677 , Val-loss =  562.324999848 , Time for epoch =  0.011967182159423828 s\n",
      "Epoch  71 / 1000  : Train-loss =  592.038523557 , Val-loss =  562.314250922 , Time for epoch =  0.015519142150878906 s\n",
      "Epoch  72 / 1000  : Train-loss =  592.035040071 , Val-loss =  562.303736671 , Time for epoch =  0.01410055160522461 s\n",
      "Epoch  73 / 1000  : Train-loss =  592.03164303 , Val-loss =  562.293448268 , Time for epoch =  0.014272689819335938 s\n",
      "Epoch  74 / 1000  : Train-loss =  592.028329307 , Val-loss =  562.283377359 , Time for epoch =  0.014075994491577148 s\n",
      "Epoch  75 / 1000  : Train-loss =  592.025095919 , Val-loss =  562.273516027 , Time for epoch =  0.011649847030639648 s\n",
      "Epoch  76 / 1000  : Train-loss =  592.021940022 , Val-loss =  562.263856767 , Time for epoch =  0.010560750961303711 s\n",
      "Epoch  77 / 1000  : Train-loss =  592.018941528 , Val-loss =  562.25183639 , Time for epoch =  0.01016855239868164 s\n",
      "Epoch  78 / 1000  : Train-loss =  592.014835906 , Val-loss =  562.240713348 , Time for epoch =  0.010564565658569336 s\n",
      "Epoch  79 / 1000  : Train-loss =  592.010918118 , Val-loss =  562.229878759 , Time for epoch =  0.01049661636352539 s\n",
      "Epoch  80 / 1000  : Train-loss =  592.007126792 , Val-loss =  562.219226731 , Time for epoch =  0.009991884231567383 s\n",
      "Epoch  81 / 1000  : Train-loss =  592.003447049 , Val-loss =  562.208736468 , Time for epoch =  0.010323047637939453 s\n",
      "Epoch  82 / 1000  : Train-loss =  591.999872121 , Val-loss =  562.198401736 , Time for epoch =  0.010239124298095703 s\n",
      "Epoch  83 / 1000  : Train-loss =  591.996396892 , Val-loss =  562.188218965 , Time for epoch =  0.010550260543823242 s\n",
      "Epoch  84 / 1000  : Train-loss =  591.993016793 , Val-loss =  562.178185223 , Time for epoch =  0.01027059555053711 s\n",
      "Epoch  85 / 1000  : Train-loss =  591.98972759 , Val-loss =  562.16829784 , Time for epoch =  0.013901710510253906 s\n",
      "Epoch  86 / 1000  : Train-loss =  591.986525323 , Val-loss =  562.158554324 , Time for epoch =  0.014572381973266602 s\n",
      "Epoch  87 / 1000  : Train-loss =  591.983406286 , Val-loss =  562.148952315 , Time for epoch =  0.015622377395629883 s\n",
      "Epoch  88 / 1000  : Train-loss =  591.988402419 , Val-loss =  562.123534659 , Time for epoch =  0.011121988296508789 s\n",
      "Epoch  89 / 1000  : Train-loss =  591.892923824 , Val-loss =  562.056697396 , Time for epoch =  0.010668039321899414 s\n",
      "Epoch  90 / 1000  : Train-loss =  591.806942998 , Val-loss =  561.98907651 , Time for epoch =  0.01040959358215332 s\n",
      "Epoch  91 / 1000  : Train-loss =  591.730729519 , Val-loss =  561.927097505 , Time for epoch =  0.01083827018737793 s\n",
      "Epoch  92 / 1000  : Train-loss =  591.663022257 , Val-loss =  561.870862381 , Time for epoch =  0.013930320739746094 s\n",
      "Epoch  93 / 1000  : Train-loss =  591.602419054 , Val-loss =  561.789040349 , Time for epoch =  0.01111149787902832 s\n",
      "Epoch  94 / 1000  : Train-loss =  591.54775527 , Val-loss =  561.706545167 , Time for epoch =  0.014059782028198242 s\n",
      "Epoch  95 / 1000  : Train-loss =  591.498103227 , Val-loss =  561.630771718 , Time for epoch =  0.013932466506958008 s\n",
      "Epoch  96 / 1000  : Train-loss =  591.452723024 , Val-loss =  561.560912312 , Time for epoch =  0.01250457763671875 s\n",
      "Epoch  97 / 1000  : Train-loss =  591.411019123 , Val-loss =  561.496295438 , Time for epoch =  0.010492324829101562 s\n",
      "Epoch  98 / 1000  : Train-loss =  591.372507316 , Val-loss =  561.42242262 , Time for epoch =  0.010218143463134766 s\n",
      "Epoch  99 / 1000  : Train-loss =  591.324589831 , Val-loss =  561.292608543 , Time for epoch =  0.011258363723754883 s\n",
      "Epoch  100 / 1000  : Train-loss =  591.271680696 , Val-loss =  561.171325817 , Time for epoch =  0.010839462280273438 s\n",
      "Epoch  101 / 1000  : Train-loss =  591.222948033 , Val-loss =  561.059440469 , Time for epoch =  0.010567903518676758 s\n",
      "Epoch  102 / 1000  : Train-loss =  591.1780453 , Val-loss =  560.95603913 , Time for epoch =  0.011124372482299805 s\n",
      "Epoch  103 / 1000  : Train-loss =  591.136514213 , Val-loss =  560.860192234 , Time for epoch =  0.011297225952148438 s\n",
      "Epoch  104 / 1000  : Train-loss =  591.097957004 , Val-loss =  560.771092442 , Time for epoch =  0.014835596084594727 s\n",
      "Epoch  105 / 1000  : Train-loss =  591.062038477 , Val-loss =  560.688044263 , Time for epoch =  0.011531591415405273 s\n",
      "Epoch  106 / 1000  : Train-loss =  591.028474982 , Val-loss =  560.610445299 , Time for epoch =  0.0142822265625 s\n",
      "Epoch  107 / 1000  : Train-loss =  590.997024717 , Val-loss =  560.537770587 , Time for epoch =  0.01395273208618164 s\n",
      "Epoch  108 / 1000  : Train-loss =  590.96748007 , Val-loss =  560.469560069 , Time for epoch =  0.01433253288269043 s\n",
      "Epoch  109 / 1000  : Train-loss =  590.939661579 , Val-loss =  560.405408524 , Time for epoch =  0.012330293655395508 s\n",
      "Epoch  110 / 1000  : Train-loss =  590.913413129 , Val-loss =  560.344957417 , Time for epoch =  0.014055490493774414 s\n",
      "Epoch  111 / 1000  : Train-loss =  590.875083962 , Val-loss =  560.263043461 , Time for epoch =  0.013964176177978516 s\n",
      "Epoch  112 / 1000  : Train-loss =  590.829482047 , Val-loss =  560.186309154 , Time for epoch =  0.012588262557983398 s\n",
      "Epoch  113 / 1000  : Train-loss =  590.787326501 , Val-loss =  560.114930326 , Time for epoch =  0.010529279708862305 s\n",
      "Epoch  114 / 1000  : Train-loss =  590.748290393 , Val-loss =  560.048377236 , Time for epoch =  0.010414838790893555 s\n",
      "Epoch  115 / 1000  : Train-loss =  590.712026866 , Val-loss =  559.956498773 , Time for epoch =  0.01066446304321289 s\n",
      "Epoch  116 / 1000  : Train-loss =  590.678237244 , Val-loss =  559.856730586 , Time for epoch =  0.010575532913208008 s\n",
      "Epoch  117 / 1000  : Train-loss =  590.646630854 , Val-loss =  559.762711768 , Time for epoch =  0.010596990585327148 s\n",
      "Epoch  118 / 1000  : Train-loss =  590.616878908 , Val-loss =  559.674630483 , Time for epoch =  0.010587692260742188 s\n",
      "Epoch  119 / 1000  : Train-loss =  590.588988504 , Val-loss =  559.591493426 , Time for epoch =  0.01171255111694336 s\n",
      "Epoch  120 / 1000  : Train-loss =  590.562740718 , Val-loss =  559.512851068 , Time for epoch =  0.013962268829345703 s\n",
      "Epoch  121 / 1000  : Train-loss =  590.537988612 , Val-loss =  559.414822043 , Time for epoch =  0.014266014099121094 s\n",
      "Epoch  122 / 1000  : Train-loss =  590.514604238 , Val-loss =  559.313888087 , Time for epoch =  0.014034271240234375 s\n",
      "Epoch  123 / 1000  : Train-loss =  590.492474385 , Val-loss =  559.217948126 , Time for epoch =  0.014073371887207031 s\n",
      "Epoch  124 / 1000  : Train-loss =  590.471498423 , Val-loss =  559.126611825 , Time for epoch =  0.011072874069213867 s\n",
      "Epoch  125 / 1000  : Train-loss =  590.451586565 , Val-loss =  559.039529351 , Time for epoch =  0.01102757453918457 s\n",
      "Epoch  126 / 1000  : Train-loss =  590.432658413 , Val-loss =  558.956386246 , Time for epoch =  0.01022648811340332 s\n",
      "Epoch  127 / 1000  : Train-loss =  590.414641728 , Val-loss =  558.876899055 , Time for epoch =  0.010419607162475586 s\n",
      "Epoch  128 / 1000  : Train-loss =  590.397471404 , Val-loss =  558.800811598 , Time for epoch =  0.010353803634643555 s\n",
      "Epoch  129 / 1000  : Train-loss =  590.382122454 , Val-loss =  558.719160586 , Time for epoch =  0.01035928726196289 s\n",
      "Epoch  130 / 1000  : Train-loss =  590.367425501 , Val-loss =  558.637599922 , Time for epoch =  0.010377168655395508 s\n",
      "Epoch  131 / 1000  : Train-loss =  590.353365272 , Val-loss =  558.559569502 , Time for epoch =  0.010100364685058594 s\n",
      "Epoch  132 / 1000  : Train-loss =  590.339907673 , Val-loss =  558.484835115 , Time for epoch =  0.010618209838867188 s\n",
      "Epoch  133 / 1000  : Train-loss =  590.327014433 , Val-loss =  558.413181483 , Time for epoch =  0.010925054550170898 s\n",
      "Epoch  134 / 1000  : Train-loss =  590.295406959 , Val-loss =  558.28115478 , Time for epoch =  0.010711193084716797 s\n",
      "Epoch  135 / 1000  : Train-loss =  590.2505912 , Val-loss =  558.1538576 , Time for epoch =  0.010664224624633789 s\n",
      "Epoch  136 / 1000  : Train-loss =  590.209956656 , Val-loss =  558.037602491 , Time for epoch =  0.010487794876098633 s\n",
      "Epoch  137 / 1000  : Train-loss =  590.170868661 , Val-loss =  557.928365732 , Time for epoch =  0.015722036361694336 s\n",
      "Epoch  138 / 1000  : Train-loss =  590.132902649 , Val-loss =  557.822472237 , Time for epoch =  0.015112638473510742 s\n",
      "Epoch  139 / 1000  : Train-loss =  590.100098425 , Val-loss =  557.725957487 , Time for epoch =  0.011072874069213867 s\n",
      "Epoch  140 / 1000  : Train-loss =  590.035512003 , Val-loss =  557.516390498 , Time for epoch =  0.011376619338989258 s\n",
      "Epoch  141 / 1000  : Train-loss =  589.914390612 , Val-loss =  557.298310715 , Time for epoch =  0.010814905166625977 s\n",
      "Epoch  142 / 1000  : Train-loss =  589.764116186 , Val-loss =  556.94108579 , Time for epoch =  0.010138988494873047 s\n",
      "Epoch  143 / 1000  : Train-loss =  589.494983016 , Val-loss =  556.347207159 , Time for epoch =  0.010567188262939453 s\n",
      "Epoch  144 / 1000  : Train-loss =  589.052199828 , Val-loss =  555.609016545 , Time for epoch =  0.010113000869750977 s\n",
      "Epoch  145 / 1000  : Train-loss =  588.423832014 , Val-loss =  554.42672222 , Time for epoch =  0.010246753692626953 s\n",
      "Epoch  146 / 1000  : Train-loss =  585.667402791 , Val-loss =  547.873413633 , Time for epoch =  0.010045766830444336 s\n",
      "Epoch  147 / 1000  : Train-loss =  568.402650485 , Val-loss =  525.605736852 , Time for epoch =  0.010771036148071289 s\n",
      "Epoch  148 / 1000  : Train-loss =  551.169253444 , Val-loss =  513.887425854 , Time for epoch =  0.010165214538574219 s\n",
      "Epoch  149 / 1000  : Train-loss =  541.432274823 , Val-loss =  508.440184792 , Time for epoch =  0.011081695556640625 s\n",
      "Epoch  150 / 1000  : Train-loss =  533.569605018 , Val-loss =  503.343724058 , Time for epoch =  0.010959625244140625 s\n",
      "Epoch  151 / 1000  : Train-loss =  523.652980033 , Val-loss =  497.971474626 , Time for epoch =  0.01076197624206543 s\n",
      "Epoch  152 / 1000  : Train-loss =  503.447054677 , Val-loss =  470.811968507 , Time for epoch =  0.010671615600585938 s\n",
      "Epoch  153 / 1000  : Train-loss =  478.44445779 , Val-loss =  412.554171837 , Time for epoch =  0.010531902313232422 s\n",
      "Epoch  154 / 1000  : Train-loss =  583.506104219 , Val-loss =  421.279480836 , Time for epoch =  0.010552167892456055 s\n",
      "Epoch  155 / 1000  : Train-loss =  455.016630324 , Val-loss =  332.642113953 , Time for epoch =  0.011308908462524414 s\n",
      "Epoch  156 / 1000  : Train-loss =  498.606567171 , Val-loss =  342.721249846 , Time for epoch =  0.014203786849975586 s\n",
      "Epoch  157 / 1000  : Train-loss =  357.498620053 , Val-loss =  300.002744073 , Time for epoch =  0.012361526489257812 s\n",
      "Epoch  158 / 1000  : Train-loss =  351.677460134 , Val-loss =  288.027955564 , Time for epoch =  0.011026859283447266 s\n",
      "Epoch  159 / 1000  : Train-loss =  290.027116679 , Val-loss =  288.35822721 , Time for epoch =  0.010194063186645508 s\n",
      "Epoch  160 / 1000  : Train-loss =  255.985302216 , Val-loss =  279.560284757 , Time for epoch =  0.010214090347290039 s\n",
      "Epoch  161 / 1000  : Train-loss =  222.98970969 , Val-loss =  260.442916354 , Time for epoch =  0.010514974594116211 s\n",
      "Epoch  162 / 1000  : Train-loss =  193.703890292 , Val-loss =  237.353440402 , Time for epoch =  0.010312557220458984 s\n",
      "Epoch  163 / 1000  : Train-loss =  169.898414709 , Val-loss =  212.048115287 , Time for epoch =  0.01027369499206543 s\n",
      "Epoch  164 / 1000  : Train-loss =  150.220183785 , Val-loss =  185.810361852 , Time for epoch =  0.010476827621459961 s\n",
      "Epoch  165 / 1000  : Train-loss =  133.611299844 , Val-loss =  160.457582104 , Time for epoch =  0.010354995727539062 s\n",
      "Epoch  166 / 1000  : Train-loss =  119.461363671 , Val-loss =  138.012989596 , Time for epoch =  0.01046299934387207 s\n",
      "Epoch  167 / 1000  : Train-loss =  107.728204734 , Val-loss =  120.156527796 , Time for epoch =  0.010374784469604492 s\n",
      "Epoch  168 / 1000  : Train-loss =  98.6230975008 , Val-loss =  107.330994434 , Time for epoch =  0.009939193725585938 s\n",
      "Epoch  169 / 1000  : Train-loss =  92.142753639 , Val-loss =  98.8759535129 , Time for epoch =  0.010461091995239258 s\n",
      "Epoch  170 / 1000  : Train-loss =  87.7834358476 , Val-loss =  93.4332011513 , Time for epoch =  0.010689020156860352 s\n",
      "Epoch  171 / 1000  : Train-loss =  84.7947125075 , Val-loss =  89.8310468313 , Time for epoch =  0.013935327529907227 s\n",
      "Epoch  172 / 1000  : Train-loss =  82.8538673317 , Val-loss =  87.6063591706 , Time for epoch =  0.014257431030273438 s\n",
      "Epoch  173 / 1000  : Train-loss =  81.6186503742 , Val-loss =  86.1854782252 , Time for epoch =  0.014751672744750977 s\n",
      "Epoch  174 / 1000  : Train-loss =  80.7473788373 , Val-loss =  85.1688612056 , Time for epoch =  0.012917041778564453 s\n",
      "Epoch  175 / 1000  : Train-loss =  80.0420686926 , Val-loss =  84.3517419199 , Time for epoch =  0.012736797332763672 s\n",
      "Epoch  176 / 1000  : Train-loss =  79.3953690961 , Val-loss =  84.0576055552 , Time for epoch =  0.012714385986328125 s\n",
      "Epoch  177 / 1000  : Train-loss =  78.9011115357 , Val-loss =  83.3588880496 , Time for epoch =  0.011638164520263672 s\n",
      "Epoch  178 / 1000  : Train-loss =  78.4719398267 , Val-loss =  82.7946526285 , Time for epoch =  0.011644363403320312 s\n",
      "Epoch  179 / 1000  : Train-loss =  78.119625663 , Val-loss =  82.3527300917 , Time for epoch =  0.011548042297363281 s\n",
      "Epoch  180 / 1000  : Train-loss =  77.808565102 , Val-loss =  81.9285634918 , Time for epoch =  0.011458635330200195 s\n",
      "Epoch  181 / 1000  : Train-loss =  77.5344343692 , Val-loss =  81.5371189709 , Time for epoch =  0.011201858520507812 s\n",
      "Epoch  182 / 1000  : Train-loss =  77.289480032 , Val-loss =  81.0897244869 , Time for epoch =  0.01107645034790039 s\n",
      "Epoch  183 / 1000  : Train-loss =  77.0560951873 , Val-loss =  80.7102055872 , Time for epoch =  0.013245105743408203 s\n",
      "Epoch  184 / 1000  : Train-loss =  76.848884589 , Val-loss =  80.3743317646 , Time for epoch =  0.015321493148803711 s\n",
      "Epoch  185 / 1000  : Train-loss =  76.6720942191 , Val-loss =  79.9592280943 , Time for epoch =  0.015201807022094727 s\n",
      "Epoch  186 / 1000  : Train-loss =  76.4870649621 , Val-loss =  79.6105955601 , Time for epoch =  0.015319585800170898 s\n",
      "Epoch  187 / 1000  : Train-loss =  76.3210649766 , Val-loss =  79.3040784029 , Time for epoch =  0.011816978454589844 s\n",
      "Epoch  188 / 1000  : Train-loss =  76.1722776628 , Val-loss =  78.8984626679 , Time for epoch =  0.017333507537841797 s\n",
      "Epoch  189 / 1000  : Train-loss =  76.0125572641 , Val-loss =  78.57042465 , Time for epoch =  0.018962383270263672 s\n",
      "Epoch  190 / 1000  : Train-loss =  75.869821453 , Val-loss =  78.2874387337 , Time for epoch =  0.011599302291870117 s\n",
      "Epoch  191 / 1000  : Train-loss =  75.7392097325 , Val-loss =  78.0324333395 , Time for epoch =  0.01700758934020996 s\n",
      "Epoch  192 / 1000  : Train-loss =  75.6179851767 , Val-loss =  77.7961325752 , Time for epoch =  0.01769876480102539 s\n",
      "Epoch  193 / 1000  : Train-loss =  75.5044053701 , Val-loss =  77.5733026737 , Time for epoch =  0.011704206466674805 s\n",
      "Epoch  194 / 1000  : Train-loss =  75.3972533079 , Val-loss =  77.3608559537 , Time for epoch =  0.015994787216186523 s\n",
      "Epoch  195 / 1000  : Train-loss =  75.2956196924 , Val-loss =  77.1568706165 , Time for epoch =  0.015505790710449219 s\n",
      "Epoch  196 / 1000  : Train-loss =  75.1987913854 , Val-loss =  76.9600750603 , Time for epoch =  0.014770984649658203 s\n",
      "Epoch  197 / 1000  : Train-loss =  75.1061894248 , Val-loss =  76.7695723656 , Time for epoch =  0.01394033432006836 s\n",
      "Epoch  198 / 1000  : Train-loss =  75.0173321574 , Val-loss =  76.5846906538 , Time for epoch =  0.012647867202758789 s\n",
      "Epoch  199 / 1000  : Train-loss =  74.9318119284 , Val-loss =  76.4049001504 , Time for epoch =  0.016569137573242188 s\n",
      "Epoch  200 / 1000  : Train-loss =  74.8492794722 , Val-loss =  76.2297659961 , Time for epoch =  0.01308131217956543 s\n",
      "Epoch  201 / 1000  : Train-loss =  74.7694328954 , Val-loss =  76.0585002216 , Time for epoch =  0.016955137252807617 s\n",
      "Epoch  202 / 1000  : Train-loss =  74.6920095287 , Val-loss =  75.8874616607 , Time for epoch =  0.015894651412963867 s\n",
      "Epoch  203 / 1000  : Train-loss =  74.616779664 , Val-loss =  75.7204131102 , Time for epoch =  0.014457464218139648 s\n",
      "Epoch  204 / 1000  : Train-loss =  74.5435415906 , Val-loss =  75.5570850098 , Time for epoch =  0.023335933685302734 s\n",
      "Epoch  205 / 1000  : Train-loss =  74.4706838153 , Val-loss =  75.3841027396 , Time for epoch =  0.016034841537475586 s\n",
      "Epoch  206 / 1000  : Train-loss =  74.3779269026 , Val-loss =  75.2083699684 , Time for epoch =  0.01737070083618164 s\n",
      "Epoch  207 / 1000  : Train-loss =  74.2743748143 , Val-loss =  74.9719148655 , Time for epoch =  0.017741680145263672 s\n",
      "Epoch  208 / 1000  : Train-loss =  74.1789444113 , Val-loss =  74.7679881378 , Time for epoch =  0.017818212509155273 s\n",
      "Epoch  209 / 1000  : Train-loss =  74.088818652 , Val-loss =  74.5831800718 , Time for epoch =  0.014357328414916992 s\n",
      "Epoch  210 / 1000  : Train-loss =  74.0024458453 , Val-loss =  74.4102417793 , Time for epoch =  0.014122486114501953 s\n",
      "Epoch  211 / 1000  : Train-loss =  73.9189684836 , Val-loss =  74.2452000213 , Time for epoch =  0.013785600662231445 s\n",
      "Epoch  212 / 1000  : Train-loss =  73.8378764865 , Val-loss =  74.0858366072 , Time for epoch =  0.016137361526489258 s\n",
      "Epoch  213 / 1000  : Train-loss =  73.7588430154 , Val-loss =  73.9308804786 , Time for epoch =  0.01816272735595703 s\n",
      "Epoch  214 / 1000  : Train-loss =  73.6816423097 , Val-loss =  73.7795754654 , Time for epoch =  0.015898942947387695 s\n",
      "Epoch  215 / 1000  : Train-loss =  73.6061068636 , Val-loss =  73.6314479146 , Time for epoch =  0.014358758926391602 s\n",
      "Epoch  216 / 1000  : Train-loss =  73.5321044273 , Val-loss =  73.4861812873 , Time for epoch =  0.014509201049804688 s\n",
      "Epoch  217 / 1000  : Train-loss =  73.4595253383 , Val-loss =  73.343548212 , Time for epoch =  0.013181686401367188 s\n",
      "Epoch  218 / 1000  : Train-loss =  73.3882753544 , Val-loss =  73.2033734806 , Time for epoch =  0.014008760452270508 s\n",
      "Epoch  219 / 1000  : Train-loss =  73.318271466 , Val-loss =  73.0655137436 , Time for epoch =  0.01191568374633789 s\n",
      "Epoch  220 / 1000  : Train-loss =  73.2494393445 , Val-loss =  72.9298462387 , Time for epoch =  0.020231246948242188 s\n",
      "Epoch  221 / 1000  : Train-loss =  73.1817117061 , Val-loss =  72.7962624214 , Time for epoch =  0.011089086532592773 s\n",
      "Epoch  222 / 1000  : Train-loss =  73.1150271967 , Val-loss =  72.6646642679 , Time for epoch =  0.010588645935058594 s\n",
      "Epoch  223 / 1000  : Train-loss =  73.0493295849 , Val-loss =  72.5349620447 , Time for epoch =  0.010480880737304688 s\n",
      "Epoch  224 / 1000  : Train-loss =  72.9845671434 , Val-loss =  72.407072893 , Time for epoch =  0.010599851608276367 s\n",
      "Epoch  225 / 1000  : Train-loss =  72.9206921517 , Val-loss =  72.2809198727 , Time for epoch =  0.010620594024658203 s\n",
      "Epoch  226 / 1000  : Train-loss =  72.8576604819 , Val-loss =  72.1564312727 , Time for epoch =  0.010457038879394531 s\n",
      "Epoch  227 / 1000  : Train-loss =  72.7954312448 , Val-loss =  72.0335400814 , Time for epoch =  0.01410222053527832 s\n",
      "Epoch  228 / 1000  : Train-loss =  72.7339664825 , Val-loss =  71.9121835575 , Time for epoch =  0.011287212371826172 s\n",
      "Epoch  229 / 1000  : Train-loss =  72.6732308967 , Val-loss =  71.7923028692 , Time for epoch =  0.011489152908325195 s\n",
      "Epoch  230 / 1000  : Train-loss =  72.6131916091 , Val-loss =  71.6738427799 , Time for epoch =  0.013630390167236328 s\n",
      "Epoch  231 / 1000  : Train-loss =  72.553817947 , Val-loss =  71.5567513713 , Time for epoch =  0.015769243240356445 s\n",
      "Epoch  232 / 1000  : Train-loss =  72.4950812512 , Val-loss =  71.4409797946 , Time for epoch =  0.015233278274536133 s\n",
      "Epoch  233 / 1000  : Train-loss =  72.4369547038 , Val-loss =  71.3264820463 , Time for epoch =  0.015330791473388672 s\n",
      "Epoch  234 / 1000  : Train-loss =  72.3794131735 , Val-loss =  71.2132147638 , Time for epoch =  0.013656854629516602 s\n",
      "Epoch  235 / 1000  : Train-loss =  72.3224330755 , Val-loss =  71.0941404393 , Time for epoch =  0.013389825820922852 s\n",
      "Epoch  236 / 1000  : Train-loss =  72.2659922457 , Val-loss =  70.9736160513 , Time for epoch =  0.011307477951049805 s\n",
      "Epoch  237 / 1000  : Train-loss =  72.2100698261 , Val-loss =  70.8544394523 , Time for epoch =  0.011195659637451172 s\n",
      "Epoch  238 / 1000  : Train-loss =  72.1546461621 , Val-loss =  70.7365675763 , Time for epoch =  0.01120448112487793 s\n",
      "Epoch  239 / 1000  : Train-loss =  72.0997027083 , Val-loss =  70.61995963 , Time for epoch =  0.01129770278930664 s\n",
      "Epoch  240 / 1000  : Train-loss =  72.0452219441 , Val-loss =  70.5045769443 , Time for epoch =  0.011119842529296875 s\n",
      "Epoch  241 / 1000  : Train-loss =  71.9911872955 , Val-loss =  70.3903828368 , Time for epoch =  0.010771989822387695 s\n",
      "Epoch  242 / 1000  : Train-loss =  71.9375830659 , Val-loss =  70.2773424857 , Time for epoch =  0.011639833450317383 s\n",
      "Epoch  243 / 1000  : Train-loss =  71.8843943709 , Val-loss =  70.165422812 , Time for epoch =  0.011251449584960938 s\n",
      "Epoch  244 / 1000  : Train-loss =  71.8316070808 , Val-loss =  70.054592371 , Time for epoch =  0.012124061584472656 s\n",
      "Epoch  245 / 1000  : Train-loss =  71.7792077664 , Val-loss =  69.9448212513 , Time for epoch =  0.016832351684570312 s\n",
      "Epoch  246 / 1000  : Train-loss =  71.7271836509 , Val-loss =  69.8360809812 , Time for epoch =  0.011954307556152344 s\n",
      "Epoch  247 / 1000  : Train-loss =  71.6755225647 , Val-loss =  69.7283444419 , Time for epoch =  0.01266932487487793 s\n",
      "Epoch  248 / 1000  : Train-loss =  71.6242129047 , Val-loss =  69.6215857863 , Time for epoch =  0.016662120819091797 s\n",
      "Epoch  249 / 1000  : Train-loss =  71.5732435968 , Val-loss =  69.5157803639 , Time for epoch =  0.011290550231933594 s\n",
      "Epoch  250 / 1000  : Train-loss =  71.522604061 , Val-loss =  69.4109046501 , Time for epoch =  0.010588407516479492 s\n",
      "Epoch  251 / 1000  : Train-loss =  71.4687116725 , Val-loss =  69.3023514576 , Time for epoch =  0.010154008865356445 s\n",
      "Epoch  252 / 1000  : Train-loss =  71.4142761383 , Val-loss =  69.1943221948 , Time for epoch =  0.01099848747253418 s\n",
      "Epoch  253 / 1000  : Train-loss =  71.3602570895 , Val-loss =  69.0872153525 , Time for epoch =  0.01191854476928711 s\n",
      "Epoch  254 / 1000  : Train-loss =  71.3066657928 , Val-loss =  68.9810733648 , Time for epoch =  0.011089563369750977 s\n",
      "Epoch  255 / 1000  : Train-loss =  71.2534931736 , Val-loss =  68.8759052095 , Time for epoch =  0.011214017868041992 s\n",
      "Epoch  256 / 1000  : Train-loss =  71.1989354448 , Val-loss =  68.7677822161 , Time for epoch =  0.011641740798950195 s\n",
      "Epoch  257 / 1000  : Train-loss =  71.1417633332 , Val-loss =  68.6602442623 , Time for epoch =  0.011392354965209961 s\n",
      "Epoch  258 / 1000  : Train-loss =  71.0851128571 , Val-loss =  68.5536090613 , Time for epoch =  0.011744022369384766 s\n",
      "Epoch  259 / 1000  : Train-loss =  71.028988259 , Val-loss =  68.447958721 , Time for epoch =  0.01151895523071289 s\n",
      "Epoch  260 / 1000  : Train-loss =  70.973378608 , Val-loss =  68.3433206821 , Time for epoch =  0.012772321701049805 s\n",
      "Epoch  261 / 1000  : Train-loss =  70.9182691441 , Val-loss =  68.2396948014 , Time for epoch =  0.013778209686279297 s\n",
      "Epoch  262 / 1000  : Train-loss =  70.8636435805 , Val-loss =  68.1370673172 , Time for epoch =  0.013719797134399414 s\n",
      "Epoch  263 / 1000  : Train-loss =  70.8094852673 , Val-loss =  68.0354180769 , Time for epoch =  0.015668869018554688 s\n",
      "Epoch  264 / 1000  : Train-loss =  70.755777768 , Val-loss =  67.9347242528 , Time for epoch =  0.014744997024536133 s\n",
      "Epoch  265 / 1000  : Train-loss =  70.7025051323 , Val-loss =  67.8349622318 , Time for epoch =  0.01201319694519043 s\n",
      "Epoch  266 / 1000  : Train-loss =  70.6496520132 , Val-loss =  67.736108557 , Time for epoch =  0.011647939682006836 s\n",
      "Epoch  267 / 1000  : Train-loss =  70.5972037049 , Val-loss =  67.6381403821 , Time for epoch =  0.012387514114379883 s\n",
      "Epoch  268 / 1000  : Train-loss =  70.545146142 , Val-loss =  67.5410356725 , Time for epoch =  0.012316465377807617 s\n",
      "Epoch  269 / 1000  : Train-loss =  70.4934658794 , Val-loss =  67.4447732801 , Time for epoch =  0.01430821418762207 s\n",
      "Epoch  270 / 1000  : Train-loss =  70.4421500645 , Val-loss =  67.3493329537 , Time for epoch =  0.015362024307250977 s\n",
      "Epoch  271 / 1000  : Train-loss =  70.3911864065 , Val-loss =  67.2546953178 , Time for epoch =  0.011887073516845703 s\n",
      "Epoch  272 / 1000  : Train-loss =  70.340563145 , Val-loss =  67.1608418383 , Time for epoch =  0.015444517135620117 s\n",
      "Epoch  273 / 1000  : Train-loss =  70.2902690197 , Val-loss =  67.0677547813 , Time for epoch =  0.015299797058105469 s\n",
      "Epoch  274 / 1000  : Train-loss =  70.2402932421 , Val-loss =  66.9754171718 , Time for epoch =  0.015505313873291016 s\n",
      "Epoch  275 / 1000  : Train-loss =  70.1906254686 , Val-loss =  66.8838127518 , Time for epoch =  0.011715412139892578 s\n",
      "Epoch  276 / 1000  : Train-loss =  70.1412557749 , Val-loss =  66.7929259411 , Time for epoch =  0.015689611434936523 s\n",
      "Epoch  277 / 1000  : Train-loss =  70.0921746326 , Val-loss =  66.7027417997 , Time for epoch =  0.01757979393005371 s\n",
      "Epoch  278 / 1000  : Train-loss =  70.0433728874 , Val-loss =  66.6132459924 , Time for epoch =  0.01186370849609375 s\n",
      "Epoch  279 / 1000  : Train-loss =  69.9940221607 , Val-loss =  66.5228942457 , Time for epoch =  0.013795852661132812 s\n",
      "Epoch  280 / 1000  : Train-loss =  69.9442183307 , Val-loss =  66.4329073557 , Time for epoch =  0.012460708618164062 s\n",
      "Epoch  281 / 1000  : Train-loss =  69.894704735 , Val-loss =  66.343462186 , Time for epoch =  0.015185832977294922 s\n",
      "Epoch  282 / 1000  : Train-loss =  69.8454860336 , Val-loss =  66.2546399256 , Time for epoch =  0.015251636505126953 s\n",
      "Epoch  283 / 1000  : Train-loss =  69.796559433 , Val-loss =  66.1664739331 , Time for epoch =  0.015050649642944336 s\n",
      "Epoch  284 / 1000  : Train-loss =  69.7479185956 , Val-loss =  66.0789738195 , Time for epoch =  0.01238393783569336 s\n",
      "Epoch  285 / 1000  : Train-loss =  69.6995556043 , Val-loss =  65.9921375624 , Time for epoch =  0.011945962905883789 s\n",
      "Epoch  286 / 1000  : Train-loss =  69.6514619419 , Val-loss =  65.9059575804 , Time for epoch =  0.011431455612182617 s\n",
      "Epoch  287 / 1000  : Train-loss =  69.6036289724 , Val-loss =  65.8204237672 , Time for epoch =  0.011593341827392578 s\n",
      "Epoch  288 / 1000  : Train-loss =  69.5560481723 , Val-loss =  65.7355249968 , Time for epoch =  0.011420726776123047 s\n",
      "Epoch  289 / 1000  : Train-loss =  69.5087112368 , Val-loss =  65.651249864 , Time for epoch =  0.01197195053100586 s\n",
      "Epoch  290 / 1000  : Train-loss =  69.4616101244 , Val-loss =  65.567587041 , Time for epoch =  0.012693166732788086 s\n",
      "Epoch  291 / 1000  : Train-loss =  69.4147370706 , Val-loss =  65.4845254442 , Time for epoch =  0.012618064880371094 s\n",
      "Epoch  292 / 1000  : Train-loss =  69.3680845869 , Val-loss =  65.4020543063 , Time for epoch =  0.015212297439575195 s\n",
      "Epoch  293 / 1000  : Train-loss =  69.3216454535 , Val-loss =  65.3201632017 , Time for epoch =  0.011946678161621094 s\n",
      "Epoch  294 / 1000  : Train-loss =  69.2754127083 , Val-loss =  65.23884205 , Time for epoch =  0.014366865158081055 s\n",
      "Epoch  295 / 1000  : Train-loss =  69.2293796359 , Val-loss =  65.158081109 , Time for epoch =  0.012872457504272461 s\n",
      "Epoch  296 / 1000  : Train-loss =  69.1835397553 , Val-loss =  65.0778709618 , Time for epoch =  0.012453794479370117 s\n",
      "Epoch  297 / 1000  : Train-loss =  69.1377398266 , Val-loss =  64.9984610129 , Time for epoch =  0.012250423431396484 s\n",
      "Epoch  298 / 1000  : Train-loss =  69.0885761313 , Val-loss =  64.9365667669 , Time for epoch =  0.011361360549926758 s\n",
      "Epoch  299 / 1000  : Train-loss =  69.0410020597 , Val-loss =  64.8738909548 , Time for epoch =  0.011802434921264648 s\n",
      "Epoch  300 / 1000  : Train-loss =  68.9939937619 , Val-loss =  64.8098639272 , Time for epoch =  0.012615203857421875 s\n",
      "Epoch  301 / 1000  : Train-loss =  68.9474975741 , Val-loss =  64.7460411579 , Time for epoch =  0.01222848892211914 s\n",
      "Epoch  302 / 1000  : Train-loss =  68.9014634598 , Val-loss =  64.6824003154 , Time for epoch =  0.012078046798706055 s\n",
      "Epoch  303 / 1000  : Train-loss =  68.8558458633 , Val-loss =  64.6189168324 , Time for epoch =  0.011847496032714844 s\n",
      "Epoch  304 / 1000  : Train-loss =  68.8106038026 , Val-loss =  64.5555679047 , Time for epoch =  0.011941909790039062 s\n",
      "Epoch  305 / 1000  : Train-loss =  68.7657006296 , Val-loss =  64.4923340355 , Time for epoch =  0.011882543563842773 s\n",
      "Epoch  306 / 1000  : Train-loss =  68.7211036651 , Val-loss =  64.4291994335 , Time for epoch =  0.012048721313476562 s\n",
      "Epoch  307 / 1000  : Train-loss =  68.676783802 , Val-loss =  64.3661518975 , Time for epoch =  0.013926029205322266 s\n",
      "Epoch  308 / 1000  : Train-loss =  68.6327151204 , Val-loss =  64.3031824909 , Time for epoch =  0.01845836639404297 s\n",
      "Epoch  309 / 1000  : Train-loss =  68.5863990531 , Val-loss =  64.2500426886 , Time for epoch =  0.01926589012145996 s\n",
      "Epoch  310 / 1000  : Train-loss =  68.5359277087 , Val-loss =  64.2075598676 , Time for epoch =  0.016057491302490234 s\n",
      "Epoch  311 / 1000  : Train-loss =  68.4891873118 , Val-loss =  64.1635479937 , Time for epoch =  0.011658430099487305 s\n",
      "Epoch  312 / 1000  : Train-loss =  68.4438454418 , Val-loss =  64.116537371 , Time for epoch =  0.011051177978515625 s\n",
      "Epoch  313 / 1000  : Train-loss =  68.3995913684 , Val-loss =  64.0655049082 , Time for epoch =  0.012420177459716797 s\n",
      "Epoch  314 / 1000  : Train-loss =  68.3561783035 , Val-loss =  64.0130741609 , Time for epoch =  0.015294075012207031 s\n",
      "Epoch  315 / 1000  : Train-loss =  68.3134096556 , Val-loss =  63.9593100025 , Time for epoch =  0.01672959327697754 s\n",
      "Epoch  316 / 1000  : Train-loss =  68.271129824 , Val-loss =  63.9042899489 , Time for epoch =  0.014299392700195312 s\n",
      "Epoch  317 / 1000  : Train-loss =  68.2292164086 , Val-loss =  63.848101088 , Time for epoch =  0.014326333999633789 s\n",
      "Epoch  318 / 1000  : Train-loss =  68.1875737394 , Val-loss =  63.7908366642 , Time for epoch =  0.010872125625610352 s\n",
      "Epoch  319 / 1000  : Train-loss =  68.1461275621 , Val-loss =  63.7325929744 , Time for epoch =  0.014294862747192383 s\n",
      "Epoch  320 / 1000  : Train-loss =  68.1048207021 , Val-loss =  63.673466793 , Time for epoch =  0.014355182647705078 s\n",
      "Epoch  321 / 1000  : Train-loss =  68.0636095441 , Val-loss =  63.6135533577 , Time for epoch =  0.015515327453613281 s\n",
      "Epoch  322 / 1000  : Train-loss =  68.0224611852 , Val-loss =  63.5529448641 , Time for epoch =  0.012163400650024414 s\n",
      "Epoch  323 / 1000  : Train-loss =  67.981351141 , Val-loss =  63.4917293953 , Time for epoch =  0.011681795120239258 s\n",
      "Epoch  324 / 1000  : Train-loss =  67.9402615025 , Val-loss =  63.4299902055 , Time for epoch =  0.014343976974487305 s\n",
      "Epoch  325 / 1000  : Train-loss =  67.8991794615 , Val-loss =  63.3678052879 , Time for epoch =  0.014541864395141602 s\n",
      "Epoch  326 / 1000  : Train-loss =  67.8580961341 , Val-loss =  63.3052471612 , Time for epoch =  0.015200138092041016 s\n",
      "Epoch  327 / 1000  : Train-loss =  67.8170056266 , Val-loss =  63.2423828249 , Time for epoch =  0.010905742645263672 s\n",
      "Epoch  328 / 1000  : Train-loss =  67.7759042966 , Val-loss =  63.1792738385 , Time for epoch =  0.014489412307739258 s\n",
      "Epoch  329 / 1000  : Train-loss =  67.7347901711 , Val-loss =  63.1159764907 , Time for epoch =  0.014497518539428711 s\n",
      "Epoch  330 / 1000  : Train-loss =  67.6936624915 , Val-loss =  63.0525420317 , Time for epoch =  0.014402389526367188 s\n",
      "Epoch  331 / 1000  : Train-loss =  67.6525213589 , Val-loss =  62.9890169459 , Time for epoch =  0.014393091201782227 s\n",
      "Epoch  332 / 1000  : Train-loss =  67.6113674607 , Val-loss =  62.9254432484 , Time for epoch =  0.011681795120239258 s\n",
      "Epoch  333 / 1000  : Train-loss =  67.5702018606 , Val-loss =  62.8627158978 , Time for epoch =  0.011630535125732422 s\n",
      "Epoch  334 / 1000  : Train-loss =  67.5290258397 , Val-loss =  62.8000756303 , Time for epoch =  0.01096034049987793 s\n",
      "Epoch  335 / 1000  : Train-loss =  67.4878407774 , Val-loss =  62.7374393828 , Time for epoch =  0.011138916015625 s\n",
      "Epoch  336 / 1000  : Train-loss =  67.4466480638 , Val-loss =  62.6748394126 , Time for epoch =  0.010818958282470703 s\n",
      "Epoch  337 / 1000  : Train-loss =  67.4054490361 , Val-loss =  62.6123044967 , Time for epoch =  0.012457132339477539 s\n",
      "Epoch  338 / 1000  : Train-loss =  67.3642449348 , Val-loss =  62.5498602493 , Time for epoch =  0.013739824295043945 s\n",
      "Epoch  339 / 1000  : Train-loss =  67.3230368734 , Val-loss =  62.4875294178 , Time for epoch =  0.01265096664428711 s\n",
      "Epoch  340 / 1000  : Train-loss =  67.2818258206 , Val-loss =  62.4253321563 , Time for epoch =  0.012042760848999023 s\n",
      "Epoch  341 / 1000  : Train-loss =  67.2406125895 , Val-loss =  62.3632862777 , Time for epoch =  0.010974407196044922 s\n",
      "Epoch  342 / 1000  : Train-loss =  67.1993978344 , Val-loss =  62.3014074854 , Time for epoch =  0.010843515396118164 s\n",
      "Epoch  343 / 1000  : Train-loss =  67.1581820513 , Val-loss =  62.2397095839 , Time for epoch =  0.010788202285766602 s\n",
      "Epoch  344 / 1000  : Train-loss =  67.1169655818 , Val-loss =  62.178204672 , Time for epoch =  0.010262250900268555 s\n",
      "Epoch  345 / 1000  : Train-loss =  67.07574862 , Val-loss =  62.1169033176 , Time for epoch =  0.01062631607055664 s\n",
      "Epoch  346 / 1000  : Train-loss =  67.0345312203 , Val-loss =  62.0558147166 , Time for epoch =  0.010323047637939453 s\n",
      "Epoch  347 / 1000  : Train-loss =  66.9929463188 , Val-loss =  61.9937058404 , Time for epoch =  0.014424800872802734 s\n",
      "Epoch  348 / 1000  : Train-loss =  66.9512477214 , Val-loss =  61.9316784716 , Time for epoch =  0.014142990112304688 s\n",
      "Epoch  349 / 1000  : Train-loss =  66.9095486309 , Val-loss =  61.8698494832 , Time for epoch =  0.014244318008422852 s\n",
      "Epoch  350 / 1000  : Train-loss =  66.8678554172 , Val-loss =  61.80826967 , Time for epoch =  0.018258333206176758 s\n",
      "Epoch  351 / 1000  : Train-loss =  66.8261699645 , Val-loss =  61.7469617698 , Time for epoch =  0.017047405242919922 s\n",
      "Epoch  352 / 1000  : Train-loss =  66.7844922622 , Val-loss =  61.6859359768 , Time for epoch =  0.015616416931152344 s\n",
      "Epoch  353 / 1000  : Train-loss =  66.7428214577 , Val-loss =  61.6251967941 , Time for epoch =  0.01184535026550293 s\n",
      "Epoch  354 / 1000  : Train-loss =  66.7011563209 , Val-loss =  61.5647460664 , Time for epoch =  0.012704849243164062 s\n",
      "Epoch  355 / 1000  : Train-loss =  66.6594954492 , Val-loss =  61.5045843281 , Time for epoch =  0.012199878692626953 s\n",
      "Epoch  356 / 1000  : Train-loss =  66.6178373592 , Val-loss =  61.4447114128 , Time for epoch =  0.012275457382202148 s\n",
      "Epoch  357 / 1000  : Train-loss =  66.5761805283 , Val-loss =  61.3851267355 , Time for epoch =  0.014078140258789062 s\n",
      "Epoch  358 / 1000  : Train-loss =  66.5345234143 , Val-loss =  61.3258294315 , Time for epoch =  0.014084815979003906 s\n",
      "Epoch  359 / 1000  : Train-loss =  66.4928644654 , Val-loss =  61.266181199 , Time for epoch =  0.011069536209106445 s\n",
      "Epoch  360 / 1000  : Train-loss =  66.451202126 , Val-loss =  61.2056091392 , Time for epoch =  0.010863780975341797 s\n",
      "Epoch  361 / 1000  : Train-loss =  66.4095348399 , Val-loss =  61.1453495432 , Time for epoch =  0.010830163955688477 s\n",
      "Epoch  362 / 1000  : Train-loss =  66.3678610533 , Val-loss =  61.0854002846 , Time for epoch =  0.010640382766723633 s\n",
      "Epoch  363 / 1000  : Train-loss =  66.3261792168 , Val-loss =  61.0257591645 , Time for epoch =  0.010473012924194336 s\n",
      "Epoch  364 / 1000  : Train-loss =  66.284487787 , Val-loss =  60.9664239273 , Time for epoch =  0.01154327392578125 s\n",
      "Epoch  365 / 1000  : Train-loss =  66.2427852281 , Val-loss =  60.9073922747 , Time for epoch =  0.010245800018310547 s\n",
      "Epoch  366 / 1000  : Train-loss =  66.2010700131 , Val-loss =  60.8487825124 , Time for epoch =  0.010735511779785156 s\n",
      "Epoch  367 / 1000  : Train-loss =  66.1593406247 , Val-loss =  60.7905940281 , Time for epoch =  0.010649442672729492 s\n",
      "Epoch  368 / 1000  : Train-loss =  66.1175955567 , Val-loss =  60.732701982 , Time for epoch =  0.0108795166015625 s\n",
      "Epoch  369 / 1000  : Train-loss =  66.0758333147 , Val-loss =  60.6751037956 , Time for epoch =  0.011446714401245117 s\n",
      "Epoch  370 / 1000  : Train-loss =  66.0340524166 , Val-loss =  60.6177969018 , Time for epoch =  0.011432170867919922 s\n",
      "Epoch  371 / 1000  : Train-loss =  65.9922513934 , Val-loss =  60.5607787507 , Time for epoch =  0.011020898818969727 s\n",
      "Epoch  372 / 1000  : Train-loss =  65.9504287898 , Val-loss =  60.504046816 , Time for epoch =  0.013484954833984375 s\n",
      "Epoch  373 / 1000  : Train-loss =  65.9113369498 , Val-loss =  60.4640438322 , Time for epoch =  0.01082301139831543 s\n",
      "Epoch  374 / 1000  : Train-loss =  65.8723845169 , Val-loss =  60.4224438439 , Time for epoch =  0.011045217514038086 s\n",
      "Epoch  375 / 1000  : Train-loss =  65.8332090745 , Val-loss =  60.3794649032 , Time for epoch =  0.01075124740600586 s\n",
      "Epoch  376 / 1000  : Train-loss =  65.7938013311 , Val-loss =  60.3352821847 , Time for epoch =  0.010563373565673828 s\n",
      "Epoch  377 / 1000  : Train-loss =  65.7541580551 , Val-loss =  60.2900463739 , Time for epoch =  0.010512113571166992 s\n",
      "Epoch  378 / 1000  : Train-loss =  65.714280936 , Val-loss =  60.2438914162 , Time for epoch =  0.010487794876098633 s\n",
      "Epoch  379 / 1000  : Train-loss =  65.6741753223 , Val-loss =  60.1969379609 , Time for epoch =  0.010580778121948242 s\n",
      "Epoch  380 / 1000  : Train-loss =  65.6338490984 , Val-loss =  60.1492951005 , Time for epoch =  0.010442972183227539 s\n",
      "Epoch  381 / 1000  : Train-loss =  65.5933117736 , Val-loss =  60.1010614557 , Time for epoch =  0.010214090347290039 s\n",
      "Epoch  382 / 1000  : Train-loss =  65.5525737739 , Val-loss =  60.0523260192 , Time for epoch =  0.010495901107788086 s\n",
      "Epoch  383 / 1000  : Train-loss =  65.5116459121 , Val-loss =  60.0031689105 , Time for epoch =  0.010361671447753906 s\n",
      "Epoch  384 / 1000  : Train-loss =  65.4705390001 , Val-loss =  59.9536620927 , Time for epoch =  0.010364532470703125 s\n",
      "Epoch  385 / 1000  : Train-loss =  65.4292635748 , Val-loss =  59.903870062 , Time for epoch =  0.010402441024780273 s\n",
      "Epoch  386 / 1000  : Train-loss =  65.3878297108 , Val-loss =  59.8538505094 , Time for epoch =  0.01078033447265625 s\n",
      "Epoch  387 / 1000  : Train-loss =  65.3462468998 , Val-loss =  59.8036549489 , Time for epoch =  0.01051640510559082 s\n",
      "Epoch  388 / 1000  : Train-loss =  65.3045239797 , Val-loss =  59.7533293089 , Time for epoch =  0.016496658325195312 s\n",
      "Epoch  389 / 1000  : Train-loss =  65.2626691004 , Val-loss =  59.7029144835 , Time for epoch =  0.01695418357849121 s\n",
      "Epoch  390 / 1000  : Train-loss =  65.2206897165 , Val-loss =  59.652446842 , Time for epoch =  0.011590003967285156 s\n",
      "Epoch  391 / 1000  : Train-loss =  65.1785925991 , Val-loss =  59.6019586976 , Time for epoch =  0.013950824737548828 s\n",
      "Epoch  392 / 1000  : Train-loss =  65.13638386 , Val-loss =  59.5514787357 , Time for epoch =  0.014065027236938477 s\n",
      "Epoch  393 / 1000  : Train-loss =  65.0940689851 , Val-loss =  59.5010324032 , Time for epoch =  0.014080047607421875 s\n",
      "Epoch  394 / 1000  : Train-loss =  65.0516528728 , Val-loss =  59.4506422614 , Time for epoch =  0.013493776321411133 s\n",
      "Epoch  395 / 1000  : Train-loss =  65.0091398752 , Val-loss =  59.4003283044 , Time for epoch =  0.014919519424438477 s\n",
      "Epoch  396 / 1000  : Train-loss =  64.9665338402 , Val-loss =  59.3501082466 , Time for epoch =  0.012661218643188477 s\n",
      "Epoch  397 / 1000  : Train-loss =  64.923838153 , Val-loss =  59.2999977798 , Time for epoch =  0.0145416259765625 s\n",
      "Epoch  398 / 1000  : Train-loss =  64.8810557769 , Val-loss =  59.2500108049 , Time for epoch =  0.01462244987487793 s\n",
      "Epoch  399 / 1000  : Train-loss =  64.8381892921 , Val-loss =  59.2001596382 , Time for epoch =  0.014517784118652344 s\n",
      "Epoch  400 / 1000  : Train-loss =  64.7952409319 , Val-loss =  59.1504551969 , Time for epoch =  0.011627674102783203 s\n",
      "Epoch  401 / 1000  : Train-loss =  64.7522126171 , Val-loss =  59.1009071635 , Time for epoch =  0.014209270477294922 s\n",
      "Epoch  402 / 1000  : Train-loss =  64.7091059878 , Val-loss =  59.0515241334 , Time for epoch =  0.014726877212524414 s\n",
      "Epoch  403 / 1000  : Train-loss =  64.664421328 , Val-loss =  58.9972613804 , Time for epoch =  0.014711380004882812 s\n",
      "Epoch  404 / 1000  : Train-loss =  64.6183590522 , Val-loss =  58.9431680844 , Time for epoch =  0.012368917465209961 s\n",
      "Epoch  405 / 1000  : Train-loss =  64.5722866646 , Val-loss =  58.8893563769 , Time for epoch =  0.011335372924804688 s\n",
      "Epoch  406 / 1000  : Train-loss =  64.5262053179 , Val-loss =  58.8358628755 , Time for epoch =  0.01266336441040039 s\n",
      "Epoch  407 / 1000  : Train-loss =  64.4801134071 , Val-loss =  58.7826984864 , Time for epoch =  0.01514124870300293 s\n",
      "Epoch  408 / 1000  : Train-loss =  64.4340085462 , Val-loss =  58.7298645316 , Time for epoch =  0.012415409088134766 s\n",
      "Epoch  409 / 1000  : Train-loss =  64.3879428477 , Val-loss =  58.677358652 , Time for epoch =  0.010493040084838867 s\n",
      "Epoch  410 / 1000  : Train-loss =  64.3419974637 , Val-loss =  58.6251968097 , Time for epoch =  0.01120758056640625 s\n",
      "Epoch  411 / 1000  : Train-loss =  64.2960381089 , Val-loss =  58.5732063457 , Time for epoch =  0.011658191680908203 s\n",
      "Epoch  412 / 1000  : Train-loss =  64.2500629843 , Val-loss =  58.5211896583 , Time for epoch =  0.014190912246704102 s\n",
      "Epoch  413 / 1000  : Train-loss =  64.2040697962 , Val-loss =  58.4695034713 , Time for epoch =  0.010994672775268555 s\n",
      "Epoch  414 / 1000  : Train-loss =  64.1580561442 , Val-loss =  58.4181456039 , Time for epoch =  0.011467218399047852 s\n",
      "Epoch  415 / 1000  : Train-loss =  64.1120196613 , Val-loss =  58.3671131455 , Time for epoch =  0.010815858840942383 s\n",
      "Epoch  416 / 1000  : Train-loss =  64.0659580613 , Val-loss =  58.3164031111 , Time for epoch =  0.014169454574584961 s\n",
      "Epoch  417 / 1000  : Train-loss =  64.0198691514 , Val-loss =  58.2660126758 , Time for epoch =  0.014252185821533203 s\n",
      "Epoch  418 / 1000  : Train-loss =  63.9737508344 , Val-loss =  58.2159392508 , Time for epoch =  0.014856576919555664 s\n",
      "Epoch  419 / 1000  : Train-loss =  63.9276011049 , Val-loss =  58.1661805027 , Time for epoch =  0.012635946273803711 s\n",
      "Epoch  420 / 1000  : Train-loss =  63.8814180458 , Val-loss =  58.1167343503 , Time for epoch =  0.011432409286499023 s\n",
      "Epoch  421 / 1000  : Train-loss =  63.8351998242 , Val-loss =  58.0675989554 , Time for epoch =  0.013458013534545898 s\n",
      "Epoch  422 / 1000  : Train-loss =  63.7889446871 , Val-loss =  58.0187727122 , Time for epoch =  0.012170076370239258 s\n",
      "Epoch  423 / 1000  : Train-loss =  63.7426509585 , Val-loss =  57.9702542365 , Time for epoch =  0.01429295539855957 s\n",
      "Epoch  424 / 1000  : Train-loss =  63.6963170356 , Val-loss =  57.9220423556 , Time for epoch =  0.01425313949584961 s\n",
      "Epoch  425 / 1000  : Train-loss =  63.6499413862 , Val-loss =  57.8741360992 , Time for epoch =  0.015102148056030273 s\n",
      "Epoch  426 / 1000  : Train-loss =  63.6035225459 , Val-loss =  57.8265346915 , Time for epoch =  0.010849237442016602 s\n",
      "Epoch  427 / 1000  : Train-loss =  63.5570591163 , Val-loss =  57.7792375437 , Time for epoch =  0.014137029647827148 s\n",
      "Epoch  428 / 1000  : Train-loss =  63.5105497623 , Val-loss =  57.7322442476 , Time for epoch =  0.014338970184326172 s\n",
      "Epoch  429 / 1000  : Train-loss =  63.4639932105 , Val-loss =  57.6855545691 , Time for epoch =  0.015558719635009766 s\n",
      "Epoch  430 / 1000  : Train-loss =  63.4173882476 , Val-loss =  57.6386204849 , Time for epoch =  0.011728763580322266 s\n",
      "Epoch  431 / 1000  : Train-loss =  63.3707337192 , Val-loss =  57.5907800703 , Time for epoch =  0.011053323745727539 s\n",
      "Epoch  432 / 1000  : Train-loss =  63.3240285278 , Val-loss =  57.5432637692 , Time for epoch =  0.010392189025878906 s\n",
      "Epoch  433 / 1000  : Train-loss =  63.2772716325 , Val-loss =  57.496071371 , Time for epoch =  0.010354280471801758 s\n",
      "Epoch  434 / 1000  : Train-loss =  63.2304620475 , Val-loss =  57.4492028313 , Time for epoch =  0.011766433715820312 s\n",
      "Epoch  435 / 1000  : Train-loss =  63.1835988412 , Val-loss =  57.4026582668 , Time for epoch =  0.014914751052856445 s\n",
      "Epoch  436 / 1000  : Train-loss =  63.1366811359 , Val-loss =  57.3564379522 , Time for epoch =  0.012381315231323242 s\n",
      "Epoch  437 / 1000  : Train-loss =  63.0897081068 , Val-loss =  57.3105423163 , Time for epoch =  0.011426925659179688 s\n",
      "Epoch  438 / 1000  : Train-loss =  63.0426789814 , Val-loss =  57.2649719377 , Time for epoch =  0.013188600540161133 s\n",
      "Epoch  439 / 1000  : Train-loss =  62.9955930394 , Val-loss =  57.2197275423 , Time for epoch =  0.010599613189697266 s\n",
      "Epoch  440 / 1000  : Train-loss =  62.9484496122 , Val-loss =  57.1748099991 , Time for epoch =  0.01049494743347168 s\n",
      "Epoch  441 / 1000  : Train-loss =  62.9012480823 , Val-loss =  57.1302203173 , Time for epoch =  0.010220766067504883 s\n",
      "Epoch  442 / 1000  : Train-loss =  62.8536183864 , Val-loss =  57.085075095 , Time for epoch =  0.01000356674194336 s\n",
      "Epoch  443 / 1000  : Train-loss =  62.8056956961 , Val-loss =  57.0402332655 , Time for epoch =  0.01105952262878418 s\n",
      "Epoch  444 / 1000  : Train-loss =  62.7577354674 , Val-loss =  56.9957297753 , Time for epoch =  0.010614871978759766 s\n",
      "Epoch  445 / 1000  : Train-loss =  62.7097363755 , Val-loss =  56.9515769578 , Time for epoch =  0.010416984558105469 s\n",
      "Epoch  446 / 1000  : Train-loss =  62.6616972518 , Val-loss =  56.9077793705 , Time for epoch =  0.010558128356933594 s\n",
      "Epoch  447 / 1000  : Train-loss =  62.6134686492 , Val-loss =  56.8639434696 , Time for epoch =  0.013890743255615234 s\n",
      "Epoch  448 / 1000  : Train-loss =  62.5650814141 , Val-loss =  56.8204586554 , Time for epoch =  0.014073610305786133 s\n",
      "Epoch  449 / 1000  : Train-loss =  62.5156691145 , Val-loss =  56.7730568475 , Time for epoch =  0.013897418975830078 s\n",
      "Epoch  450 / 1000  : Train-loss =  62.4646935876 , Val-loss =  56.7261117324 , Time for epoch =  0.014754295349121094 s\n",
      "Epoch  451 / 1000  : Train-loss =  62.4137802896 , Val-loss =  56.679508445 , Time for epoch =  0.012540340423583984 s\n",
      "Epoch  452 / 1000  : Train-loss =  62.3629223135 , Val-loss =  56.633196972 , Time for epoch =  0.013791084289550781 s\n",
      "Epoch  453 / 1000  : Train-loss =  62.3121136041 , Val-loss =  56.5874133957 , Time for epoch =  0.011121749877929688 s\n",
      "Epoch  454 / 1000  : Train-loss =  62.2613487043 , Val-loss =  56.5421511517 , Time for epoch =  0.013244390487670898 s\n",
      "Epoch  455 / 1000  : Train-loss =  62.210622645 , Val-loss =  56.4974031891 , Time for epoch =  0.01195216178894043 s\n",
      "Epoch  456 / 1000  : Train-loss =  62.1599308849 , Val-loss =  56.453162882 , Time for epoch =  0.014248371124267578 s\n",
      "Epoch  457 / 1000  : Train-loss =  62.1092692703 , Val-loss =  56.4094242849 , Time for epoch =  0.01455545425415039 s\n",
      "Epoch  458 / 1000  : Train-loss =  62.0586340033 , Val-loss =  56.3661821668 , Time for epoch =  0.013849020004272461 s\n",
      "Epoch  459 / 1000  : Train-loss =  62.0080216156 , Val-loss =  56.323431976 , Time for epoch =  0.015617609024047852 s\n",
      "Epoch  460 / 1000  : Train-loss =  61.9574289447 , Val-loss =  56.2811697848 , Time for epoch =  0.016034841537475586 s\n",
      "Epoch  461 / 1000  : Train-loss =  61.9068531137 , Val-loss =  56.2393922322 , Time for epoch =  0.01525735855102539 s\n",
      "Epoch  462 / 1000  : Train-loss =  61.8562915129 , Val-loss =  56.1980964688 , Time for epoch =  0.013792276382446289 s\n",
      "Epoch  463 / 1000  : Train-loss =  61.8057417825 , Val-loss =  56.1572801061 , Time for epoch =  0.01543116569519043 s\n",
      "Epoch  464 / 1000  : Train-loss =  61.7552017978 , Val-loss =  56.1169411695 , Time for epoch =  0.015523195266723633 s\n",
      "Epoch  465 / 1000  : Train-loss =  61.7046696558 , Val-loss =  56.0729290729 , Time for epoch =  0.017547607421875 s\n",
      "Epoch  466 / 1000  : Train-loss =  61.6541436621 , Val-loss =  56.0285668933 , Time for epoch =  0.015192031860351562 s\n",
      "Epoch  467 / 1000  : Train-loss =  61.6036223203 , Val-loss =  55.9847550115 , Time for epoch =  0.013037919998168945 s\n",
      "Epoch  468 / 1000  : Train-loss =  61.553104321 , Val-loss =  55.9414907358 , Time for epoch =  0.01091456413269043 s\n",
      "Epoch  469 / 1000  : Train-loss =  61.5025885332 , Val-loss =  55.8987716703 , Time for epoch =  0.013108253479003906 s\n",
      "Epoch  470 / 1000  : Train-loss =  61.4520739949 , Val-loss =  55.8565956836 , Time for epoch =  0.016652822494506836 s\n",
      "Epoch  471 / 1000  : Train-loss =  61.401559906 , Val-loss =  55.8149608793 , Time for epoch =  0.01653909683227539 s\n",
      "Epoch  472 / 1000  : Train-loss =  61.3506198911 , Val-loss =  55.7709427054 , Time for epoch =  0.014609813690185547 s\n",
      "Epoch  473 / 1000  : Train-loss =  61.297462771 , Val-loss =  55.7274503591 , Time for epoch =  0.019139766693115234 s\n",
      "Epoch  474 / 1000  : Train-loss =  61.2443611079 , Val-loss =  55.6845544091 , Time for epoch =  0.015536069869995117 s\n",
      "Epoch  475 / 1000  : Train-loss =  61.1913101652 , Val-loss =  55.642274062 , Time for epoch =  0.017342567443847656 s\n",
      "Epoch  476 / 1000  : Train-loss =  61.1383291642 , Val-loss =  55.6000435461 , Time for epoch =  0.014076471328735352 s\n",
      "Epoch  477 / 1000  : Train-loss =  61.0858052325 , Val-loss =  55.5551393371 , Time for epoch =  0.016606569290161133 s\n",
      "Epoch  478 / 1000  : Train-loss =  61.0333009083 , Val-loss =  55.5108673439 , Time for epoch =  0.01562786102294922 s\n",
      "Epoch  479 / 1000  : Train-loss =  60.9808141718 , Val-loss =  55.4672382812 , Time for epoch =  0.017274141311645508 s\n",
      "Epoch  480 / 1000  : Train-loss =  60.9283430899 , Val-loss =  55.4242527305 , Time for epoch =  0.011880874633789062 s\n",
      "Epoch  481 / 1000  : Train-loss =  60.8723102255 , Val-loss =  55.3755054118 , Time for epoch =  0.011133670806884766 s\n",
      "Epoch  482 / 1000  : Train-loss =  60.8151546081 , Val-loss =  55.3275157509 , Time for epoch =  0.012450933456420898 s\n",
      "Epoch  483 / 1000  : Train-loss =  60.7581327685 , Val-loss =  55.2803429152 , Time for epoch =  0.015410423278808594 s\n",
      "Epoch  484 / 1000  : Train-loss =  60.7012366807 , Val-loss =  55.2339956254 , Time for epoch =  0.013350963592529297 s\n",
      "Epoch  485 / 1000  : Train-loss =  60.6444597587 , Val-loss =  55.1884671065 , Time for epoch =  0.01111292839050293 s\n",
      "Epoch  486 / 1000  : Train-loss =  60.5877962488 , Val-loss =  55.1437462822 , Time for epoch =  0.011194229125976562 s\n",
      "Epoch  487 / 1000  : Train-loss =  60.5312410196 , Val-loss =  55.0998212976 , Time for epoch =  0.01141810417175293 s\n",
      "Epoch  488 / 1000  : Train-loss =  60.4747894709 , Val-loss =  55.0566805782 , Time for epoch =  0.01610708236694336 s\n",
      "Epoch  489 / 1000  : Train-loss =  60.4184374813 , Val-loss =  55.0143131048 , Time for epoch =  0.014919519424438477 s\n",
      "Epoch  490 / 1000  : Train-loss =  60.3621813705 , Val-loss =  54.9727084432 , Time for epoch =  0.012295961380004883 s\n",
      "Epoch  491 / 1000  : Train-loss =  60.3060178682 , Val-loss =  54.9318567012 , Time for epoch =  0.011738777160644531 s\n",
      "Epoch  492 / 1000  : Train-loss =  60.2498226771 , Val-loss =  54.8863131981 , Time for epoch =  0.011236190795898438 s\n",
      "Epoch  493 / 1000  : Train-loss =  60.191568953 , Val-loss =  54.841441336 , Time for epoch =  0.010973930358886719 s\n",
      "Epoch  494 / 1000  : Train-loss =  60.1337562735 , Val-loss =  54.7979404262 , Time for epoch =  0.01502847671508789 s\n",
      "Epoch  495 / 1000  : Train-loss =  60.0763536423 , Val-loss =  54.7552462701 , Time for epoch =  0.012577295303344727 s\n",
      "Epoch  496 / 1000  : Train-loss =  60.0190831444 , Val-loss =  54.7133668523 , Time for epoch =  0.011442422866821289 s\n",
      "Epoch  497 / 1000  : Train-loss =  59.9619400622 , Val-loss =  54.6722948287 , Time for epoch =  0.012539863586425781 s\n",
      "Epoch  498 / 1000  : Train-loss =  59.9049203369 , Val-loss =  54.6320185186 , Time for epoch =  0.013781547546386719 s\n",
      "Epoch  499 / 1000  : Train-loss =  59.8480204215 , Val-loss =  54.5925252882 , Time for epoch =  0.014621973037719727 s\n",
      "Epoch  500 / 1000  : Train-loss =  59.7912372173 , Val-loss =  54.5538025525 , Time for epoch =  0.01139521598815918 s\n",
      "Epoch  501 / 1000  : Train-loss =  59.7345680344 , Val-loss =  54.5158380385 , Time for epoch =  0.014506816864013672 s\n",
      "Epoch  502 / 1000  : Train-loss =  59.6800298875 , Val-loss =  54.4890999935 , Time for epoch =  0.014921426773071289 s\n",
      "Epoch  503 / 1000  : Train-loss =  59.6275265465 , Val-loss =  54.4632119372 , Time for epoch =  0.014309406280517578 s\n",
      "Epoch  504 / 1000  : Train-loss =  59.5752002243 , Val-loss =  54.4379350486 , Time for epoch =  0.014225482940673828 s\n",
      "Epoch  505 / 1000  : Train-loss =  59.5160426189 , Val-loss =  54.4042234327 , Time for epoch =  0.011535406112670898 s\n",
      "Epoch  506 / 1000  : Train-loss =  59.4494892904 , Val-loss =  54.3690223101 , Time for epoch =  0.010975837707519531 s\n",
      "Epoch  507 / 1000  : Train-loss =  59.3830594764 , Val-loss =  54.3347054963 , Time for epoch =  0.010499000549316406 s\n",
      "Epoch  508 / 1000  : Train-loss =  59.3164817498 , Val-loss =  54.3007391418 , Time for epoch =  0.011218070983886719 s\n",
      "Epoch  509 / 1000  : Train-loss =  59.2502933412 , Val-loss =  54.2677044583 , Time for epoch =  0.010712862014770508 s\n",
      "Epoch  510 / 1000  : Train-loss =  59.1846490254 , Val-loss =  54.2353529723 , Time for epoch =  0.011818885803222656 s\n",
      "Epoch  511 / 1000  : Train-loss =  59.1195324744 , Val-loss =  54.2037337972 , Time for epoch =  0.010874032974243164 s\n",
      "Epoch  512 / 1000  : Train-loss =  59.1202394999 , Val-loss =  54.1605877644 , Time for epoch =  0.010993719100952148 s\n",
      "Epoch  513 / 1000  : Train-loss =  59.0459605862 , Val-loss =  54.1330275151 , Time for epoch =  0.01114034652709961 s\n",
      "Epoch  514 / 1000  : Train-loss =  59.043831024 , Val-loss =  54.1042912773 , Time for epoch =  0.012866020202636719 s\n",
      "Epoch  515 / 1000  : Train-loss =  58.9686399192 , Val-loss =  54.0670938545 , Time for epoch =  0.014001131057739258 s\n",
      "Epoch  516 / 1000  : Train-loss =  58.8643848618 , Val-loss =  54.0343523226 , Time for epoch =  0.016810894012451172 s\n",
      "Epoch  517 / 1000  : Train-loss =  58.765730059 , Val-loss =  54.0039101763 , Time for epoch =  0.012865781784057617 s\n",
      "Epoch  518 / 1000  : Train-loss =  58.670073583 , Val-loss =  53.9753009958 , Time for epoch =  0.014219284057617188 s\n",
      "Epoch  519 / 1000  : Train-loss =  58.5772565062 , Val-loss =  53.9483025495 , Time for epoch =  0.014474868774414062 s\n",
      "Epoch  520 / 1000  : Train-loss =  58.4837304682 , Val-loss =  53.9145601386 , Time for epoch =  0.011987447738647461 s\n",
      "Epoch  521 / 1000  : Train-loss =  58.374086134 , Val-loss =  53.8824920468 , Time for epoch =  0.010917186737060547 s\n",
      "Epoch  522 / 1000  : Train-loss =  58.2650684178 , Val-loss =  53.8575402261 , Time for epoch =  0.012430906295776367 s\n",
      "Epoch  523 / 1000  : Train-loss =  58.1494922705 , Val-loss =  53.836960135 , Time for epoch =  0.011413097381591797 s\n",
      "Epoch  524 / 1000  : Train-loss =  58.0308609066 , Val-loss =  53.8196580391 , Time for epoch =  0.010747671127319336 s\n",
      "Epoch  525 / 1000  : Train-loss =  57.9176752753 , Val-loss =  53.8075017717 , Time for epoch =  0.011275291442871094 s\n",
      "Epoch  526 / 1000  : Train-loss =  57.8171293749 , Val-loss =  53.812329168 , Time for epoch =  0.018055438995361328 s\n",
      "Epoch  527 / 1000  : Train-loss =  57.7220601545 , Val-loss =  53.8158844738 , Time for epoch =  0.017620086669921875 s\n",
      "Epoch  528 / 1000  : Train-loss =  57.6308093503 , Val-loss =  53.8192599407 , Time for epoch =  0.011197090148925781 s\n",
      "Epoch  529 / 1000  : Train-loss =  57.5605857692 , Val-loss =  53.8230685941 , Time for epoch =  0.011267423629760742 s\n",
      "Epoch  530 / 1000  : Train-loss =  57.5035298789 , Val-loss =  53.8297249857 , Time for epoch =  0.014741897583007812 s\n",
      "Epoch  531 / 1000  : Train-loss =  57.4456263493 , Val-loss =  53.8457511366 , Time for epoch =  0.01457071304321289 s\n",
      "Epoch  532 / 1000  : Train-loss =  57.3960072126 , Val-loss =  53.8544592197 , Time for epoch =  0.014419317245483398 s\n",
      "Epoch  533 / 1000  : Train-loss =  57.3460326284 , Val-loss =  53.8630086895 , Time for epoch =  0.011512994766235352 s\n",
      "Epoch  534 / 1000  : Train-loss =  57.2918214873 , Val-loss =  53.8626248412 , Time for epoch =  0.011377573013305664 s\n",
      "Epoch  535 / 1000  : Train-loss =  57.2413749732 , Val-loss =  53.8628305813 , Time for epoch =  0.010505914688110352 s\n",
      "Epoch  536 / 1000  : Train-loss =  57.1907984286 , Val-loss =  53.8631729173 , Time for epoch =  0.014097213745117188 s\n",
      "Epoch  537 / 1000  : Train-loss =  57.1404139475 , Val-loss =  53.860731045 , Time for epoch =  0.014190435409545898 s\n",
      "Epoch  538 / 1000  : Train-loss =  57.0889941212 , Val-loss =  53.861414567 , Time for epoch =  0.013766050338745117 s\n",
      "Epoch  539 / 1000  : Train-loss =  57.0382170996 , Val-loss =  53.8619852495 , Time for epoch =  0.014301538467407227 s\n",
      "Epoch  540 / 1000  : Train-loss =  56.9874101508 , Val-loss =  53.8625266421 , Time for epoch =  0.01157069206237793 s\n",
      "Epoch  541 / 1000  : Train-loss =  56.9365833585 , Val-loss =  53.8630666039 , Time for epoch =  0.011517047882080078 s\n",
      "Epoch  542 / 1000  : Train-loss =  56.8857496967 , Val-loss =  53.8636163069 , Time for epoch =  0.01129913330078125 s\n",
      "Epoch  543 / 1000  : Train-loss =  56.8353300641 , Val-loss =  53.8614516234 , Time for epoch =  0.011444568634033203 s\n",
      "Epoch  544 / 1000  : Train-loss =  56.7837652349 , Val-loss =  53.8623779851 , Time for epoch =  0.011702299118041992 s\n",
      "Epoch  545 / 1000  : Train-loss =  56.7329942139 , Val-loss =  53.8631918481 , Time for epoch =  0.011983394622802734 s\n",
      "Epoch  546 / 1000  : Train-loss =  56.682273842 , Val-loss =  53.8639761365 , Time for epoch =  0.012789011001586914 s\n",
      "Epoch  547 / 1000  : Train-loss =  56.6316127461 , Val-loss =  53.864749946 , Time for epoch =  0.014200210571289062 s\n",
      "Epoch  548 / 1000  : Train-loss =  56.5810213253 , Val-loss =  53.8655142153 , Time for epoch =  0.014060020446777344 s\n",
      "Epoch  549 / 1000  : Train-loss =  56.5305100073 , Val-loss =  53.8650756318 , Time for epoch =  0.01631307601928711 s\n",
      "Epoch  550 / 1000  : Train-loss =  56.4800887737 , Val-loss =  53.861724559 , Time for epoch =  0.015805482864379883 s\n",
      "Epoch  551 / 1000  : Train-loss =  56.429767045 , Val-loss =  53.8583456768 , Time for epoch =  0.01577281951904297 s\n",
      "Epoch  552 / 1000  : Train-loss =  56.3795536634 , Val-loss =  53.8549292427 , Time for epoch =  0.014463186264038086 s\n",
      "Epoch  553 / 1000  : Train-loss =  56.3294569023 , Val-loss =  53.85146469 , Time for epoch =  0.014863729476928711 s\n",
      "Epoch  554 / 1000  : Train-loss =  56.2794844847 , Val-loss =  53.8479407771 , Time for epoch =  0.015115499496459961 s\n",
      "Epoch  555 / 1000  : Train-loss =  56.2296436049 , Val-loss =  53.8443457084 , Time for epoch =  0.018261432647705078 s\n",
      "Epoch  556 / 1000  : Train-loss =  56.179940951 , Val-loss =  53.8406672421 , Time for epoch =  0.016227006912231445 s\n",
      "Epoch  557 / 1000  : Train-loss =  56.131952583 , Val-loss =  53.8387234829 , Time for epoch =  0.014926671981811523 s\n",
      "Epoch  558 / 1000  : Train-loss =  56.0811540979 , Val-loss =  53.8343097717 , Time for epoch =  0.01125478744506836 s\n",
      "Epoch  559 / 1000  : Train-loss =  56.0318971543 , Val-loss =  53.8300689471 , Time for epoch =  0.01188349723815918 s\n",
      "Epoch  560 / 1000  : Train-loss =  55.9827983175 , Val-loss =  53.8257786208 , Time for epoch =  0.01819157600402832 s\n",
      "Epoch  561 / 1000  : Train-loss =  55.9338634981 , Val-loss =  53.8213751984 , Time for epoch =  0.014419078826904297 s\n",
      "Epoch  562 / 1000  : Train-loss =  55.8867001318 , Val-loss =  53.8186215662 , Time for epoch =  0.01782679557800293 s\n",
      "Epoch  563 / 1000  : Train-loss =  55.836668045 , Val-loss =  53.8133193398 , Time for epoch =  0.014293670654296875 s\n",
      "Epoch  564 / 1000  : Train-loss =  55.7882428712 , Val-loss =  53.8081666564 , Time for epoch =  0.018545866012573242 s\n",
      "Epoch  565 / 1000  : Train-loss =  55.7399961397 , Val-loss =  53.8029380407 , Time for epoch =  0.01685166358947754 s\n",
      "Epoch  566 / 1000  : Train-loss =  55.6935524171 , Val-loss =  53.7992729721 , Time for epoch =  0.012726545333862305 s\n",
      "Epoch  567 / 1000  : Train-loss =  55.6442024766 , Val-loss =  53.7930237431 , Time for epoch =  0.01449441909790039 s\n",
      "Epoch  568 / 1000  : Train-loss =  55.5964984143 , Val-loss =  53.7869245437 , Time for epoch =  0.016104698181152344 s\n",
      "Epoch  569 / 1000  : Train-loss =  55.5506303616 , Val-loss =  53.7824259823 , Time for epoch =  0.014699220657348633 s\n",
      "Epoch  570 / 1000  : Train-loss =  55.5018033661 , Val-loss =  53.7753152111 , Time for epoch =  0.011597156524658203 s\n",
      "Epoch  571 / 1000  : Train-loss =  55.4546593033 , Val-loss =  53.7683674323 , Time for epoch =  0.012331962585449219 s\n",
      "Epoch  572 / 1000  : Train-loss =  55.4093702202 , Val-loss =  53.7629536987 , Time for epoch =  0.011543512344360352 s\n",
      "Epoch  573 / 1000  : Train-loss =  55.3610904627 , Val-loss =  53.7549121954 , Time for epoch =  0.015179872512817383 s\n",
      "Epoch  574 / 1000  : Train-loss =  55.3145178016 , Val-loss =  53.7470606 , Time for epoch =  0.014664411544799805 s\n",
      "Epoch  575 / 1000  : Train-loss =  55.2698067463 , Val-loss =  53.7406410926 , Time for epoch =  0.01590895652770996 s\n",
      "Epoch  576 / 1000  : Train-loss =  55.2220918471 , Val-loss =  53.7316167706 , Time for epoch =  0.012451410293579102 s\n",
      "Epoch  577 / 1000  : Train-loss =  55.1777932888 , Val-loss =  53.7243875592 , Time for epoch =  0.012958288192749023 s\n",
      "Epoch  578 / 1000  : Train-loss =  55.1304227805 , Val-loss =  53.7145709729 , Time for epoch =  0.012422800064086914 s\n",
      "Epoch  579 / 1000  : Train-loss =  55.0848103134 , Val-loss =  53.7049964811 , Time for epoch =  0.013554811477661133 s\n",
      "Epoch  580 / 1000  : Train-loss =  55.0410824215 , Val-loss =  53.6967563161 , Time for epoch =  0.013041496276855469 s\n",
      "Epoch  581 / 1000  : Train-loss =  54.9942966727 , Val-loss =  53.6859428814 , Time for epoch =  0.01451873779296875 s\n",
      "Epoch  582 / 1000  : Train-loss =  54.9509702442 , Val-loss =  53.676823942 , Time for epoch =  0.017431259155273438 s\n",
      "Epoch  583 / 1000  : Train-loss =  54.904535 , Val-loss =  53.6651747853 , Time for epoch =  0.012389898300170898 s\n",
      "Epoch  584 / 1000  : Train-loss =  54.8616089184 , Val-loss =  53.6552672275 , Time for epoch =  0.013976097106933594 s\n",
      "Epoch  585 / 1000  : Train-loss =  54.8155268053 , Val-loss =  53.6428113158 , Time for epoch =  0.010976076126098633 s\n",
      "Epoch  586 / 1000  : Train-loss =  54.7729952261 , Val-loss =  53.6321130214 , Time for epoch =  0.011685848236083984 s\n",
      "Epoch  587 / 1000  : Train-loss =  54.7272666303 , Val-loss =  53.6188565691 , Time for epoch =  0.012217521667480469 s\n",
      "Epoch  588 / 1000  : Train-loss =  54.6851231063 , Val-loss =  53.6073625545 , Time for epoch =  0.011400699615478516 s\n",
      "Epoch  589 / 1000  : Train-loss =  54.6397473995 , Val-loss =  53.5933120752 , Time for epoch =  0.014050960540771484 s\n",
      "Epoch  590 / 1000  : Train-loss =  54.597985245 , Val-loss =  53.5810203202 , Time for epoch =  0.014033317565917969 s\n",
      "Epoch  591 / 1000  : Train-loss =  54.5529609564 , Val-loss =  53.566184177 , Time for epoch =  0.014045476913452148 s\n",
      "Epoch  592 / 1000  : Train-loss =  54.5115733381 , Val-loss =  53.5530959407 , Time for epoch =  0.011340856552124023 s\n",
      "Epoch  593 / 1000  : Train-loss =  54.4668982486 , Val-loss =  53.5374844438 , Time for epoch =  0.010522603988647461 s\n",
      "Epoch  594 / 1000  : Train-loss =  54.4258782533 , Val-loss =  53.5236041787 , Time for epoch =  0.01387476921081543 s\n",
      "Epoch  595 / 1000  : Train-loss =  54.3815494826 , Val-loss =  53.5072295449 , Time for epoch =  0.013953447341918945 s\n",
      "Epoch  596 / 1000  : Train-loss =  54.3408901796 , Val-loss =  53.4925647484 , Time for epoch =  0.013830900192260742 s\n",
      "Epoch  597 / 1000  : Train-loss =  54.2969042702 , Val-loss =  53.4754410145 , Time for epoch =  0.013913869857788086 s\n",
      "Epoch  598 / 1000  : Train-loss =  54.2565987689 , Val-loss =  53.4600020471 , Time for epoch =  0.011064767837524414 s\n",
      "Epoch  599 / 1000  : Train-loss =  54.2129517656 , Val-loss =  53.442144952 , Time for epoch =  0.011079549789428711 s\n",
      "Epoch  600 / 1000  : Train-loss =  54.1729932678 , Val-loss =  53.4259448319 , Time for epoch =  0.01413416862487793 s\n",
      "Epoch  601 / 1000  : Train-loss =  54.1296807933 , Val-loss =  53.4073716747 , Time for epoch =  0.011281728744506836 s\n",
      "Epoch  602 / 1000  : Train-loss =  54.0900626401 , Val-loss =  53.390425855 , Time for epoch =  0.011236190795898438 s\n",
      "Epoch  603 / 1000  : Train-loss =  54.0470799657 , Val-loss =  53.3711553356 , Time for epoch =  0.012680292129516602 s\n",
      "Epoch  604 / 1000  : Train-loss =  54.0077956785 , Val-loss =  53.353481471 , Time for epoch =  0.014938831329345703 s\n",
      "Epoch  605 / 1000  : Train-loss =  53.9651377902 , Val-loss =  53.3335335196 , Time for epoch =  0.014311075210571289 s\n",
      "Epoch  606 / 1000  : Train-loss =  53.926181106 , Val-loss =  53.3151512275 , Time for epoch =  0.011373758316040039 s\n",
      "Epoch  607 / 1000  : Train-loss =  53.8838427648 , Val-loss =  53.2945468283 , Time for epoch =  0.014163017272949219 s\n",
      "Epoch  608 / 1000  : Train-loss =  53.8452076662 , Val-loss =  53.2754774487 , Time for epoch =  0.010966777801513672 s\n",
      "Epoch  609 / 1000  : Train-loss =  53.8031834634 , Val-loss =  53.254238461 , Time for epoch =  0.010545015335083008 s\n",
      "Epoch  610 / 1000  : Train-loss =  53.7648642025 , Val-loss =  53.2345048204 , Time for epoch =  0.010878324508666992 s\n",
      "Epoch  611 / 1000  : Train-loss =  53.7231486095 , Val-loss =  53.2126538027 , Time for epoch =  0.011080503463745117 s\n",
      "Epoch  612 / 1000  : Train-loss =  53.6851397266 , Val-loss =  53.1922802919 , Time for epoch =  0.010926485061645508 s\n",
      "Epoch  613 / 1000  : Train-loss =  53.6455801427 , Val-loss =  53.1708560001 , Time for epoch =  0.011081695556640625 s\n",
      "Epoch  614 / 1000  : Train-loss =  53.6042282666 , Val-loss =  53.1477624876 , Time for epoch =  0.010421037673950195 s\n",
      "Epoch  615 / 1000  : Train-loss =  53.5667238187 , Val-loss =  53.1264601651 , Time for epoch =  0.011297464370727539 s\n",
      "Epoch  616 / 1000  : Train-loss =  53.5256886385 , Val-loss =  53.1030083385 , Time for epoch =  0.015226602554321289 s\n",
      "Epoch  617 / 1000  : Train-loss =  53.4884847545 , Val-loss =  53.0811841416 , Time for epoch =  0.014254093170166016 s\n",
      "Epoch  618 / 1000  : Train-loss =  53.4477432509 , Val-loss =  53.0569000158 , Time for epoch =  0.011873483657836914 s\n",
      "Epoch  619 / 1000  : Train-loss =  53.4108314411 , Val-loss =  53.0340546952 , Time for epoch =  0.011229276657104492 s\n",
      "Epoch  620 / 1000  : Train-loss =  53.3703767839 , Val-loss =  53.0091644506 , Time for epoch =  0.011177539825439453 s\n",
      "Epoch  621 / 1000  : Train-loss =  53.3337527415 , Val-loss =  52.9858286565 , Time for epoch =  0.01316070556640625 s\n",
      "Epoch  622 / 1000  : Train-loss =  53.2935791357 , Val-loss =  52.9604943714 , Time for epoch =  0.010702371597290039 s\n",
      "Epoch  623 / 1000  : Train-loss =  53.2572391079 , Val-loss =  52.9366927644 , Time for epoch =  0.012194395065307617 s\n",
      "Epoch  624 / 1000  : Train-loss =  53.2173409246 , Val-loss =  52.9109349735 , Time for epoch =  0.014137029647827148 s\n",
      "Epoch  625 / 1000  : Train-loss =  53.1812814901 , Val-loss =  52.8866928418 , Time for epoch =  0.014511823654174805 s\n",
      "Epoch  626 / 1000  : Train-loss =  53.1416532281 , Val-loss =  52.8605321891 , Time for epoch =  0.011578083038330078 s\n",
      "Epoch  627 / 1000  : Train-loss =  53.105871272 , Val-loss =  52.8358751039 , Time for epoch =  0.013814926147460938 s\n",
      "Epoch  628 / 1000  : Train-loss =  53.0665075651 , Val-loss =  52.8093321388 , Time for epoch =  0.011167764663696289 s\n",
      "Epoch  629 / 1000  : Train-loss =  53.0310002635 , Val-loss =  52.7842857514 , Time for epoch =  0.014098882675170898 s\n",
      "Epoch  630 / 1000  : Train-loss =  52.9918958888 , Val-loss =  52.7573807802 , Time for epoch =  0.014043569564819336 s\n",
      "Epoch  631 / 1000  : Train-loss =  52.956660693 , Val-loss =  52.7319706561 , Time for epoch =  0.014176607131958008 s\n",
      "Epoch  632 / 1000  : Train-loss =  52.9178105772 , Val-loss =  52.7047236229 , Time for epoch =  0.015618562698364258 s\n",
      "Epoch  633 / 1000  : Train-loss =  52.8828451972 , Val-loss =  52.6789751026 , Time for epoch =  0.011075258255004883 s\n",
      "Epoch  634 / 1000  : Train-loss =  52.8442444205 , Val-loss =  52.6514054911 , Time for epoch =  0.010836124420166016 s\n",
      "Epoch  635 / 1000  : Train-loss =  52.8095468079 , Val-loss =  52.6253435741 , Time for epoch =  0.011079072952270508 s\n",
      "Epoch  636 / 1000  : Train-loss =  52.7711906074 , Val-loss =  52.5974703311 , Time for epoch =  0.011185646057128906 s\n",
      "Epoch  637 / 1000  : Train-loss =  52.7367589379 , Val-loss =  52.5711195803 , Time for epoch =  0.013150453567504883 s\n",
      "Epoch  638 / 1000  : Train-loss =  52.6986427087 , Val-loss =  52.5429610546 , Time for epoch =  0.014451742172241211 s\n",
      "Epoch  639 / 1000  : Train-loss =  52.664475364 , Val-loss =  52.5163455178 , Time for epoch =  0.014320135116577148 s\n",
      "Epoch  640 / 1000  : Train-loss =  52.6265946596 , Val-loss =  52.4879194151 , Time for epoch =  0.014138221740722656 s\n",
      "Epoch  641 / 1000  : Train-loss =  52.5907968943 , Val-loss =  52.4605449515 , Time for epoch =  0.011359453201293945 s\n",
      "Epoch  642 / 1000  : Train-loss =  52.5569986144 , Val-loss =  52.4337706835 , Time for epoch =  0.01068878173828125 s\n",
      "Epoch  643 / 1000  : Train-loss =  52.5195392382 , Val-loss =  52.4051936274 , Time for epoch =  0.010584354400634766 s\n",
      "Epoch  644 / 1000  : Train-loss =  52.4859754858 , Val-loss =  52.3779800942 , Time for epoch =  0.010680437088012695 s\n",
      "Epoch  645 / 1000  : Train-loss =  52.4487298225 , Val-loss =  52.3491025004 , Time for epoch =  0.010543107986450195 s\n",
      "Epoch  646 / 1000  : Train-loss =  52.4154260364 , Val-loss =  52.3216799056 , Time for epoch =  0.010563850402832031 s\n",
      "Epoch  647 / 1000  : Train-loss =  52.3783968582 , Val-loss =  52.29256885 , Time for epoch =  0.010915756225585938 s\n",
      "Epoch  648 / 1000  : Train-loss =  52.3434630072 , Val-loss =  52.264560565 , Time for epoch =  0.014204978942871094 s\n",
      "Epoch  649 / 1000  : Train-loss =  52.3105277747 , Val-loss =  52.2370869072 , Time for epoch =  0.015375852584838867 s\n",
      "Epoch  650 / 1000  : Train-loss =  52.2738937755 , Val-loss =  52.2078814098 , Time for epoch =  0.014458179473876953 s\n",
      "Epoch  651 / 1000  : Train-loss =  52.2411894313 , Val-loss =  52.180029786 , Time for epoch =  0.011556386947631836 s\n",
      "Epoch  652 / 1000  : Train-loss =  52.2047520884 , Val-loss =  52.1505607287 , Time for epoch =  0.01114797592163086 s\n",
      "Epoch  653 / 1000  : Train-loss =  52.1704182057 , Val-loss =  52.122224621 , Time for epoch =  0.01120448112487793 s\n",
      "Epoch  654 / 1000  : Train-loss =  52.1380824357 , Val-loss =  52.0943791531 , Time for epoch =  0.013769388198852539 s\n",
      "Epoch  655 / 1000  : Train-loss =  52.1020214251 , Val-loss =  52.0648423293 , Time for epoch =  0.011062383651733398 s\n",
      "Epoch  656 / 1000  : Train-loss =  52.0699151547 , Val-loss =  52.0366578403 , Time for epoch =  0.010952949523925781 s\n",
      "Epoch  657 / 1000  : Train-loss =  52.0340389632 , Val-loss =  52.0068866915 , Time for epoch =  0.011722803115844727 s\n",
      "Epoch  658 / 1000  : Train-loss =  52.000289986 , Val-loss =  51.9782890939 , Time for epoch =  0.013979911804199219 s\n",
      "Epoch  659 / 1000  : Train-loss =  51.9685548722 , Val-loss =  51.9500147156 , Time for epoch =  0.015134096145629883 s\n",
      "Epoch  660 / 1000  : Train-loss =  51.9330387346 , Val-loss =  51.9187726296 , Time for epoch =  0.01455068588256836 s\n",
      "Epoch  661 / 1000  : Train-loss =  51.899640261 , Val-loss =  51.8887216779 , Time for epoch =  0.014981746673583984 s\n",
      "Epoch  662 / 1000  : Train-loss =  51.8682455032 , Val-loss =  51.8564992529 , Time for epoch =  0.01088404655456543 s\n",
      "Epoch  663 / 1000  : Train-loss =  51.8330695361 , Val-loss =  51.819666825 , Time for epoch =  0.014267206192016602 s\n",
      "Epoch  664 / 1000  : Train-loss =  51.8000142864 , Val-loss =  51.7841101221 , Time for epoch =  0.011658191680908203 s\n",
      "Epoch  665 / 1000  : Train-loss =  51.7689610926 , Val-loss =  51.7487820729 , Time for epoch =  0.019867897033691406 s\n",
      "Epoch  666 / 1000  : Train-loss =  51.7341139053 , Val-loss =  51.7119012369 , Time for epoch =  0.016764163970947266 s\n",
      "Epoch  667 / 1000  : Train-loss =  51.7013969712 , Val-loss =  51.6763190456 , Time for epoch =  0.01665973663330078 s\n",
      "Epoch  668 / 1000  : Train-loss =  51.6706871454 , Val-loss =  51.6409547881 , Time for epoch =  0.01158285140991211 s\n",
      "Epoch  669 / 1000  : Train-loss =  51.636158428 , Val-loss =  51.6040486326 , Time for epoch =  0.0115966796875 s\n",
      "Epoch  670 / 1000  : Train-loss =  51.6037754545 , Val-loss =  51.5684451631 , Time for epoch =  0.013259410858154297 s\n",
      "Epoch  671 / 1000  : Train-loss =  51.5734108677 , Val-loss =  51.5330772797 , Time for epoch =  0.01052403450012207 s\n",
      "Epoch  672 / 1000  : Train-loss =  51.5391911582 , Val-loss =  51.4961634575 , Time for epoch =  0.011478900909423828 s\n",
      "Epoch  673 / 1000  : Train-loss =  51.5071381938 , Val-loss =  51.4605492913 , Time for epoch =  0.010685920715332031 s\n",
      "Epoch  674 / 1000  : Train-loss =  51.4752399529 , Val-loss =  51.4253022672 , Time for epoch =  0.010657310485839844 s\n",
      "Epoch  675 / 1000  : Train-loss =  51.4453117272 , Val-loss =  51.389994767 , Time for epoch =  0.010644674301147461 s\n",
      "Epoch  676 / 1000  : Train-loss =  51.4115349976 , Val-loss =  51.3531506246 , Time for epoch =  0.010596036911010742 s\n",
      "Epoch  677 / 1000  : Train-loss =  51.3799215492 , Val-loss =  51.317615968 , Time for epoch =  0.011586427688598633 s\n",
      "Epoch  678 / 1000  : Train-loss =  51.3484611021 , Val-loss =  51.2824413325 , Time for epoch =  0.014390945434570312 s\n",
      "Epoch  679 / 1000  : Train-loss =  51.3189652228 , Val-loss =  51.2471709604 , Time for epoch =  0.014359712600708008 s\n",
      "Epoch  680 / 1000  : Train-loss =  51.2856084705 , Val-loss =  51.2103908742 , Time for epoch =  0.014230489730834961 s\n",
      "Epoch  681 / 1000  : Train-loss =  51.2544256457 , Val-loss =  51.1749303715 , Time for epoch =  0.014801263809204102 s\n",
      "Epoch  682 / 1000  : Train-loss =  51.2233949486 , Val-loss =  51.1398418762 , Time for epoch =  0.013632059097290039 s\n",
      "Epoch  683 / 1000  : Train-loss =  51.1943361091 , Val-loss =  51.1046526974 , Time for epoch =  0.01096653938293457 s\n",
      "Epoch  684 / 1000  : Train-loss =  51.161381061 , Val-loss =  51.067951517 , Time for epoch =  0.010869979858398438 s\n",
      "Epoch  685 / 1000  : Train-loss =  51.1306217717 , Val-loss =  51.0325962309 , Time for epoch =  0.011145353317260742 s\n",
      "Epoch  686 / 1000  : Train-loss =  51.1000141012 , Val-loss =  50.9975859542 , Time for epoch =  0.018609046936035156 s\n",
      "Epoch  687 / 1000  : Train-loss =  51.0695248707 , Val-loss =  50.9627250603 , Time for epoch =  0.011714458465576172 s\n",
      "Epoch  688 / 1000  : Train-loss =  51.0409945249 , Val-loss =  50.9276759932 , Time for epoch =  0.013085126876831055 s\n",
      "Epoch  689 / 1000  : Train-loss =  51.0085494632 , Val-loss =  50.8911263066 , Time for epoch =  0.01153874397277832 s\n",
      "Epoch  690 / 1000  : Train-loss =  50.9783137988 , Val-loss =  50.8559434317 , Time for epoch =  0.014472484588623047 s\n",
      "Epoch  691 / 1000  : Train-loss =  50.9482285386 , Val-loss =  50.8211116902 , Time for epoch =  0.01378011703491211 s\n",
      "Epoch  692 / 1000  : Train-loss =  50.9182596321 , Val-loss =  50.7864008493 , Time for epoch =  0.014022111892700195 s\n",
      "Epoch  693 / 1000  : Train-loss =  50.8883986301 , Val-loss =  50.751781052 , Time for epoch =  0.013979196548461914 s\n",
      "Epoch  694 / 1000  : Train-loss =  50.8586428949 , Val-loss =  50.7172133379 , Time for epoch =  0.012031316757202148 s\n",
      "Epoch  695 / 1000  : Train-loss =  50.8308213038 , Val-loss =  50.6823722296 , Time for epoch =  0.014513015747070312 s\n",
      "Epoch  696 / 1000  : Train-loss =  50.7990879039 , Val-loss =  50.6460836711 , Time for epoch =  0.01479339599609375 s\n",
      "Epoch  697 / 1000  : Train-loss =  50.7695705991 , Val-loss =  50.6111846692 , Time for epoch =  0.013990640640258789 s\n",
      "Epoch  698 / 1000  : Train-loss =  50.740201779 , Val-loss =  50.5766423761 , Time for epoch =  0.011421442031860352 s\n",
      "Epoch  699 / 1000  : Train-loss =  50.6316552171 , Val-loss =  50.4347919576 , Time for epoch =  0.011768579483032227 s\n",
      "Epoch  700 / 1000  : Train-loss =  50.6030519027 , Val-loss =  50.4011747808 , Time for epoch =  0.010973215103149414 s\n",
      "Epoch  701 / 1000  : Train-loss =  50.5781925757 , Val-loss =  50.3899425047 , Time for epoch =  0.011392593383789062 s\n",
      "Epoch  702 / 1000  : Train-loss =  50.5553140018 , Val-loss =  50.3826051901 , Time for epoch =  0.016985177993774414 s\n",
      "Epoch  703 / 1000  : Train-loss =  50.5295051313 , Val-loss =  50.3730520962 , Time for epoch =  0.017587661743164062 s\n",
      "Epoch  704 / 1000  : Train-loss =  50.5041475299 , Val-loss =  50.3588904823 , Time for epoch =  0.014876127243041992 s\n",
      "Epoch  705 / 1000  : Train-loss =  50.4800709882 , Val-loss =  50.3437705351 , Time for epoch =  0.01389932632446289 s\n",
      "Epoch  706 / 1000  : Train-loss =  50.4529302089 , Val-loss =  50.3260969705 , Time for epoch =  0.013432502746582031 s\n",
      "Epoch  707 / 1000  : Train-loss =  50.426406248 , Val-loss =  50.3045701301 , Time for epoch =  0.014689922332763672 s\n",
      "Epoch  708 / 1000  : Train-loss =  50.4013320671 , Val-loss =  50.2831137066 , Time for epoch =  0.01839756965637207 s\n",
      "Epoch  709 / 1000  : Train-loss =  50.3732907466 , Val-loss =  50.2598177153 , Time for epoch =  0.013262510299682617 s\n",
      "Epoch  710 / 1000  : Train-loss =  50.3460001634 , Val-loss =  50.2333902675 , Time for epoch =  0.01984715461730957 s\n",
      "Epoch  711 / 1000  : Train-loss =  50.3202903923 , Val-loss =  50.2077948727 , Time for epoch =  0.01591658592224121 s\n",
      "Epoch  712 / 1000  : Train-loss =  50.291693142 , Val-loss =  50.180828236 , Time for epoch =  0.017620325088500977 s\n",
      "Epoch  713 / 1000  : Train-loss =  50.2639363828 , Val-loss =  50.1512111048 , Time for epoch =  0.013660669326782227 s\n",
      "Epoch  714 / 1000  : Train-loss =  50.2378690097 , Val-loss =  50.1229538045 , Time for epoch =  0.01078033447265625 s\n",
      "Epoch  715 / 1000  : Train-loss =  50.2089459614 , Val-loss =  50.0936182074 , Time for epoch =  0.010833024978637695 s\n",
      "Epoch  716 / 1000  : Train-loss =  50.1809358193 , Val-loss =  50.0619478655 , Time for epoch =  0.013940095901489258 s\n",
      "Epoch  717 / 1000  : Train-loss =  50.1547206834 , Val-loss =  50.0320048235 , Time for epoch =  0.014569759368896484 s\n",
      "Epoch  718 / 1000  : Train-loss =  50.1256179648 , Val-loss =  50.0011614531 , Time for epoch =  0.01523137092590332 s\n",
      "Epoch  719 / 1000  : Train-loss =  50.0975029131 , Val-loss =  49.968188963 , Time for epoch =  0.011733531951904297 s\n",
      "Epoch  720 / 1000  : Train-loss =  50.0712995154 , Val-loss =  49.9372022465 , Time for epoch =  0.014037370681762695 s\n",
      "Epoch  721 / 1000  : Train-loss =  50.042104314 , Val-loss =  49.9054194702 , Time for epoch =  0.011034727096557617 s\n",
      "Epoch  722 / 1000  : Train-loss =  50.0139873284 , Val-loss =  49.8716404841 , Time for epoch =  0.010557889938354492 s\n",
      "Epoch  723 / 1000  : Train-loss =  49.9879190801 , Val-loss =  49.8400314766 , Time for epoch =  0.014022350311279297 s\n",
      "Epoch  724 / 1000  : Train-loss =  49.9586782542 , Val-loss =  49.807684542 , Time for epoch =  0.01079702377319336 s\n",
      "Epoch  725 / 1000  : Train-loss =  49.9306307472 , Val-loss =  49.7734265721 , Time for epoch =  0.011138200759887695 s\n",
      "Epoch  726 / 1000  : Train-loss =  49.9047954849 , Val-loss =  49.7414717368 , Time for epoch =  0.011461496353149414 s\n",
      "Epoch  727 / 1000  : Train-loss =  49.8755287901 , Val-loss =  49.7088091185 , Time for epoch =  0.01123952865600586 s\n",
      "Epoch  728 / 1000  : Train-loss =  49.8494842978 , Val-loss =  49.6772034761 , Time for epoch =  0.011284112930297852 s\n",
      "Epoch  729 / 1000  : Train-loss =  49.8207205487 , Val-loss =  49.6445426196 , Time for epoch =  0.010733604431152344 s\n",
      "Epoch  730 / 1000  : Train-loss =  49.7928946238 , Val-loss =  49.6099727966 , Time for epoch =  0.011595010757446289 s\n",
      "Epoch  731 / 1000  : Train-loss =  49.7671748335 , Val-loss =  49.577867429 , Time for epoch =  0.01257944107055664 s\n",
      "Epoch  732 / 1000  : Train-loss =  49.738325205 , Val-loss =  49.5450153432 , Time for epoch =  0.014212608337402344 s\n",
      "Epoch  733 / 1000  : Train-loss =  49.7106620675 , Val-loss =  49.5103476851 , Time for epoch =  0.01406097412109375 s\n",
      "Epoch  734 / 1000  : Train-loss =  49.6853867001 , Val-loss =  49.4782201053 , Time for epoch =  0.014132976531982422 s\n",
      "Epoch  735 / 1000  : Train-loss =  49.6564547263 , Val-loss =  49.4453589215 , Time for epoch =  0.01202845573425293 s\n",
      "Epoch  736 / 1000  : Train-loss =  49.6310913354 , Val-loss =  49.4137362641 , Time for epoch =  0.010354042053222656 s\n",
      "Epoch  737 / 1000  : Train-loss =  49.602358718 , Val-loss =  49.3823289169 , Time for epoch =  0.011513710021972656 s\n",
      "Epoch  738 / 1000  : Train-loss =  49.575655477 , Val-loss =  49.3482443682 , Time for epoch =  0.010793685913085938 s\n",
      "Epoch  739 / 1000  : Train-loss =  49.5503782887 , Val-loss =  49.3164392767 , Time for epoch =  0.010663986206054688 s\n",
      "Epoch  740 / 1000  : Train-loss =  49.5218452675 , Val-loss =  49.2850420148 , Time for epoch =  0.010816097259521484 s\n",
      "Epoch  741 / 1000  : Train-loss =  49.4951335944 , Val-loss =  49.2523515145 , Time for epoch =  0.010708332061767578 s\n",
      "Epoch  742 / 1000  : Train-loss =  49.4684384084 , Val-loss =  49.2193761247 , Time for epoch =  0.013590097427368164 s\n",
      "Epoch  743 / 1000  : Train-loss =  49.4420028263 , Val-loss =  49.1849900515 , Time for epoch =  0.012039422988891602 s\n",
      "Epoch  744 / 1000  : Train-loss =  49.4174441964 , Val-loss =  49.1532272443 , Time for epoch =  0.011512517929077148 s\n",
      "Epoch  745 / 1000  : Train-loss =  49.3889018667 , Val-loss =  49.1219750066 , Time for epoch =  0.010818719863891602 s\n",
      "Epoch  746 / 1000  : Train-loss =  49.3625559018 , Val-loss =  49.0894589547 , Time for epoch =  0.011365890502929688 s\n",
      "Epoch  747 / 1000  : Train-loss =  49.3384865761 , Val-loss =  49.0584985081 , Time for epoch =  0.01256108283996582 s\n",
      "Epoch  748 / 1000  : Train-loss =  49.3104429141 , Val-loss =  49.0275266165 , Time for epoch =  0.010794878005981445 s\n",
      "Epoch  749 / 1000  : Train-loss =  49.2843419541 , Val-loss =  48.9952133975 , Time for epoch =  0.010791301727294922 s\n",
      "Epoch  750 / 1000  : Train-loss =  49.2582576613 , Val-loss =  48.9626147279 , Time for epoch =  0.010398626327514648 s\n",
      "Epoch  751 / 1000  : Train-loss =  49.2322312016 , Val-loss =  48.9299697086 , Time for epoch =  0.010316848754882812 s\n",
      "Epoch  752 / 1000  : Train-loss =  49.2062731117 , Val-loss =  48.8973377967 , Time for epoch =  0.010631561279296875 s\n",
      "Epoch  753 / 1000  : Train-loss =  49.1830419045 , Val-loss =  48.8665539102 , Time for epoch =  0.010423660278320312 s\n",
      "Epoch  754 / 1000  : Train-loss =  49.1550592567 , Val-loss =  48.8358453981 , Time for epoch =  0.010533809661865234 s\n",
      "Epoch  755 / 1000  : Train-loss =  49.129417885 , Val-loss =  48.8038186396 , Time for epoch =  0.010297775268554688 s\n",
      "Epoch  756 / 1000  : Train-loss =  49.1037956621 , Val-loss =  48.7715228778 , Time for epoch =  0.010152339935302734 s\n",
      "Epoch  757 / 1000  : Train-loss =  49.0782336691 , Val-loss =  48.7391942351 , Time for epoch =  0.010277509689331055 s\n",
      "Epoch  758 / 1000  : Train-loss =  49.0556910681 , Val-loss =  48.7087064311 , Time for epoch =  0.010259866714477539 s\n",
      "Epoch  759 / 1000  : Train-loss =  49.0278172526 , Val-loss =  48.678295923 , Time for epoch =  0.010590314865112305 s\n",
      "Epoch  760 / 1000  : Train-loss =  49.0025712064 , Val-loss =  48.6465675985 , Time for epoch =  0.01055908203125 s\n",
      "Epoch  761 / 1000  : Train-loss =  48.9773438902 , Val-loss =  48.6145729896 , Time for epoch =  0.013305425643920898 s\n",
      "Epoch  762 / 1000  : Train-loss =  48.9545369305 , Val-loss =  48.5858879843 , Time for epoch =  0.012855768203735352 s\n",
      "Epoch  763 / 1000  : Train-loss =  48.9278938622 , Val-loss =  48.5562372841 , Time for epoch =  0.01092219352722168 s\n",
      "Epoch  764 / 1000  : Train-loss =  48.9029907396 , Val-loss =  48.5248833564 , Time for epoch =  0.010891199111938477 s\n",
      "Epoch  765 / 1000  : Train-loss =  48.8780886224 , Val-loss =  48.4931698662 , Time for epoch =  0.01137089729309082 s\n",
      "Epoch  766 / 1000  : Train-loss =  48.8532408514 , Val-loss =  48.4613981482 , Time for epoch =  0.012747526168823242 s\n",
      "Epoch  767 / 1000  : Train-loss =  48.8308827242 , Val-loss =  48.433020377 , Time for epoch =  0.014704465866088867 s\n",
      "Epoch  768 / 1000  : Train-loss =  48.8045953183 , Val-loss =  48.4036529878 , Time for epoch =  0.010392189025878906 s\n",
      "Epoch  769 / 1000  : Train-loss =  48.7800797186 , Val-loss =  48.3725784835 , Time for epoch =  0.01387166976928711 s\n",
      "Epoch  770 / 1000  : Train-loss =  48.7555639069 , Val-loss =  48.3411451525 , Time for epoch =  0.01070094108581543 s\n",
      "Epoch  771 / 1000  : Train-loss =  48.7333757276 , Val-loss =  48.3130787387 , Time for epoch =  0.011293888092041016 s\n",
      "Epoch  772 / 1000  : Train-loss =  48.7075836499 , Val-loss =  48.2839580536 , Time for epoch =  0.018611431121826172 s\n",
      "Epoch  773 / 1000  : Train-loss =  48.6833781067 , Val-loss =  48.253117358 , Time for epoch =  0.012029886245727539 s\n",
      "Epoch  774 / 1000  : Train-loss =  48.6591701134 , Val-loss =  48.2219124209 , Time for epoch =  0.0139007568359375 s\n",
      "Epoch  775 / 1000  : Train-loss =  48.6350144882 , Val-loss =  48.1906477397 , Time for epoch =  0.014218330383300781 s\n",
      "Epoch  776 / 1000  : Train-loss =  48.6135233895 , Val-loss =  48.1628299843 , Time for epoch =  0.014264345169067383 s\n",
      "Epoch  777 / 1000  : Train-loss =  48.5877982345 , Val-loss =  48.1339793456 , Time for epoch =  0.016982555389404297 s\n",
      "Epoch  778 / 1000  : Train-loss =  48.5639701475 , Val-loss =  48.1034121157 , Time for epoch =  0.018477916717529297 s\n",
      "Epoch  779 / 1000  : Train-loss =  48.5401393016 , Val-loss =  48.0724866672 , Time for epoch =  0.011694669723510742 s\n",
      "Epoch  780 / 1000  : Train-loss =  48.5189989305 , Val-loss =  48.0449612387 , Time for epoch =  0.011471271514892578 s\n",
      "Epoch  781 / 1000  : Train-loss =  48.4935649819 , Val-loss =  48.0163653792 , Time for epoch =  0.011045455932617188 s\n",
      "Epoch  782 / 1000  : Train-loss =  48.4700413033 , Val-loss =  47.9860420622 , Time for epoch =  0.01223897933959961 s\n",
      "Epoch  783 / 1000  : Train-loss =  48.4488637404 , Val-loss =  47.9588520578 , Time for epoch =  0.011484622955322266 s\n",
      "Epoch  784 / 1000  : Train-loss =  48.4239850824 , Val-loss =  47.9304708099 , Time for epoch =  0.011081218719482422 s\n",
      "Epoch  785 / 1000  : Train-loss =  48.4006891178 , Val-loss =  47.9003358971 , Time for epoch =  0.01136636734008789 s\n",
      "Epoch  786 / 1000  : Train-loss =  48.3773851117 , Val-loss =  47.8698269738 , Time for epoch =  0.012123823165893555 s\n",
      "Epoch  787 / 1000  : Train-loss =  48.3567083917 , Val-loss =  47.8427520095 , Time for epoch =  0.016295671463012695 s\n",
      "Epoch  788 / 1000  : Train-loss =  48.3319020143 , Val-loss =  47.8145588996 , Time for epoch =  0.0114898681640625 s\n",
      "Epoch  789 / 1000  : Train-loss =  48.3088950569 , Val-loss =  47.784627477 , Time for epoch =  0.010599374771118164 s\n",
      "Epoch  790 / 1000  : Train-loss =  48.2882990679 , Val-loss =  47.7578547628 , Time for epoch =  0.010376930236816406 s\n",
      "Epoch  791 / 1000  : Train-loss =  48.2639050568 , Val-loss =  47.7298634892 , Time for epoch =  0.010545015335083008 s\n",
      "Epoch  792 / 1000  : Train-loss =  48.2411183997 , Val-loss =  47.6999538996 , Time for epoch =  0.010703802108764648 s\n",
      "Epoch  793 / 1000  : Train-loss =  48.2183214417 , Val-loss =  47.6696059628 , Time for epoch =  0.010352373123168945 s\n",
      "Epoch  794 / 1000  : Train-loss =  48.1983773622 , Val-loss =  47.6425839882 , Time for epoch =  0.011203527450561523 s\n",
      "Epoch  795 / 1000  : Train-loss =  48.1738810794 , Val-loss =  47.6145315655 , Time for epoch =  0.013788938522338867 s\n",
      "Epoch  796 / 1000  : Train-loss =  48.1513773701 , Val-loss =  47.5847621069 , Time for epoch =  0.011099576950073242 s\n",
      "Epoch  797 / 1000  : Train-loss =  48.1316176067 , Val-loss =  47.5580355694 , Time for epoch =  0.011055231094360352 s\n",
      "Epoch  798 / 1000  : Train-loss =  48.1074159078 , Val-loss =  47.5301941295 , Time for epoch =  0.011566877365112305 s\n",
      "Epoch  799 / 1000  : Train-loss =  48.085128752 , Val-loss =  47.5006140104 , Time for epoch =  0.012413978576660156 s\n",
      "Epoch  800 / 1000  : Train-loss =  48.0655896259 , Val-loss =  47.4740774257 , Time for epoch =  0.011363029479980469 s\n",
      "Epoch  801 / 1000  : Train-loss =  48.0416120407 , Val-loss =  47.4464137531 , Time for epoch =  0.010943889617919922 s\n",
      "Epoch  802 / 1000  : Train-loss =  48.0195356302 , Val-loss =  47.4170062879 , Time for epoch =  0.01103663444519043 s\n",
      "Epoch  803 / 1000  : Train-loss =  48.0002663739 , Val-loss =  47.3906428683 , Time for epoch =  0.010731220245361328 s\n",
      "Epoch  804 / 1000  : Train-loss =  47.9764521699 , Val-loss =  47.3631495784 , Time for epoch =  0.010963201522827148 s\n",
      "Epoch  805 / 1000  : Train-loss =  47.9545831184 , Val-loss =  47.3339094168 , Time for epoch =  0.013240575790405273 s\n",
      "Epoch  806 / 1000  : Train-loss =  47.9356307234 , Val-loss =  47.3077099991 , Time for epoch =  0.010498523712158203 s\n",
      "Epoch  807 / 1000  : Train-loss =  47.9119245422 , Val-loss =  47.2803853173 , Time for epoch =  0.010898590087890625 s\n",
      "Epoch  808 / 1000  : Train-loss =  47.8927748976 , Val-loss =  47.2547646491 , Time for epoch =  0.013875007629394531 s\n",
      "Epoch  809 / 1000  : Train-loss =  47.8696768137 , Val-loss =  47.2276631897 , Time for epoch =  0.01096034049987793 s\n",
      "Epoch  810 / 1000  : Train-loss =  47.8481538727 , Val-loss =  47.1987358359 , Time for epoch =  0.01071929931640625 s\n",
      "Epoch  811 / 1000  : Train-loss =  47.829350836 , Val-loss =  47.1728493958 , Time for epoch =  0.01478266716003418 s\n",
      "Epoch  812 / 1000  : Train-loss =  47.806211366 , Val-loss =  47.1457984611 , Time for epoch =  0.01453089714050293 s\n",
      "Epoch  813 / 1000  : Train-loss =  47.7848798058 , Val-loss =  47.1169893345 , Time for epoch =  0.011887311935424805 s\n",
      "Epoch  814 / 1000  : Train-loss =  47.7665347535 , Val-loss =  47.0912226633 , Time for epoch =  0.010859012603759766 s\n",
      "Epoch  815 / 1000  : Train-loss =  47.7433325548 , Val-loss =  47.0643229573 , Time for epoch =  0.011584043502807617 s\n",
      "Epoch  816 / 1000  : Train-loss =  47.7248786374 , Val-loss =  47.0391185737 , Time for epoch =  0.01094508171081543 s\n",
      "Epoch  817 / 1000  : Train-loss =  47.7021807688 , Val-loss =  47.0124385816 , Time for epoch =  0.012669086456298828 s\n",
      "Epoch  818 / 1000  : Train-loss =  47.6811825002 , Val-loss =  46.9839286532 , Time for epoch =  0.010734796524047852 s\n",
      "Epoch  819 / 1000  : Train-loss =  47.6632037378 , Val-loss =  46.9584428858 , Time for epoch =  0.014663219451904297 s\n",
      "Epoch  820 / 1000  : Train-loss =  47.6403174353 , Val-loss =  46.93181676 , Time for epoch =  0.011109352111816406 s\n",
      "Epoch  821 / 1000  : Train-loss =  47.6222844775 , Val-loss =  46.9068751661 , Time for epoch =  0.012344121932983398 s\n",
      "Epoch  822 / 1000  : Train-loss =  47.5998358461 , Val-loss =  46.8804621898 , Time for epoch =  0.010721921920776367 s\n",
      "Epoch  823 / 1000  : Train-loss =  47.5817109631 , Val-loss =  46.8556805492 , Time for epoch =  0.01038050651550293 s\n",
      "Epoch  824 / 1000  : Train-loss =  47.5596380393 , Val-loss =  46.8293824975 , Time for epoch =  0.010324239730834961 s\n",
      "Epoch  825 / 1000  : Train-loss =  47.53908449 , Val-loss =  46.8012416371 , Time for epoch =  0.010299205780029297 s\n",
      "Epoch  826 / 1000  : Train-loss =  47.521560405 , Val-loss =  46.7761187771 , Time for epoch =  0.01055908203125 s\n",
      "Epoch  827 / 1000  : Train-loss =  47.4991419401 , Val-loss =  46.7498509788 , Time for epoch =  0.011179924011230469 s\n",
      "Epoch  828 / 1000  : Train-loss =  47.4816446423 , Val-loss =  46.7252547332 , Time for epoch =  0.01491093635559082 s\n",
      "Epoch  829 / 1000  : Train-loss =  47.4595688172 , Val-loss =  46.6991948432 , Time for epoch =  0.01394033432006836 s\n",
      "Epoch  830 / 1000  : Train-loss =  47.4420571322 , Val-loss =  46.6747469149 , Time for epoch =  0.010601282119750977 s\n",
      "Epoch  831 / 1000  : Train-loss =  47.4202675227 , Val-loss =  46.6488015327 , Time for epoch =  0.010835409164428711 s\n",
      "Epoch  832 / 1000  : Train-loss =  47.4027665006 , Val-loss =  46.6244682044 , Time for epoch =  0.010811090469360352 s\n",
      "Epoch  833 / 1000  : Train-loss =  47.3812248307 , Val-loss =  46.5986225962 , Time for epoch =  0.010298013687133789 s\n",
      "Epoch  834 / 1000  : Train-loss =  47.3612198132 , Val-loss =  46.5709294402 , Time for epoch =  0.01380014419555664 s\n",
      "Epoch  835 / 1000  : Train-loss =  47.344474668 , Val-loss =  46.5462281785 , Time for epoch =  0.011315345764160156 s\n",
      "Epoch  836 / 1000  : Train-loss =  47.3224102489 , Val-loss =  46.5204098595 , Time for epoch =  0.010664701461791992 s\n",
      "Epoch  837 / 1000  : Train-loss =  47.3057886814 , Val-loss =  46.4962262808 , Time for epoch =  0.010382890701293945 s\n",
      "Epoch  838 / 1000  : Train-loss =  47.2839586123 , Val-loss =  46.470619496 , Time for epoch =  0.010443925857543945 s\n",
      "Epoch  839 / 1000  : Train-loss =  47.2674145252 , Val-loss =  46.4465788311 , Time for epoch =  0.010232210159301758 s\n",
      "Epoch  840 / 1000  : Train-loss =  47.2457682952 , Val-loss =  46.4210930114 , Time for epoch =  0.010439634323120117 s\n",
      "Epoch  841 / 1000  : Train-loss =  47.2293225599 , Val-loss =  46.3971643704 , Time for epoch =  0.010321617126464844 s\n",
      "Epoch  842 / 1000  : Train-loss =  47.2078275092 , Val-loss =  46.3717866946 , Time for epoch =  0.00995492935180664 s\n",
      "Epoch  843 / 1000  : Train-loss =  47.191503666 , Val-loss =  46.3479592975 , Time for epoch =  0.010324239730834961 s\n",
      "Epoch  844 / 1000  : Train-loss =  47.1701302048 , Val-loss =  46.3226839929 , Time for epoch =  0.01035928726196289 s\n",
      "Epoch  845 / 1000  : Train-loss =  47.1539512867 , Val-loss =  46.2989510007 , Time for epoch =  0.010062694549560547 s\n",
      "Epoch  846 / 1000  : Train-loss =  47.1326719047 , Val-loss =  46.2737750623 , Time for epoch =  0.011343717575073242 s\n",
      "Epoch  847 / 1000  : Train-loss =  47.1166601619 , Val-loss =  46.2501319721 , Time for epoch =  0.013843774795532227 s\n",
      "Epoch  848 / 1000  : Train-loss =  47.0954491472 , Val-loss =  46.2250542339 , Time for epoch =  0.011097431182861328 s\n",
      "Epoch  849 / 1000  : Train-loss =  47.0796259545 , Val-loss =  46.2014981829 , Time for epoch =  0.010529756546020508 s\n",
      "Epoch  850 / 1000  : Train-loss =  47.0584591968 , Val-loss =  46.176518746 , Time for epoch =  0.010833501815795898 s\n",
      "Epoch  851 / 1000  : Train-loss =  47.0428450037 , Val-loss =  46.1530480281 , Time for epoch =  0.010932445526123047 s\n",
      "Epoch  852 / 1000  : Train-loss =  47.0216998395 , Val-loss =  46.1281678527 , Time for epoch =  0.013023853302001953 s\n",
      "Epoch  853 / 1000  : Train-loss =  47.0063141467 , Val-loss =  46.1047815708 , Time for epoch =  0.012330293655395508 s\n",
      "Epoch  854 / 1000  : Train-loss =  46.9879508671 , Val-loss =  46.0834070583 , Time for epoch =  0.010534524917602539 s\n",
      "Epoch  855 / 1000  : Train-loss =  46.9677226328 , Val-loss =  46.0591211498 , Time for epoch =  0.010547637939453125 s\n",
      "Epoch  856 / 1000  : Train-loss =  46.951972426 , Val-loss =  46.0360411791 , Time for epoch =  0.010509967803955078 s\n",
      "Epoch  857 / 1000  : Train-loss =  46.9315802498 , Val-loss =  46.0114356981 , Time for epoch =  0.010380268096923828 s\n",
      "Epoch  858 / 1000  : Train-loss =  46.916118027 , Val-loss =  45.9883132621 , Time for epoch =  0.010492563247680664 s\n",
      "Epoch  859 / 1000  : Train-loss =  46.8956242169 , Val-loss =  45.9637608026 , Time for epoch =  0.01038503646850586 s\n",
      "Epoch  860 / 1000  : Train-loss =  46.8804825906 , Val-loss =  45.9406866207 , Time for epoch =  0.01045846939086914 s\n",
      "Epoch  861 / 1000  : Train-loss =  46.8598780567 , Val-loss =  45.9162175936 , Time for epoch =  0.01035761833190918 s\n",
      "Epoch  862 / 1000  : Train-loss =  46.8450732511 , Val-loss =  45.8932057883 , Time for epoch =  0.011024236679077148 s\n",
      "Epoch  863 / 1000  : Train-loss =  46.8272832568 , Val-loss =  45.8721973645 , Time for epoch =  0.01166081428527832 s\n",
      "Epoch  864 / 1000  : Train-loss =  46.8074142224 , Val-loss =  45.848315779 , Time for epoch =  0.011445045471191406 s\n",
      "Epoch  865 / 1000  : Train-loss =  46.7923964929 , Val-loss =  45.8255916247 , Time for epoch =  0.01200413703918457 s\n",
      "Epoch  866 / 1000  : Train-loss =  46.8211282851 , Val-loss =  45.8060700961 , Time for epoch =  0.011437177658081055 s\n",
      "Epoch  867 / 1000  : Train-loss =  46.8174308895 , Val-loss =  45.8342791569 , Time for epoch =  0.010826587677001953 s\n",
      "Epoch  868 / 1000  : Train-loss =  46.812020955 , Val-loss =  45.8672250011 , Time for epoch =  0.010704755783081055 s\n",
      "Epoch  869 / 1000  : Train-loss =  46.8057389171 , Val-loss =  45.8949652626 , Time for epoch =  0.010817766189575195 s\n",
      "Epoch  870 / 1000  : Train-loss =  46.7981665366 , Val-loss =  45.9161730028 , Time for epoch =  0.015535831451416016 s\n",
      "Epoch  871 / 1000  : Train-loss =  46.7893363298 , Val-loss =  45.9312961605 , Time for epoch =  0.01209259033203125 s\n",
      "Epoch  872 / 1000  : Train-loss =  46.7793711212 , Val-loss =  45.9410775328 , Time for epoch =  0.010710954666137695 s\n",
      "Epoch  873 / 1000  : Train-loss =  46.7415689602 , Val-loss =  45.8899192232 , Time for epoch =  0.010768890380859375 s\n",
      "Epoch  874 / 1000  : Train-loss =  46.7137843659 , Val-loss =  45.750991151 , Time for epoch =  0.010863542556762695 s\n",
      "Epoch  875 / 1000  : Train-loss =  46.6960633338 , Val-loss =  45.7107699115 , Time for epoch =  0.010344982147216797 s\n",
      "Epoch  876 / 1000  : Train-loss =  46.6819178807 , Val-loss =  45.6925302735 , Time for epoch =  0.012727499008178711 s\n",
      "Epoch  877 / 1000  : Train-loss =  46.6690934187 , Val-loss =  45.6733355322 , Time for epoch =  0.011422157287597656 s\n",
      "Epoch  878 / 1000  : Train-loss =  46.6545305058 , Val-loss =  45.652057932 , Time for epoch =  0.019134998321533203 s\n",
      "Epoch  879 / 1000  : Train-loss =  46.6411811869 , Val-loss =  45.6323690338 , Time for epoch =  0.01397848129272461 s\n",
      "Epoch  880 / 1000  : Train-loss =  46.6268217318 , Val-loss =  45.6116444043 , Time for epoch =  0.016322851181030273 s\n",
      "Epoch  881 / 1000  : Train-loss =  46.6120408987 , Val-loss =  45.5895217857 , Time for epoch =  0.015756607055664062 s\n",
      "Epoch  882 / 1000  : Train-loss =  46.5968776484 , Val-loss =  45.5661223839 , Time for epoch =  0.011932373046875 s\n",
      "Epoch  883 / 1000  : Train-loss =  46.5806820391 , Val-loss =  45.5480216982 , Time for epoch =  0.010608911514282227 s\n",
      "Epoch  884 / 1000  : Train-loss =  46.56675576 , Val-loss =  45.5237603006 , Time for epoch =  0.011208295822143555 s\n",
      "Epoch  885 / 1000  : Train-loss =  46.5500262841 , Val-loss =  45.5041831681 , Time for epoch =  0.011395692825317383 s\n",
      "Epoch  886 / 1000  : Train-loss =  46.5350318314 , Val-loss =  45.4849435909 , Time for epoch =  0.011115789413452148 s\n",
      "Epoch  887 / 1000  : Train-loss =  46.5205239601 , Val-loss =  45.4589724771 , Time for epoch =  0.015888452529907227 s\n",
      "Epoch  888 / 1000  : Train-loss =  46.5032682193 , Val-loss =  45.4378229021 , Time for epoch =  0.01466512680053711 s\n",
      "Epoch  889 / 1000  : Train-loss =  46.4878199185 , Val-loss =  45.4172207781 , Time for epoch =  0.01629924774169922 s\n",
      "Epoch  890 / 1000  : Train-loss =  46.4729204829 , Val-loss =  45.390099443 , Time for epoch =  0.012403249740600586 s\n",
      "Epoch  891 / 1000  : Train-loss =  46.4553126181 , Val-loss =  45.3679948559 , Time for epoch =  0.013237714767456055 s\n",
      "Epoch  892 / 1000  : Train-loss =  46.4395777449 , Val-loss =  45.3465832949 , Time for epoch =  0.01834249496459961 s\n",
      "Epoch  893 / 1000  : Train-loss =  46.4238150598 , Val-loss =  45.3251983436 , Time for epoch =  0.013928890228271484 s\n",
      "Epoch  894 / 1000  : Train-loss =  46.4085825337 , Val-loss =  45.2972642355 , Time for epoch =  0.017070770263671875 s\n",
      "Epoch  895 / 1000  : Train-loss =  46.3907407871 , Val-loss =  45.27449794 , Time for epoch =  0.020963668823242188 s\n",
      "Epoch  896 / 1000  : Train-loss =  46.3747933405 , Val-loss =  45.2525405544 , Time for epoch =  0.01680302619934082 s\n",
      "Epoch  897 / 1000  : Train-loss =  46.3594614361 , Val-loss =  45.2243078429 , Time for epoch =  0.016627788543701172 s\n",
      "Epoch  898 / 1000  : Train-loss =  46.3414721121 , Val-loss =  45.2013055826 , Time for epoch =  0.017407655715942383 s\n",
      "Epoch  899 / 1000  : Train-loss =  46.3254445825 , Val-loss =  45.1791600271 , Time for epoch =  0.017710208892822266 s\n",
      "Epoch  900 / 1000  : Train-loss =  46.3094421456 , Val-loss =  45.1571828127 , Time for epoch =  0.017206192016601562 s\n",
      "Epoch  901 / 1000  : Train-loss =  46.2934393015 , Val-loss =  45.1352331985 , Time for epoch =  0.017156600952148438 s\n",
      "Epoch  902 / 1000  : Train-loss =  46.2779986713 , Val-loss =  45.1068508617 , Time for epoch =  0.016899824142456055 s\n",
      "Epoch  903 / 1000  : Train-loss =  46.2599704944 , Val-loss =  45.0837371895 , Time for epoch =  0.017349720001220703 s\n",
      "Epoch  904 / 1000  : Train-loss =  46.2439082593 , Val-loss =  45.0615128179 , Time for epoch =  0.017178058624267578 s\n",
      "Epoch  905 / 1000  : Train-loss =  46.2278871944 , Val-loss =  45.0394818802 , Time for epoch =  0.014602184295654297 s\n",
      "Epoch  906 / 1000  : Train-loss =  46.2118804791 , Val-loss =  45.0175133253 , Time for epoch =  0.018284320831298828 s\n",
      "Epoch  907 / 1000  : Train-loss =  46.1958830001 , Val-loss =  44.995551139 , Time for epoch =  0.011728048324584961 s\n",
      "Epoch  908 / 1000  : Train-loss =  46.1804296555 , Val-loss =  44.967181564 , Time for epoch =  0.011165618896484375 s\n",
      "Epoch  909 / 1000  : Train-loss =  46.1624326847 , Val-loss =  44.9441111519 , Time for epoch =  0.011266708374023438 s\n",
      "Epoch  910 / 1000  : Train-loss =  46.1464162198 , Val-loss =  44.9219529393 , Time for epoch =  0.010711431503295898 s\n",
      "Epoch  911 / 1000  : Train-loss =  46.1304522259 , Val-loss =  44.9000071072 , Time for epoch =  0.010776758193969727 s\n",
      "Epoch  912 / 1000  : Train-loss =  46.1145129588 , Val-loss =  44.8781207843 , Time for epoch =  0.012993335723876953 s\n",
      "Epoch  913 / 1000  : Train-loss =  46.0985925538 , Val-loss =  44.8562650438 , Time for epoch =  0.015208244323730469 s\n",
      "Epoch  914 / 1000  : Train-loss =  46.0826899439 , Val-loss =  44.834432254 , Time for epoch =  0.01535177230834961 s\n",
      "Epoch  915 / 1000  : Train-loss =  46.0668051704 , Val-loss =  44.8126117326 , Time for epoch =  0.013167858123779297 s\n",
      "Epoch  916 / 1000  : Train-loss =  46.0509385727 , Val-loss =  44.7908026005 , Time for epoch =  0.016816139221191406 s\n",
      "Epoch  917 / 1000  : Train-loss =  46.0355991578 , Val-loss =  44.7626330536 , Time for epoch =  0.015377044677734375 s\n",
      "Epoch  918 / 1000  : Train-loss =  46.0177355302 , Val-loss =  44.7397535052 , Time for epoch =  0.01238870620727539 s\n",
      "Epoch  919 / 1000  : Train-loss =  46.0018819737 , Val-loss =  44.7178008552 , Time for epoch =  0.01132965087890625 s\n",
      "Epoch  920 / 1000  : Train-loss =  45.9860898264 , Val-loss =  44.6960732968 , Time for epoch =  0.016475677490234375 s\n",
      "Epoch  921 / 1000  : Train-loss =  45.9703307894 , Val-loss =  44.6744173384 , Time for epoch =  0.016717910766601562 s\n",
      "Epoch  922 / 1000  : Train-loss =  45.9545986074 , Val-loss =  44.6527980904 , Time for epoch =  0.01468348503112793 s\n",
      "Epoch  923 / 1000  : Train-loss =  45.9388918662 , Val-loss =  44.6312066756 , Time for epoch =  0.01160573959350586 s\n",
      "Epoch  924 / 1000  : Train-loss =  45.9232102784 , Val-loss =  44.609640215 , Time for epoch =  0.011483192443847656 s\n",
      "Epoch  925 / 1000  : Train-loss =  45.907553867 , Val-loss =  44.5880974505 , Time for epoch =  0.01092672348022461 s\n",
      "Epoch  926 / 1000  : Train-loss =  45.8919227765 , Val-loss =  44.5665777441 , Time for epoch =  0.011373758316040039 s\n",
      "Epoch  927 / 1000  : Train-loss =  45.8763172203 , Val-loss =  44.545080814 , Time for epoch =  0.0113983154296875 s\n",
      "Epoch  928 / 1000  : Train-loss =  45.8607374607 , Val-loss =  44.5236066371 , Time for epoch =  0.012928485870361328 s\n",
      "Epoch  929 / 1000  : Train-loss =  45.845183796 , Val-loss =  44.5021553935 , Time for epoch =  0.014036178588867188 s\n",
      "Epoch  930 / 1000  : Train-loss =  45.8296565513 , Val-loss =  44.4807274258 , Time for epoch =  0.01407933235168457 s\n",
      "Epoch  931 / 1000  : Train-loss =  45.8141560717 , Val-loss =  44.4593232061 , Time for epoch =  0.011322021484375 s\n",
      "Epoch  932 / 1000  : Train-loss =  45.7986827156 , Val-loss =  44.4379433088 , Time for epoch =  0.010307073593139648 s\n",
      "Epoch  933 / 1000  : Train-loss =  45.7832368498 , Val-loss =  44.4165883881 , Time for epoch =  0.010317802429199219 s\n",
      "Epoch  934 / 1000  : Train-loss =  45.7678188458 , Val-loss =  44.3952591586 , Time for epoch =  0.011177778244018555 s\n",
      "Epoch  935 / 1000  : Train-loss =  45.7524290761 , Val-loss =  44.37395638 , Time for epoch =  0.010731220245361328 s\n",
      "Epoch  936 / 1000  : Train-loss =  45.7370679115 , Val-loss =  44.3526438058 , Time for epoch =  0.010334014892578125 s\n",
      "Epoch  937 / 1000  : Train-loss =  45.721735719 , Val-loss =  44.3311840294 , Time for epoch =  0.01039433479309082 s\n",
      "Epoch  938 / 1000  : Train-loss =  45.70643286 , Val-loss =  44.3097539107 , Time for epoch =  0.011230230331420898 s\n",
      "Epoch  939 / 1000  : Train-loss =  45.6911596887 , Val-loss =  44.2883542844 , Time for epoch =  0.014186620712280273 s\n",
      "Epoch  940 / 1000  : Train-loss =  45.6759165514 , Val-loss =  44.2669859839 , Time for epoch =  0.011251688003540039 s\n",
      "Epoch  941 / 1000  : Train-loss =  45.6607037853 , Val-loss =  44.245649836 , Time for epoch =  0.011226892471313477 s\n",
      "Epoch  942 / 1000  : Train-loss =  45.6455217178 , Val-loss =  44.2243466572 , Time for epoch =  0.011008501052856445 s\n",
      "Epoch  943 / 1000  : Train-loss =  45.6303706665 , Val-loss =  44.2030772503 , Time for epoch =  0.011352777481079102 s\n",
      "Epoch  944 / 1000  : Train-loss =  45.6152509383 , Val-loss =  44.181842402 , Time for epoch =  0.011196374893188477 s\n",
      "Epoch  945 / 1000  : Train-loss =  45.6001628295 , Val-loss =  44.1606428806 , Time for epoch =  0.015665769577026367 s\n",
      "Epoch  946 / 1000  : Train-loss =  45.5851066257 , Val-loss =  44.1394794346 , Time for epoch =  0.011782646179199219 s\n",
      "Epoch  947 / 1000  : Train-loss =  45.5700826016 , Val-loss =  44.1183527918 , Time for epoch =  0.010737180709838867 s\n",
      "Epoch  948 / 1000  : Train-loss =  45.555091021 , Val-loss =  44.0972636581 , Time for epoch =  0.010825395584106445 s\n",
      "Epoch  949 / 1000  : Train-loss =  45.5401321373 , Val-loss =  44.0762127172 , Time for epoch =  0.011188268661499023 s\n",
      "Epoch  950 / 1000  : Train-loss =  45.5252061928 , Val-loss =  44.0552006302 , Time for epoch =  0.010616540908813477 s\n",
      "Epoch  951 / 1000  : Train-loss =  45.5103134199 , Val-loss =  44.0342280352 , Time for epoch =  0.010452985763549805 s\n",
      "Epoch  952 / 1000  : Train-loss =  45.4954540402 , Val-loss =  44.0132955476 , Time for epoch =  0.010428190231323242 s\n",
      "Epoch  953 / 1000  : Train-loss =  45.4806282654 , Val-loss =  43.9924037604 , Time for epoch =  0.010397672653198242 s\n",
      "Epoch  954 / 1000  : Train-loss =  45.4658362975 , Val-loss =  43.9715532436 , Time for epoch =  0.010403871536254883 s\n",
      "Epoch  955 / 1000  : Train-loss =  45.4510783284 , Val-loss =  43.9507445455 , Time for epoch =  0.01030588150024414 s\n",
      "Epoch  956 / 1000  : Train-loss =  45.436354541 , Val-loss =  43.9299781922 , Time for epoch =  0.016736268997192383 s\n",
      "Epoch  957 / 1000  : Train-loss =  45.4216651086 , Val-loss =  43.9092546886 , Time for epoch =  0.017828702926635742 s\n",
      "Epoch  958 / 1000  : Train-loss =  45.4070101958 , Val-loss =  43.8885745184 , Time for epoch =  0.014858007431030273 s\n",
      "Epoch  959 / 1000  : Train-loss =  45.3923899582 , Val-loss =  43.867938145 , Time for epoch =  0.010846138000488281 s\n",
      "Epoch  960 / 1000  : Train-loss =  45.3778045431 , Val-loss =  43.8473542277 , Time for epoch =  0.010702371597290039 s\n",
      "Epoch  961 / 1000  : Train-loss =  45.3628623529 , Val-loss =  43.8324424823 , Time for epoch =  0.011147022247314453 s\n",
      "Epoch  962 / 1000  : Train-loss =  45.3502898008 , Val-loss =  43.8129238332 , Time for epoch =  0.011040687561035156 s\n",
      "Epoch  963 / 1000  : Train-loss =  45.3358398154 , Val-loss =  43.7926196864 , Time for epoch =  0.012054443359375 s\n",
      "Epoch  964 / 1000  : Train-loss =  45.3213874702 , Val-loss =  43.7721703882 , Time for epoch =  0.010658025741577148 s\n",
      "Epoch  965 / 1000  : Train-loss =  45.306959843 , Val-loss =  43.7517329032 , Time for epoch =  0.010408639907836914 s\n",
      "Epoch  966 / 1000  : Train-loss =  45.2921686587 , Val-loss =  43.7368825575 , Time for epoch =  0.010326862335205078 s\n",
      "Epoch  967 / 1000  : Train-loss =  45.2797645726 , Val-loss =  43.7174712107 , Time for epoch =  0.010350465774536133 s\n",
      "Epoch  968 / 1000  : Train-loss =  45.2654637353 , Val-loss =  43.6972741049 , Time for epoch =  0.01070547103881836 s\n",
      "Epoch  969 / 1000  : Train-loss =  45.2511599136 , Val-loss =  43.6769452824 , Time for epoch =  0.010755300521850586 s\n",
      "Epoch  970 / 1000  : Train-loss =  45.236494155 , Val-loss =  43.6621261894 , Time for epoch =  0.010586977005004883 s\n",
      "Epoch  971 / 1000  : Train-loss =  45.2242075643 , Val-loss =  43.6427736421 , Time for epoch =  0.010280609130859375 s\n",
      "Epoch  972 / 1000  : Train-loss =  45.2100196665 , Val-loss =  43.6226466546 , Time for epoch =  0.010164260864257812 s\n",
      "Epoch  973 / 1000  : Train-loss =  45.1954768997 , Val-loss =  43.6078748626 , Time for epoch =  0.010476827621459961 s\n",
      "Epoch  974 / 1000  : Train-loss =  45.1832467272 , Val-loss =  43.5885468409 , Time for epoch =  0.011524200439453125 s\n",
      "Epoch  975 / 1000  : Train-loss =  45.1691376608 , Val-loss =  43.5684376511 , Time for epoch =  0.011092662811279297 s\n",
      "Epoch  976 / 1000  : Train-loss =  45.1546937269 , Val-loss =  43.5536611178 , Time for epoch =  0.010808706283569336 s\n",
      "Epoch  977 / 1000  : Train-loss =  45.1425299192 , Val-loss =  43.5343469657 , Time for epoch =  0.01100301742553711 s\n",
      "Epoch  978 / 1000  : Train-loss =  45.1284981538 , Val-loss =  43.5142583445 , Time for epoch =  0.011451005935668945 s\n",
      "Epoch  979 / 1000  : Train-loss =  45.114139315 , Val-loss =  43.4994713109 , Time for epoch =  0.010894298553466797 s\n",
      "Epoch  980 / 1000  : Train-loss =  45.1020539805 , Val-loss =  43.4801751498 , Time for epoch =  0.011598825454711914 s\n",
      "Epoch  981 / 1000  : Train-loss =  45.0880997289 , Val-loss =  43.4601167339 , Time for epoch =  0.012924671173095703 s\n",
      "Epoch  982 / 1000  : Train-loss =  45.0738145303 , Val-loss =  43.4453187659 , Time for epoch =  0.014641761779785156 s\n",
      "Epoch  983 / 1000  : Train-loss =  45.0618197233 , Val-loss =  43.4260549175 , Time for epoch =  0.014220952987670898 s\n",
      "Epoch  984 / 1000  : Train-loss =  45.0479439483 , Val-loss =  43.4060321564 , Time for epoch =  0.011270761489868164 s\n",
      "Epoch  985 / 1000  : Train-loss =  45.0337225093 , Val-loss =  43.3912254807 , Time for epoch =  0.010471343994140625 s\n",
      "Epoch  986 / 1000  : Train-loss =  45.0218298237 , Val-loss =  43.3720134031 , Time for epoch =  0.010614395141601562 s\n",
      "Epoch  987 / 1000  : Train-loss =  45.0077305539 , Val-loss =  43.3573857126 , Time for epoch =  0.014052391052246094 s\n",
      "Epoch  988 / 1000  : Train-loss =  44.995858175 , Val-loss =  43.3381962007 , Time for epoch =  0.010663270950317383 s\n",
      "Epoch  989 / 1000  : Train-loss =  44.9821095846 , Val-loss =  43.3180089098 , Time for epoch =  0.011055469512939453 s\n",
      "Epoch  990 / 1000  : Train-loss =  44.9680192381 , Val-loss =  43.3029790747 , Time for epoch =  0.01134800910949707 s\n",
      "Epoch  991 / 1000  : Train-loss =  44.9562610595 , Val-loss =  43.2835992401 , Time for epoch =  0.015084981918334961 s\n",
      "Epoch  992 / 1000  : Train-loss =  44.9422793841 , Val-loss =  43.2687445191 , Time for epoch =  0.014651298522949219 s\n",
      "Epoch  993 / 1000  : Train-loss =  44.9305538687 , Val-loss =  43.2494025364 , Time for epoch =  0.011395454406738281 s\n",
      "Epoch  994 / 1000  : Train-loss =  44.9166412437 , Val-loss =  43.2345502135 , Time for epoch =  0.010985851287841797 s\n",
      "Epoch  995 / 1000  : Train-loss =  44.904946748 , Val-loss =  43.2152090104 , Time for epoch =  0.010967493057250977 s\n",
      "Epoch  996 / 1000  : Train-loss =  44.8910949238 , Val-loss =  43.2003464752 , Time for epoch =  0.011336565017700195 s\n",
      "Epoch  997 / 1000  : Train-loss =  44.8794365263 , Val-loss =  43.1810069762 , Time for epoch =  0.011125564575195312 s\n",
      "Epoch  998 / 1000  : Train-loss =  44.865639522 , Val-loss =  43.1661313669 , Time for epoch =  0.013166427612304688 s\n",
      "Epoch  999 / 1000  : Train-loss =  44.854022703 , Val-loss =  43.1467977298 , Time for epoch =  0.015234708786010742 s\n",
      "Epoch  1000 / 1000  : Train-loss =  44.8402752655 , Val-loss =  43.1319081293 , Time for epoch =  0.014081001281738281 s\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAd4AAAEWCAYAAADIJfYaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUmoKaeFVSimlptD/B13udFVTCoqiAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f217eee0908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.2 s, sys: 542 ms, total: 13.8 s\n",
      "Wall time: 13.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train([X_train, y_train, X_test, y_test], batch_size, [w1, w2], n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
